{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PIB :** https://donnees.banquemondiale.org/indicateur/NY.GDP.MKTP.CD  \n",
    "**Taux de chômage :** https://ec.europa.eu/eurostat/databrowser/view/UNE_RT_M__custom_14826434/default/table?lang=fr\n",
    "**IPCH :**  https://ec.europa.eu/eurostat/databrowser/view/PRC_HICP_MANR__custom_14819170/default/table?lang=fr\n",
    "**Historique des actions :**  \n",
    "**Devise:** https://ec.europa.eu/eurostat/databrowser/view/tec00033/default/table?lang=en&category=t_ert  \n",
    "**Matières premières:** https://bdm.insee.fr/series/sdmx/data/SERIES_BDM/010002100 **et** https://bdm.insee.fr/series/sdmx/data/SERIES_BDM/010002091  \n",
    "**Dette publique:** https://ec.europa.eu/eurostat/databrowser/view/sdg_17_40/default/table?lang=fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type d'analyses prévus et résultats attendus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses prévues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Corrélations entre les différentes données  \n",
    "- Etude d'indices boursiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultats attendus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIB\n",
    "Un PIB croissant est souvent associé à une économie forte, ce qui peut influencer positivement les marchés boursiers. L'analyse cherchera à quantifier cette relation.  \n",
    "\n",
    "### Taux de chômage\n",
    "Un faible taux de chômage peut refléter une économie robuste et un climat favorable aux entreprises, impactant ainsi les actions. Les corrélations entre ces données et les performances boursières seront examinées.   \n",
    "\n",
    "### IPCH (Indice des Prix à la Consommation Harmonisé)\n",
    "L'inflation, mesurée ici par l'IPCH, est un facteur clé pour comprendre les ajustements des marchés financiers aux variations des taux d'intérêt et des prix.  \n",
    "\n",
    "### Historique des actions\n",
    "L'analyse des tendances passées dans les cours des actions permettra d'évaluer la réactivité des marchés aux changements des indicateurs économiques.  \n",
    "\n",
    "### Devise\n",
    "Les fluctuations des taux de change peuvent avoir un impact direct, notamment pour les entreprises opérant à l'international. Les relations entre les cours des actions et les variations des devises seront explorées.  \n",
    "\n",
    "### Matières premières\n",
    "Certains secteurs boursiers sont fortement dépendants des prix des matières premières. L'étude analysera les corrélations spécifiques entre ces prix et les performances des actions dans les secteurs concernés.  \n",
    "\n",
    "### Dette intérieure\n",
    "Le niveau d'endettement d'un pays peut influencer la confiance des investisseurs et, par conséquent, le comportement des marchés. L'étude des corrélations dans ce contexte sera essentielle.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Début du code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import warnings\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "\n",
    "import ipywidgets as  widgets\n",
    "from ipywidgets import interact, widgets, VBox, HBox\n",
    "from ipywidgets import interact_manual\n",
    "import geopandas as gpd\n",
    "\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dette publique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data from the Excel file\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "# Importer les données depuis l'URL\n",
    "dette_publique_url = 'https://sebastien-hein.emi.u-bordeaux.fr/OI-sbzrthstrm/DATA/dette_pub.xlsx'\n",
    "code_url = 'https://sebastien-hein.emi.u-bordeaux.fr/OI-sbzrthstrm/DATA/code.tsv'\n",
    "\n",
    "# Charger les fichiers directement depuis l'URL\n",
    "dette_publique = pd.read_excel(dette_publique_url, sheet_name='Feuille 1')\n",
    "code = pd.read_csv(code_url, sep='\\t')\n",
    "\n",
    "# Define column names for code and label\n",
    "col_code = 'CODE'\n",
    "col_label = 'Label - French'\n",
    "\n",
    "def find_value(x):\n",
    "    \"\"\"Find the label value based on the code.\"\"\"\n",
    "    matched = not code[code[col_code] == x].empty\n",
    "    if matched:\n",
    "        return code[code[col_code] == x].loc[:, col_label].iloc[0]\n",
    "    elif x == 'TIME':\n",
    "        return 'Country'\n",
    "    return None\n",
    "\n",
    "# Modify the index of the dataframe\n",
    "dette_publique.index = dette_publique.iloc[:, 0].apply(find_value)\n",
    "dette_publique.index.name = None\n",
    "\n",
    "# Set the column names based on the 'Country' row\n",
    "dette_publique.columns = dette_publique.loc['Country']\n",
    "\n",
    "# Filter the dataframe to include only rows from 'Belgique' to 'Suède' onwards and exclude the 'TIME' column\n",
    "# Indeed, only the countries interest us,\n",
    "# and the datas are missing for Islande, Norvège, Suisse and United Kingdom.\n",
    "dette_publique = dette_publique.loc['Belgique':'Suède', dette_publique.columns != 'TIME']\n",
    "\n",
    "# Filter the dataframe to include only columns from the year 2002 onwards\n",
    "dette_publique = dette_publique.loc[:,2002:] # Problème non résolu: Si l'on prend une date inférieure à 2002, \n",
    "                                             #                      l'interpolation ne fonctionne nul part.\n",
    "\n",
    "def to_date(x):\n",
    "    \"\"\"Convert a value to datetime.\"\"\"\n",
    "    return pd.to_datetime(x, format='%Y')\n",
    "\n",
    "# Vectorize the to_date function\n",
    "vect_to_date = np.vectorize(to_date)\n",
    "\n",
    "# Convert the columns to datetime\n",
    "dette_publique.columns = vect_to_date(dette_publique.columns.values)\n",
    "\n",
    "monthly_dates = pd.date_range(start=dette_publique.columns.values[0], end=dette_publique.columns.values[-1], freq='MS')\n",
    "\n",
    "# Add columns for each month from 2013 to 2023\n",
    "dette_publique = dette_publique.reindex(columns=dette_publique.columns.union(monthly_dates))\n",
    "\n",
    "def fill_val(x):\n",
    "    \"\"\"Fill missing values by resampling and interpolating.\"\"\"\n",
    "    return x.resample('MS').interpolate(method='quadratic')\n",
    "\n",
    "# Apply the fill_val function to each row\n",
    "dette_publique = dette_publique.apply(func=fill_val, axis=1).T\n",
    "dette_publique.columns.name = 'Country'\n",
    "\n",
    "dette_publique.isna().sum().sum() # Number of missing values (0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albanie: [('2010-12', '2016-11')]\n",
      "Monténégro: [('2010-12', '2015-11')]\n",
      "United Kingdom: [('2020-12', '2024-11')]\n",
      "Kosovo*: [('2010-12', '2016-11')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define URLs for the data sources\n",
    "ipch_url = 'https://sebastien-hein.emi.u-bordeaux.fr/OI-sbzrthstrm/DATA/ipch.tsv'\n",
    "code_cp_url = 'https://sebastien-hein.emi.u-bordeaux.fr/OI-sbzrthstrm/DATA/code_cp.tsv'\n",
    "\n",
    "# Load the data directly from the URLs\n",
    "ipch = pd.read_csv(ipch_url, sep='\\t')  # Load the ipch data using tab as a separator\n",
    "code_cp = pd.read_csv(code_cp_url, sep='\\t')  # Load the code_cp data using tab as a separator\n",
    "\n",
    "\n",
    "# Set index\n",
    "def get_CP(x): return x[8:12]  # Extract CP code\n",
    "def get_id(x): return x[13:]  # Extract id\n",
    "vect_get_cp = np.vectorize(get_CP)\n",
    "vect_get_id = np.vectorize(get_id)\n",
    "ipch['CP'] = vect_get_cp(ipch.iloc[:, 0])  # Apply CP extraction\n",
    "ipch['id'] = vect_get_id(ipch.iloc[:, 0])  # Apply id extraction\n",
    "\n",
    "ipch = ipch[ipch['CP'] == 'CP00']\n",
    "ipch.drop(columns = 'CP', inplace = True)\n",
    "\n",
    "ipch.drop(columns='freq,unit,coicop,geo\\\\TIME_PERIOD', inplace=True)  # Drop unnecessary columns\n",
    "ipch['country'] = ipch.loc[:, 'id'].apply(find_value)  # Find country names\n",
    "ipch.set_index(['country'], inplace=True)  # Set index\n",
    "ipch.drop(columns='id', inplace=True)  # Drop id column\n",
    "\n",
    "# Convert the columns to datetime\n",
    "def to_date_M(x):\n",
    "    \"\"\"Convert a value to datetime.\"\"\"\n",
    "    try:\n",
    "        return pd.to_datetime(x[:-1], format='%Y-%m')\n",
    "    except:\n",
    "        print('fail')\n",
    "        return x\n",
    "\n",
    "vect_to_date_M = np.vectorize(to_date_M)\n",
    "ipch.columns = vect_to_date_M(ipch.columns.values)  # Apply datetime conversion\n",
    "\n",
    "# Filter data\n",
    "ipch = ipch[~ipch.index.str.startswith(('Union', 'Zone', 'Espace'))]  # Exclude certain countries\n",
    "\n",
    "# Clean and convert to numeric\n",
    "ipch = ipch.map(lambda x: pd.to_numeric(\n",
    "    str(x).replace(' ', '').replace('d', ''), errors='coerce'))\n",
    "ipch = ipch.T\n",
    "ipch.columns.name = 'Country'\n",
    "\n",
    "# Missing values\n",
    "# Initialiser le dictionnaire pour stocker les plages de dates manquantes\n",
    "missing_ranges = defaultdict(list)\n",
    "\n",
    "# Identifier les valeurs manquantes\n",
    "missing_values = ipch.isna()\n",
    "\n",
    "# Parcourir chaque pays (colonne) pour trouver les plages de dates manquantes\n",
    "for country in ipch.columns:\n",
    "    country_missing = missing_values[country]\n",
    "    if not country_missing.empty:\n",
    "        # Trouver les plages de dates manquantes\n",
    "        missing_dates = country_missing[country_missing].index\n",
    "        start_date = None\n",
    "        for date in missing_dates:\n",
    "            if start_date is None:\n",
    "                start_date = date\n",
    "            if (date + pd.DateOffset(months=1)) not in missing_dates:\n",
    "                end_date = date\n",
    "                missing_ranges[country].append((start_date.strftime('%Y-%m'), end_date.strftime('%Y-%m')))\n",
    "                start_date = None\n",
    "\n",
    "# Convertir le defaultdict en dict\n",
    "missing_ranges = dict(missing_ranges)\n",
    "\n",
    "for country, dates in missing_ranges.items():\n",
    "    print(f'{country}: {dates}')\n",
    "\n",
    "# Delete the country for which the missing data is on a bigger period than 4 years\n",
    "ipch.drop(columns = 'Albanie, Kosovo*, Monténégro'.split(', '), inplace = True)\n",
    "\n",
    "# Fill missing values using interpolation as the most consistent method for long gaps\n",
    "ipch.interpolate(method='time', inplace=True, limit_direction='both')  # Interpolate linearly by date for smoother transitions\n",
    "\n",
    "# Optionally fill remaining missing values (if interpolation failed for some edge cases) with column mean\n",
    "ipch.fillna(ipch.mean(), inplace=True)\n",
    "\n",
    "ipch.isna().sum().sum() # Number of missing values (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chomage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_xml(file_url):\n",
    "    \"\"\"Parse XML file and extract data into a DataFrame.\"\"\"\n",
    "    \n",
    "    # Download the XML file content using requests\n",
    "    response = requests.get(file_url)\n",
    "    xml_content = response.text  # Get the content of the XML file as a string\n",
    "    \n",
    "    # Parse the XML content with ElementTree\n",
    "    root = ET.fromstring(xml_content)  # Parse the XML string directly\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    data = []\n",
    "    columns = set()\n",
    "    rows = set()\n",
    "\n",
    "    # Extract data from <Series> and <Obs> tags\n",
    "    for series in root.findall('.//Series'):\n",
    "        geo = series.attrib.get('geo')  # Get \"geo\" attribute\n",
    "        if geo:\n",
    "            rows.add(geo)\n",
    "            for obs in series.findall('Obs'):\n",
    "                time_period = obs.attrib.get('TIME_PERIOD')  # Get \"TIME_PERIOD\" attribute\n",
    "                obs_value = obs.attrib.get('OBS_VALUE')  # Get \"OBS_VALUE\" attribute\n",
    "                if time_period and obs_value:\n",
    "                    columns.add(time_period)\n",
    "                    data.append((geo, time_period, obs_value))\n",
    "\n",
    "    # Create DataFrame with appropriate indices\n",
    "    df = pd.DataFrame(index=sorted(rows), columns=sorted(columns))\n",
    "\n",
    "    # Fill DataFrame with extracted values\n",
    "    for geo, time_period, obs_value in data:\n",
    "        df.at[geo, time_period] = obs_value\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for the XML data\n",
    "file_url = 'https://sebastien-hein.emi.u-bordeaux.fr/OI-sbzrthstrm/DATA/chomage.xml'\n",
    "\n",
    "# Load the data\n",
    "chomage = parse_xml(file_url)\n",
    "\n",
    "# Format the data\n",
    "chomage.columns = chomage.columns.map(lambda x: \\\n",
    "                                      pd.to_datetime(x, format='%Y-%m'))  # Convert columns to datetime\n",
    "chomage.index = chomage.index.map(lambda x: \\\n",
    "                code.loc[code.loc[:, 'CODE'] == x, 'Label - French'].iloc[0])  # Map index to labels\n",
    "chomage.drop('Zone euro - 20 pays (à partir de 2023)', inplace=True)  # Drop specific rows\n",
    "chomage.drop('Union européenne - 27 pays (à partir de 2020)', inplace=True)  # Drop specific rows\n",
    "chomage = chomage.apply(pd.to_numeric).T  # Convert data to numeric\n",
    "chomage.columns.name = 'Country'\n",
    "\n",
    "\n",
    "# Missing values\n",
    "missing_ranges = defaultdict(list) # Initialize dictionary to store missing date ranges\n",
    "missing_values = chomage.isna() # Identify missing values\n",
    "\n",
    "for country in chomage.columns: # Loop through each country (column) to find missing date ranges\n",
    "    country_missing = missing_values[country]\n",
    "    if country_missing.any():\n",
    "        # Find the ranges of missing dates\n",
    "        missing_dates = country_missing[country_missing].index\n",
    "        start_date = None\n",
    "        for date in missing_dates:\n",
    "            if start_date is None:\n",
    "                start_date = date\n",
    "            if date + pd.DateOffset(months=1) not in missing_dates:\n",
    "                end_date = date\n",
    "                missing_ranges[country].append((start_date.strftime('%Y-%m'), \n",
    "                                                end_date.strftime('%Y-%m')))\n",
    "                start_date = None\n",
    "\n",
    "# Fill missing values using interpolation as the most consistent method for long gaps\n",
    "chomage.interpolate(method='time', inplace=True, limit_direction='both')  # Interpolate linearly by date for smoother transitions\n",
    "\n",
    "# Optionally fill remaining missing values (if interpolation failed for some edge cases) with column mean\n",
    "chomage.fillna(chomage.mean(), inplace=True)\n",
    "\n",
    "ipch.isna().sum().sum() # Number of missing values (0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml_pib(file_url):\n",
    "    \"\"\"Parse XML file and extract data into a DataFrame.\"\"\"\n",
    "    \n",
    "    # Download the XML file content using requests\n",
    "    response = requests.get(file_url)\n",
    "    xml_content = response.text  # Get the content of the XML file as a string\n",
    "    \n",
    "    # Parse the XML content with ElementTree\n",
    "    root = ET.fromstring(xml_content)  # Parse the XML string directly\n",
    "    \n",
    "    # Initialize a list to store data\n",
    "    data = []\n",
    "\n",
    "    # Extract data\n",
    "    for record in root.findall('.//record'):\n",
    "        record_data = {}\n",
    "        for field in record.findall('field'):\n",
    "            name = field.attrib.get('name')\n",
    "            text = field.text\n",
    "            match name:\n",
    "                case \"Country or Area\": record_data['country'] = text\n",
    "                case \"Value\": \n",
    "                    try: record_data['value'] = float(text)\n",
    "                    except: record_data['value'] = None\n",
    "                case \"Year\": record_data['year'] = pd.to_datetime(str(text))\n",
    "        data.append(record_data)\n",
    "\n",
    "    # Create DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates(subset=['year', 'country'])\n",
    "\n",
    "    # Pivot the DataFrame to get the desired format\n",
    "    df = df.pivot(index='year', columns='country', values='value')\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for the XML data\n",
    "url = 'https://julie-sclaunich.emi.u-bordeaux.fr/DATA/API_NY.GDP.MKTP.CD_DS2_fr_xml_v2_38351.xml'\n",
    "\n",
    "# Load the data\n",
    "pib = parse_xml_pib(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reindex monthly\n",
    "monthly_dates = pd.date_range(start=pib.index.values[0], end=pib.index.values[-1], freq='MS')\n",
    "\n",
    "# Add columns for each month from 2013 to 2023\n",
    "pib = pib.reindex(index=pib.index.union(monthly_dates))\n",
    "pib.columns.name = 'Country'\n",
    "\n",
    "# Select only same dates and countries as dette_publique\n",
    "pib = pib.loc[dette_publique.index, dette_publique.columns.intersection(pib.columns)]\n",
    "\n",
    "def fill_val(x):\n",
    "    \"\"\"Fill missing values by resampling and interpolating.\"\"\"\n",
    "    return x.resample('MS').interpolate(method='quadratic')\n",
    "\n",
    "# Apply the fill_val function to each row\n",
    "pib = pib.apply(func=fill_val, axis=0)\n",
    "\n",
    "pib.isna().sum().sum() # Number of missing values (0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Currency</th>\n",
       "      <th>Bosnia and Herzegovina convertible mark</th>\n",
       "      <th>Bulgarian lev</th>\n",
       "      <th>Canadian dollar</th>\n",
       "      <th>Czech koruna</th>\n",
       "      <th>Danish krone</th>\n",
       "      <th>Hungarian forint</th>\n",
       "      <th>Icelandic króna</th>\n",
       "      <th>Japanese yen</th>\n",
       "      <th>North Macedonian denar</th>\n",
       "      <th>Norwegian krone</th>\n",
       "      <th>Polish zloty</th>\n",
       "      <th>Pound sterling</th>\n",
       "      <th>Romanian leu</th>\n",
       "      <th>Russian rouble</th>\n",
       "      <th>Serbian dinar</th>\n",
       "      <th>Swedish krona</th>\n",
       "      <th>Swiss franc</th>\n",
       "      <th>Turkish lira</th>\n",
       "      <th>US dollar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>1.95583</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>1.368400</td>\n",
       "      <td>25.980000</td>\n",
       "      <td>7.457900</td>\n",
       "      <td>296.870000</td>\n",
       "      <td>162.380000</td>\n",
       "      <td>129.660000</td>\n",
       "      <td>61.585000</td>\n",
       "      <td>7.806700</td>\n",
       "      <td>4.197500</td>\n",
       "      <td>0.849260</td>\n",
       "      <td>4.419000</td>\n",
       "      <td>42.337000</td>\n",
       "      <td>113.136900</td>\n",
       "      <td>8.651500</td>\n",
       "      <td>1.231100</td>\n",
       "      <td>2.533500</td>\n",
       "      <td>1.328100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-01</th>\n",
       "      <td>1.95583</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>1.384134</td>\n",
       "      <td>26.195389</td>\n",
       "      <td>7.457178</td>\n",
       "      <td>298.365029</td>\n",
       "      <td>161.761335</td>\n",
       "      <td>131.231491</td>\n",
       "      <td>61.590730</td>\n",
       "      <td>7.849537</td>\n",
       "      <td>4.197594</td>\n",
       "      <td>0.848684</td>\n",
       "      <td>4.422440</td>\n",
       "      <td>42.618453</td>\n",
       "      <td>113.521815</td>\n",
       "      <td>8.697162</td>\n",
       "      <td>1.236956</td>\n",
       "      <td>2.577914</td>\n",
       "      <td>1.339790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-01</th>\n",
       "      <td>1.95583</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>1.397158</td>\n",
       "      <td>26.376653</td>\n",
       "      <td>7.456598</td>\n",
       "      <td>299.637286</td>\n",
       "      <td>161.199346</td>\n",
       "      <td>132.544486</td>\n",
       "      <td>61.595525</td>\n",
       "      <td>7.888816</td>\n",
       "      <td>4.197485</td>\n",
       "      <td>0.847671</td>\n",
       "      <td>4.425333</td>\n",
       "      <td>42.944501</td>\n",
       "      <td>113.864562</td>\n",
       "      <td>8.737177</td>\n",
       "      <td>1.241087</td>\n",
       "      <td>2.615999</td>\n",
       "      <td>1.348489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-01</th>\n",
       "      <td>1.95583</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>1.410265</td>\n",
       "      <td>26.562635</td>\n",
       "      <td>7.456038</td>\n",
       "      <td>300.959399</td>\n",
       "      <td>160.573608</td>\n",
       "      <td>133.880340</td>\n",
       "      <td>61.600413</td>\n",
       "      <td>7.932953</td>\n",
       "      <td>4.197150</td>\n",
       "      <td>0.846004</td>\n",
       "      <td>4.428299</td>\n",
       "      <td>43.385011</td>\n",
       "      <td>114.238586</td>\n",
       "      <td>8.780119</td>\n",
       "      <td>1.244379</td>\n",
       "      <td>2.655914</td>\n",
       "      <td>1.356062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-01</th>\n",
       "      <td>1.95583</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>1.421635</td>\n",
       "      <td>26.727907</td>\n",
       "      <td>7.455577</td>\n",
       "      <td>302.152358</td>\n",
       "      <td>159.964517</td>\n",
       "      <td>135.055220</td>\n",
       "      <td>61.604722</td>\n",
       "      <td>7.976317</td>\n",
       "      <td>4.196610</td>\n",
       "      <td>0.843845</td>\n",
       "      <td>4.430932</td>\n",
       "      <td>43.890885</td>\n",
       "      <td>114.595096</td>\n",
       "      <td>8.820316</td>\n",
       "      <td>1.246282</td>\n",
       "      <td>2.692291</td>\n",
       "      <td>1.361329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Currency    Bosnia and Herzegovina convertible mark  Bulgarian lev  \\\n",
       "2013-01-01                                  1.95583         1.9558   \n",
       "2013-02-01                                  1.95583         1.9558   \n",
       "2013-03-01                                  1.95583         1.9558   \n",
       "2013-04-01                                  1.95583         1.9558   \n",
       "2013-05-01                                  1.95583         1.9558   \n",
       "\n",
       "Currency    Canadian dollar  Czech koruna  Danish krone  Hungarian forint  \\\n",
       "2013-01-01         1.368400     25.980000      7.457900        296.870000   \n",
       "2013-02-01         1.384134     26.195389      7.457178        298.365029   \n",
       "2013-03-01         1.397158     26.376653      7.456598        299.637286   \n",
       "2013-04-01         1.410265     26.562635      7.456038        300.959399   \n",
       "2013-05-01         1.421635     26.727907      7.455577        302.152358   \n",
       "\n",
       "Currency    Icelandic króna  Japanese yen  North Macedonian denar  \\\n",
       "2013-01-01       162.380000    129.660000               61.585000   \n",
       "2013-02-01       161.761335    131.231491               61.590730   \n",
       "2013-03-01       161.199346    132.544486               61.595525   \n",
       "2013-04-01       160.573608    133.880340               61.600413   \n",
       "2013-05-01       159.964517    135.055220               61.604722   \n",
       "\n",
       "Currency    Norwegian krone  Polish zloty  Pound sterling  Romanian leu  \\\n",
       "2013-01-01         7.806700      4.197500        0.849260      4.419000   \n",
       "2013-02-01         7.849537      4.197594        0.848684      4.422440   \n",
       "2013-03-01         7.888816      4.197485        0.847671      4.425333   \n",
       "2013-04-01         7.932953      4.197150        0.846004      4.428299   \n",
       "2013-05-01         7.976317      4.196610        0.843845      4.430932   \n",
       "\n",
       "Currency    Russian rouble  Serbian dinar  Swedish krona  Swiss franc  \\\n",
       "2013-01-01       42.337000     113.136900       8.651500     1.231100   \n",
       "2013-02-01       42.618453     113.521815       8.697162     1.236956   \n",
       "2013-03-01       42.944501     113.864562       8.737177     1.241087   \n",
       "2013-04-01       43.385011     114.238586       8.780119     1.244379   \n",
       "2013-05-01       43.890885     114.595096       8.820316     1.246282   \n",
       "\n",
       "Currency    Turkish lira  US dollar  \n",
       "2013-01-01      2.533500   1.328100  \n",
       "2013-02-01      2.577914   1.339790  \n",
       "2013-03-01      2.615999   1.348489  \n",
       "2013-04-01      2.655914   1.356062  \n",
       "2013-05-01      2.692291   1.361329  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL of the CSV file\n",
    "url_3 = 'https://julie-sclaunich.emi.u-bordeaux.fr/DATA/estat_tec00033_filtered_en.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "devise = pd.read_csv(url_3)\n",
    "\n",
    "# Remove unnecessary columns to clean up the data\n",
    "columns_to_delete = ['DATAFLOW', 'LAST UPDATE', 'freq', 'statinfo', 'unit', 'OBS_FLAG']\n",
    "devise.drop(columns=columns_to_delete, inplace=True)\n",
    "\n",
    "# Convert 'TIME_PERIOD' to datetime format and filter rows after 2012\n",
    "devise['TIME_PERIOD'] = pd.to_datetime(devise['TIME_PERIOD'], format='%Y', errors='coerce')  # Convert to datetime\n",
    "devise = devise[devise['TIME_PERIOD'] > '2012-12-31']  # Keep rows with dates after 2012\n",
    "devise['TIME_PERIOD'] = devise['TIME_PERIOD'].dt.strftime('%Y/%m')  # Format as YYYY/MM\n",
    "\n",
    "# Reshape the DataFrame to have 'TIME_PERIOD' as row index and 'currency' as columns\n",
    "devise = devise.pivot(index='TIME_PERIOD', columns='currency', values='OBS_VALUE')\n",
    "\n",
    "# Set the index to be a DatetimeIndex for resampling\n",
    "devise.index = pd.to_datetime(devise.index, format='%Y/%m', errors='coerce')\n",
    "devise.index.name = None\n",
    "devise.columns.name = 'Currency'\n",
    "\n",
    "# Define a function to fill missing values by resampling and interpolating\n",
    "def fill_val(x):\n",
    "    \"\"\"Fill missing values by resampling to monthly frequency and using quadratic interpolation.\"\"\"\n",
    "    return x.resample('MS').interpolate(method='quadratic')\n",
    "\n",
    "# Apply the interpolation function to fill missing values\n",
    "devise = fill_val(devise)\n",
    "print(devise.isna().sum().sum()) # Number of missing values (24 a voir pourquoi)\n",
    "# Display a sample of the corrected data\n",
    "devise.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matières premières\n",
    "## Or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Or</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-01</th>\n",
       "      <td>201.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-01</th>\n",
       "      <td>198.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-01</th>\n",
       "      <td>195.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-01</th>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-01</th>\n",
       "      <td>190.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Or\n",
       "2023-12-01  201.5\n",
       "2023-11-01  198.6\n",
       "2023-10-01  195.8\n",
       "2023-09-01  194.0\n",
       "2023-08-01  190.2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_or = \"https://bdm.insee.fr/series/sdmx/data/SERIES_BDM/010002100\" # URL of the serie\n",
    "\n",
    "response = requests.get(url_or) # Retrieve XML data\n",
    "response.raise_for_status() # Checks that the request is successful\n",
    "xml_content = response.content\n",
    "\n",
    "\n",
    "root = ET.fromstring(xml_content) # Parse XML content\n",
    "\n",
    "root = ET.fromstring(xml_content) # Load XML content\n",
    "\n",
    "\n",
    "data = [] # Initialize a list to store the data\n",
    "\n",
    "\n",
    "for series in root.findall(\".//{*}Series\"): # Browse each series\n",
    "\n",
    "    for obs in series.findall(\".//{*}Obs\"): # Browse the observation in  each series\n",
    "\n",
    "        # Extract relevant \n",
    "        time_period = obs.attrib.get(\"TIME_PERIOD\")\n",
    "        obs_value = obs.attrib.get(\"OBS_VALUE\")\n",
    "        # Add the data at the list\n",
    "        data.append({\"TIME_PERIOD\": time_period, \"OBS_VALUE\": obs_value})\n",
    "\n",
    "\n",
    "df_or = pd.DataFrame(data) # Create a DataFrame from the extracted data\n",
    "\n",
    "\n",
    "# Convert columns to appropriate types\n",
    "df_or[\"TIME_PERIOD\"] = pd.to_datetime(df_or[\"TIME_PERIOD\"], format=\"%Y-%m\")\n",
    "df_or[\"OBS_VALUE\"] = pd.to_numeric(df_or[\"OBS_VALUE\"])\n",
    "\n",
    "\n",
    "\n",
    "# Convert TIME_PERIOD to datetime for easier filtering\n",
    "df_or['TIME_PERIOD'] = pd.to_datetime(df_or['TIME_PERIOD'], format='%Y-%m')\n",
    "\n",
    "# Filter years between 2013 and 2023\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2023-12-31'\n",
    "df_or = df_or[(df_or['TIME_PERIOD'] >= start_date) & (df_or['TIME_PERIOD'] <= end_date)]\n",
    "\n",
    "df_or.set_index('TIME_PERIOD', inplace=True) #indexes the years\n",
    "df_or.index.name = None\n",
    "df_or.columns.name = None\n",
    "df_or.rename(columns = {'OBS_VALUE': 'Or'}, inplace = True)\n",
    "print(df_or.isna().sum().sum()) # Number of missing values (0)\n",
    "# show the 5 first rows\n",
    "df_or.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pétrole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Material</th>\n",
       "      <th>Petrol</th>\n",
       "      <th>Or</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-01</th>\n",
       "      <td>118.1</td>\n",
       "      <td>201.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-01</th>\n",
       "      <td>127.3</td>\n",
       "      <td>198.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-01</th>\n",
       "      <td>142.3</td>\n",
       "      <td>195.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-01</th>\n",
       "      <td>145.2</td>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-01</th>\n",
       "      <td>130.9</td>\n",
       "      <td>190.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Material    Petrol     Or\n",
       "2023-12-01   118.1  201.5\n",
       "2023-11-01   127.3  198.6\n",
       "2023-10-01   142.3  195.8\n",
       "2023-09-01   145.2  194.0\n",
       "2023-08-01   130.9  190.2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_petrol = \"https://bdm.insee.fr/series/sdmx/data/SERIES_BDM/010002091\" # URL of the serie\n",
    "\n",
    "response = requests.get(url_petrol) # Retrieve XML data\n",
    "response.raise_for_status()   # Checks that the request is successful\n",
    "xml_content = response.content\n",
    "\n",
    "\n",
    "root = ET.fromstring(xml_content) # Parse XML content\n",
    "\n",
    "root = ET.fromstring(xml_content) # Load XML content\n",
    "\n",
    "# Initialiser une liste pour stocker les données\n",
    "data = []\n",
    "\n",
    "\n",
    "for series in root.findall(\".//{*}Series\"): # Browse each series\n",
    "   \n",
    "    for obs in series.findall(\".//{*}Obs\"):  # Browse the observation in  each series\n",
    "\n",
    "       # Extract relevant attributes\n",
    "        time_period = obs.attrib.get(\"TIME_PERIOD\")\n",
    "        obs_value = obs.attrib.get(\"OBS_VALUE\")\n",
    "         # Add the data at the list\n",
    "        data.append({\"TIME_PERIOD\": time_period, \"OBS_VALUE\": obs_value})\n",
    "\n",
    "\n",
    "df_petrol = pd.DataFrame(data)  # Create a DataFrame from the extracted data\n",
    "\n",
    "# Convert columns to appropriate types\n",
    "df_petrol[\"TIME_PERIOD\"] = pd.to_datetime(df_petrol[\"TIME_PERIOD\"], format=\"%Y-%m\")\n",
    "df_petrol[\"OBS_VALUE\"] = pd.to_numeric(df_petrol[\"OBS_VALUE\"])\n",
    "\n",
    "\n",
    "# Convert TIME_PERIOD to datetime for easier filtering\n",
    "df_petrol['TIME_PERIOD'] = pd.to_datetime(df_petrol['TIME_PERIOD'], format='%Y-%m')\n",
    "\n",
    "# Filter years between 2013 and 2023\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2023-12-31'\n",
    "df_petrol = df_petrol[(df_petrol['TIME_PERIOD'] >= start_date) & (df_petrol['TIME_PERIOD'] <= end_date)]\n",
    "\n",
    "df_petrol.set_index('TIME_PERIOD', inplace=True) #indexes the years\n",
    "df_petrol.index.name = None\n",
    "df_petrol.columns.name = None\n",
    "df_petrol.rename(columns = {'OBS_VALUE': 'Petrol'}, inplace = True)\n",
    "\n",
    "print(ipch.isna().sum().sum()) # Number of missing values (0)\n",
    "# show the 5 first rows\n",
    "df_petrol.head()\n",
    "material = pd.concat((df_petrol, df_or), axis = 1, join = 'inner')\n",
    "material.columns.name = 'Material'\n",
    "material.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action\n",
    "\n",
    "## Présentation de la classe ```Indice``` pour importer les données d'un actif et calculer les indices\n",
    "\n",
    "La classe importe les données depuis YahooFinance.\n",
    "\n",
    "L'argument `ticker_symbol` suffit lors de l'instanciation. Il s'agit du symbole boursier de l'action dont les données seront téléchargées.\\\n",
    "L'argument `data` lors de l'instanciation permet de donner directement les données si elles sont téléchargées.\n",
    "\n",
    "La méthode `update` calcule les différents indices qui seront affichés par la méthode `affichage`. Elle est exécutée lors de l'instanciation.\n",
    "\n",
    "L'historique du prix de l'action est accessible via la méthode `get_data`.\\\n",
    "Les calculs des indices OBV, ADLine, ADX et Aroon sont implémentés et accessibles via la méthode `get_index`.\\\n",
    "\n",
    "## Présentation des indices\n",
    "\n",
    "### OBV\n",
    "Il s'agit d'un indicateur de momentum qui mesure les flux de volume positifs et négatifs. \n",
    "\n",
    "Si la courbe de l'OBV augmente (ou diminue) de façon prononcée, sans changement significatif du prix de l'actif, cela indique qu'à un moment, le prix devrait sauter vers le haut (ou vers le bas).\n",
    "\n",
    "Lorsque les institutions commencent à acheter un actif que les particuliers continuent de vendre, le prix est encore légèrement en baisse ou se stabilise, alors que le volume augmente. Le phénomène inverse se produit également. \n",
    "\n",
    "### ADLine\n",
    "L'ADLine (*Accumulative Distribution Line*) est un indicateur qui mesure le flux d'argent pour un actif en prenant en compte à la fois les variations de prix et les volumes.\n",
    "\n",
    "Une ADLine en hausse indique une pression d'achat accrue, souvent interprétée comme une accumulation de la part des investisseurs, tandis qu'une ADLine en baisse révèle une pression de vente ou une distribution.\n",
    "\n",
    "Une divergence entre l'ADLine et le prix de l'actif peut être utilisée pour anticiper un retournement potentiel de tendance. Par exemple, si le prix monte mais que l'ADLine chute, cela pourrait signaler un affaiblissement de la tendance haussière.\n",
    "\n",
    "### ADX\n",
    "L'ADX identifie une tendance forte lorsqu'il est au-dessus de 25 et une tendance faible lorsqu'il est en-dessous de 20. \\\n",
    "On peut également utiliser le franchissement des lignes $-DI$ et $+DI$ pour générer des signaux de trade: \n",
    "- Lorsque $+DI$ passe au-dessus de $-DI$ et que l'ADX est supérieur à 20 (idéalement à 25), alors il s'agit d'un potentiel signal pour acheter.\n",
    "- Inversement, lorsque $-DI$ passe au-dessus de $+DI$ et que l'ADX est supérieur à 20 (ou 25), il s'agit d'un potentiel signal pour vendre.\n",
    "\n",
    "### Aroon\n",
    "Indique si le prix maximal ou minimal a été atteint depuis longtemps ou non sur les dernières périodes (25 par défaut). Il peut s'agir du prix d'ouverture, de clôture, le prix maximal ou minimal sur la période. S'il est à 100, c'est que le prix maximal a été atteint la veille et que le prix minimal a été atteint avant toutes les périodes étudiées. S'il est à -100 dans le cas contraire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index():\n",
    "    \"\"\"\n",
    "    A class to represent and calculate various financial indices for a given stock.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data(ticker_symbol='AAPL', period='max'):\n",
    "        \"\"\"\n",
    "        Import stock price history.\n",
    "        \n",
    "        IN: ticker_symbol: <str> Stock identifier\n",
    "            period: <str> Period over which data is downloaded\n",
    "                    arg: '1d', '5d', '1mo', '3mo', '6mo', '1y', '2y', '5y', '10y', 'ytd', 'max'\n",
    "        OUT: <pd.Series>: Stock history\n",
    "        \"\"\"\n",
    "        ticker = yf.Ticker(ticker_symbol)  # Create a Ticker object\n",
    "        data_hist = ticker.history(period=period)  # Download historical data\n",
    "        dates = pd.Series(data_hist['Open'].index).apply(lambda x: pd.to_datetime(x.strftime('%Y-%m-%d')))  # Convert dates\n",
    "        data = data_hist.reset_index(drop=True).set_index(dates).drop(['Dividends', 'Stock Splits'], axis=1)  # Prepare data\n",
    "        data.index.name = None\n",
    "        return data\n",
    "\n",
    "    @classmethod\n",
    "    def smooth(cls, series, period=14, method='simple', fill='NoFill', alpha=1/14):\n",
    "        \"\"\"\n",
    "        Calculate moving average.\n",
    "        \n",
    "        IN: series: <Pandas Series> Series for which the moving average is calculated\n",
    "            period: <int> Number of periods used for the moving average\n",
    "            method: <str> Method used for calculating the average\n",
    "                    'simple': Calculate arithmetic average\n",
    "                    'exp': Calculate exponential average\n",
    "                    'weight': Assign increasing weights to series and calculate arithmetic average\n",
    "            fill: <str> If the first period values are initialized or left empty (fill='NoFill')\n",
    "                  'constant': Fill with the first non-null value (at position period)\n",
    "                  'data': Fill with the first period values of the series\n",
    "                  'smooth': Fill value i with the moving average of the first i+1 values of the series over a period of i+1, \n",
    "                            for i from 1 to period - 1. The same method of calculating the average is used.\n",
    "            alpha: <int> Argument for calculating the average by the 'exp' method. Must be between 0 and 1.\n",
    "        OUT: <Pandas Series>: Smoothed series\n",
    "        \"\"\"\n",
    "        match method:\n",
    "            case 'simple':\n",
    "                smoothed = series.rolling(period).sum().copy() / period  # Simple moving average\n",
    "            case 'exp':\n",
    "                smoothed = [series.iloc[0]]  # Initialize with the first value\n",
    "                for i in range(1, len(series)):\n",
    "                    smoothed += [alpha * series.iloc[i] + (1 - alpha) * smoothed[i - 1]]  # Exponential moving average\n",
    "                smoothed = pd.Series(smoothed)\n",
    "            case 'weight':\n",
    "                weight = np.array([k for k in range(1, period + 1)])  # Weights for weighted moving average\n",
    "                smoothed = series.rolling(period).apply(lambda x: np.dot(x, weight).sum() / weight.sum())  # Weighted moving average\n",
    "        \n",
    "        match fill:\n",
    "            case 'constant':\n",
    "                smoothed.iloc[:period] = smoothed.iloc[period - 1]  # Fill with constant value\n",
    "            case 'data':\n",
    "                smoothed.iloc[:period - 1] = series.iloc[:period - 1]  # Fill with initial data values\n",
    "            case 'smooth':\n",
    "                smoothed.iloc[0] = series.iloc[0]  # Initialize with the first value\n",
    "                for i in range(1, period):\n",
    "                    smoothed.iloc[i] = cls.smooth(series=series.iloc[:i + 1], period=i + 1, method=method, fill='NoFill').iloc[i]  # Smooth fill\n",
    "            case 'NoFill':\n",
    "                pass  # No fill\n",
    "\n",
    "        return smoothed\n",
    "\n",
    "    def __init__(self, name = 'Apple', ticker_symbol='AAPL', data=False) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Index class.\n",
    "        \n",
    "        IN: ticker_symbol: <str> Stock ticker symbol to study\n",
    "            data: <bool or Pandas DataFrame> If data is False, the history will be downloaded from Yahoo Finance. \n",
    "                                             Otherwise, data must contain the data downloaded from Yahoo Finance and \n",
    "                                             transformed as in the static method load_data.\n",
    "        \"\"\"\n",
    "        self.__name = name\n",
    "        self.__TICKER_SYMBOL = ticker_symbol  # Stock ticker symbol\n",
    "        self.__data = None  # Data placeholder\n",
    "        self.__index = None  # Index placeholder\n",
    "        self.__date_limits = {'Start': None, 'End': None}  # Date limits\n",
    "\n",
    "        self.__start(data)  # Initialize data\n",
    "\n",
    "    def __start(self, data):\n",
    "        \"\"\"Start the data initialization.\"\"\"\n",
    "        if data is not False:\n",
    "            self.__data = data  # Use provided data\n",
    "        else:\n",
    "            self.__data = self.load_data(ticker_symbol = self.__TICKER_SYMBOL)  # Load data from Yahoo Finance\n",
    "\n",
    "        self.__index = pd.DataFrame(index = self.__data.index)  # Initialize index DataFrame\n",
    "        self.__date_limits['Start'] = self.__data.index[0]  # Set start date\n",
    "        self.__date_limits['End'] = self.__data.index[-1]  # Set end date\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.__name\n",
    " \n",
    "    def get_data(self):\n",
    "        \"\"\"Return the data.\"\"\"\n",
    "        monthly_dates = pd.date_range(start=self.__data.index.values[0], end=self.__data.index.values[-1], freq='MS')\n",
    "\n",
    "        # Add columns for each month from 2013 to 2023\n",
    "        self.__data = self.__data.reindex(index=self.__data.index.union(monthly_dates))\n",
    "\n",
    "        def fill_val(x):\n",
    "            \"\"\"Fill missing values by resampling and interpolating.\"\"\"\n",
    "            return x.resample('MS').interpolate(method='quadratic')\n",
    "\n",
    "        # Apply the fill_val function to each row\n",
    "        self.__data = self.__data.apply(func=fill_val, axis=0)\n",
    "\n",
    "        return self.__data[self.__data.index.is_month_start]\n",
    "    \n",
    "    def get_index(self):\n",
    "        \"\"\"Return the index.\"\"\"\n",
    "        monthly_dates = pd.date_range(start=self.__index.index.values[0], end=self.__index.index.values[-1], freq='MS')\n",
    "\n",
    "        # Add columns for each month from 2013 to 2023\n",
    "        self.__index = self.__index.reindex(index=self.__index.index.union(monthly_dates))\n",
    "\n",
    "        def fill_val(x):\n",
    "            \"\"\"Fill missing values by resampling and interpolating.\"\"\"\n",
    "            return x.resample('MS').interpolate(method='quadratic')\n",
    "\n",
    "        # Apply the fill_val function to each row\n",
    "        self.__index = self.__index.apply(func=fill_val, axis=0)\n",
    "\n",
    "        return self.__index[self.__index.index.is_month_start]\n",
    "   \n",
    "    def update(self, **kwargs):\n",
    "        \"\"\"Update the index with calculated indicators.\"\"\"\n",
    "        self.__index = pd.DataFrame(index=self.__data.index)  # Reset index DataFrame\n",
    "        self.OBV(**kwargs)\n",
    "        self.ADLine(**kwargs)\n",
    "        self.ADX(**kwargs)\n",
    "        self.Aroon(**kwargs)\n",
    "    \n",
    "    def add(self, indicator, dates, **kwargs):\n",
    "        \"\"\"Add a specific indicator to the index.\"\"\"\n",
    "        match indicator:\n",
    "            case 'C': return self.C(dates=dates, **kwargs)  # Add indicator 'C'\n",
    "\n",
    "    # Display methods\n",
    "    def display(self, start_date, end_date, list_indices, **kwargs):\n",
    "        \"\"\"\n",
    "        Display the graphs of the specified indices in list_indices.\n",
    "        \n",
    "        IN: start_date, end_date: <str> Start and end dates for displaying the indices\n",
    "            list_indices: <list of str> List of indices in string format to display. \n",
    "                         Multiple lists will give multiple graphs. \n",
    "                         Different indices within a single list will be displayed on the same graph.\n",
    "                         Index names: 'OBV', 'ADLine', 'ADX'\n",
    "            **kwargs: Arguments to pass to the index calculation methods\n",
    "        \"\"\"\n",
    "        self.update(**kwargs)  # Update indices\n",
    "\n",
    "        # Convert start and end date\n",
    "        start = pd.to_datetime(start_date)\n",
    "        end = pd.to_datetime(end_date)\n",
    "\n",
    "        if start < self.__date_limits['Start']: start = self.__date_limits['Start']  # Adjust start date\n",
    "        else: pass\n",
    "        if end > self.__date_limits['End']: end = self.__date_limits['End']  # Adjust end date\n",
    "        else: pass\n",
    "\n",
    "        data = self.__index.loc[start:end]  # Filter data by date range\n",
    "        x = pd.Series(data.index).apply(lambda x: x.strftime('%d/%m/%y'))  # Format dates for x-axis\n",
    "\n",
    "        fig, ax = plt.subplots(len(list_indices), 1, figsize=(20, 3 * len(list_indices)))  # Create subplots\n",
    "        fig.suptitle(f'Period {start.strftime(\"%d/%m/%y\")} - {end.strftime(\"%d/%m/%y\")}')  # Set title\n",
    "        for i, indices in enumerate(list_indices):\n",
    "            for indicator in indices:\n",
    "                if indicator in data:\n",
    "                    y = data[indicator]  # Get data for the index if present\n",
    "                else:\n",
    "                    y = self.__add(indicator, dates=data.index, **kwargs)  # Add index data if not present\n",
    "                style = kwargs.get(f'{indicator}_style', '')  # Get style for the index\n",
    "                ax[i].plot(x, y, style, label=indicator)  # Plot the index\n",
    "            ax[i].legend()\n",
    "            ax[i].set_xticks(ticks=x.iloc[np.linspace(0, 1, 20) * (len(x) - 1)])\n",
    "    \n",
    "    # Index calculation methods\n",
    "    def OBV(self, start='Close', end=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Calculate the On-Balance Volume (OBV) indicator.\n",
    "        \n",
    "        IN: start, end: <str> Among 'Open' and 'Close', the directions for adding volumes will be calculated according \n",
    "                        to the opening price ('Open') or closing price ('Close') of day n for the start and day n+1 for the end.\n",
    "            **kwargs: Exists for compatibility\n",
    "        OUT: <Pandas Series>: OBV indicator\n",
    "        \"\"\"\n",
    "        if start == 'Open' and end == 'Close':\n",
    "            direction = self.__data[end] - self.__data[start]  # Calculate direction based on Open and Close\n",
    "        elif end:\n",
    "            direction = self.__data[end] - self.__data[start].shift(1)  # Calculate direction based on shifted Close\n",
    "        else: direction = self.__data[start] - self.__data[start].shift(1)  # Calculate direction based on shifted Open\n",
    "\n",
    "        direction.iloc[0] = 1  # Set initial direction\n",
    "        OBV = (self.__data['Volume'] * direction / abs(direction)).cumsum()  # Calculate OBV\n",
    "        self.__index['OBV'] = np.where(np.isnan(OBV), OBV.shift(1), OBV)  # Handle NaN values and store OBV\n",
    "\n",
    "        return self.__index['OBV']\n",
    "        \n",
    "    def ADLine(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Calculate the Accumulation/Distribution Line (ADLine) indicator.\n",
    "        \n",
    "        IN: **kwargs: Exists for compatibility\n",
    "        OUT: <Pandas Series>: ADLine indicator\n",
    "        \"\"\"\n",
    "        MFM = ((self.__data['High'] - 2 * self.__data['Close'] + self.__data['Low'])) / \\\n",
    "              (self.__data['Low'] - self.__data['High'])  # Money Flow Multiplier\n",
    "        MFV = MFM * self.__data['Volume']  # Money Flow Volume\n",
    "\n",
    "        self.__index['ADLine'] = MFV.cumsum()  # Calculate and store ADLine\n",
    "\n",
    "        return self.__index['ADLine']\n",
    "    \n",
    "    def ADX(self, period_ADX=14, method='exp', fill='NoFill', alpha=1/14, **kwargs):\n",
    "        \"\"\"\n",
    "        Calculate the Average Directional Index (ADX) indicator.\n",
    "        \n",
    "        IN: period_ADX: <int> Number of periods used for the ADX calculation\n",
    "            method: <str> Method used for calculating the average\n",
    "                    'simple': Calculate arithmetic average\n",
    "                    'exp': Calculate exponential average\n",
    "                    'weight': Assign increasing weights to series and calculate arithmetic average\n",
    "            fill: <str> If the first period values are initialized or left empty (fill='NoFill')\n",
    "                  'constant': Fill with the first non-null value (at position period)\n",
    "                  'data': Fill with the first period values of the series\n",
    "                  'smooth': Fill value i with the moving average of the first i+1 values of the series over a period of i+1, \n",
    "                            for i from 1 to period - 1. The same method of calculating the average is used.\n",
    "            alpha: <int> Argument for calculating the average by the 'exp' method. Must be between 0 and 1.\n",
    "            **kwargs: Exists for compatibility\n",
    "        OUT: <Pandas Series>: ADX indicator\n",
    "        \"\"\"\n",
    "        DMp = np.where(self.__data['High'] - self.__data['High'].shift(1) > self.__data['Low'].shift(1) - self.__data['Low'],\n",
    "                       self.__data['High'] - self.__data['High'].shift(1), 0)  # Positive Directional Movement\n",
    "        DMm = np.where(self.__data['High'] - self.__data['High'].shift(1) <= self.__data['Low'].shift(1) - self.__data['Low'],\n",
    "                       self.__data['Low'].shift(1) - self.__data['Low'], 0)  # Negative Directional Movement\n",
    "\n",
    "        TR = pd.DataFrame([self.__data['High'] - self.__data['Low'], \n",
    "                           self.__data['High'] - self.__data['Close'].shift(1), \n",
    "                           self.__data['Close'].shift(1) - self.__data['Low']]).apply(max)  # Set True Range\n",
    "\n",
    "        # Smoothed Positive DM, Negative DM and True Range\n",
    "        DMpsmooth = self.smooth(series=pd.Series(DMp), period=period_ADX, method=method, fill=fill, alpha=alpha)\n",
    "        DMmsmooth = self.smooth(series=pd.Series(DMm), period=period_ADX, method=method, fill=fill, alpha=alpha)\n",
    "        TRsmooth = self.smooth(series=pd.Series(TR), period=period_ADX, method=method, fill=fill, alpha=alpha)\n",
    "\n",
    "        DIp = 100 * DMpsmooth / TRsmooth  # Positive Directional Indicator\n",
    "        DIm = 100 * DMmsmooth / TRsmooth  # Negative Directional Indicator\n",
    "\n",
    "        DX = 100 * (DIp - DIm) / (DIp + DIm)  # Directional Movement Index\n",
    "        DX.iloc[0] = 0  # Set initial DX\n",
    "        ADX = self.smooth(DX, period=period_ADX, method=method, fill=fill, alpha=alpha)  # Average Directional Index\n",
    "\n",
    "        # Set index for ADX, DIp, DIm\n",
    "        ADX.index = self.__index.index\n",
    "        DIp.index = self.__index.index\n",
    "        DIm.index = self.__index.index\n",
    "        \n",
    "        # Store ADX, Positive DI and Negative DI in index\n",
    "        self.__index['ADX'] = ADX\n",
    "        self.__index['pDI'] = DIp\n",
    "        self.__index['mDI'] = DIm\n",
    "        \n",
    "        return self.__index['ADX']\n",
    "    \n",
    "    def Aroon(self, period_Aroon=25, event='Close', **kwargs):\n",
    "        \"\"\"\n",
    "        Calculate the Aroon indicator.\n",
    "        \n",
    "        IN: period_Aroon: <int> Number of periods over which the index is calculated\n",
    "            event: <str> Among 'Open', 'Close', 'Low', and 'High', the Aroon index will be calculated based on the variations of these events.\n",
    "            **kwargs: Exists for compatibility\n",
    "        OUT: <Pandas Series>: Aroon indicator\n",
    "        \"\"\"\n",
    "        def indMin(i):\n",
    "            if i < period_Aroon: start, end = 0, max(i, 1)\n",
    "            else: start, end = i - period_Aroon, i\n",
    "            return np.argmin(self.__data[event].iloc[start: end])  # Index of minimum value\n",
    "        def indMax(i):\n",
    "            if i < period_Aroon: start, end = 0, max(i, 1)\n",
    "            else: start, end = i - period_Aroon, i\n",
    "            return np.argmax(self.__data[event].iloc[start: end])  # Index of maximum value\n",
    "\n",
    "        # Calculate argMin, argMax and Aroon indicator for each period\n",
    "        argMin = pd.Series(range(len(self.__data))).apply(indMin)\n",
    "        argMax = pd.Series(range(len(self.__data))).apply(indMax)\n",
    "        Aroon = 100 * (argMin - argMax) / period_Aroon\n",
    "        \n",
    "        # Store Aroon in index\n",
    "        Aroon.index = self.__index.index\n",
    "        self.__index['Aroon'] = Aroon\n",
    "\n",
    "        return self.__index['Aroon']\n",
    "    \n",
    "    # Other lines\n",
    "    def C(self, dates, c=25):\n",
    "        \"\"\"\n",
    "        Generate a horizontal curve.\n",
    "        \n",
    "        IN: dates: <Pandas Index> Dates for the curve\n",
    "            c: <int> Height of the curve\n",
    "        OUT: <Pandas Series>: Horizontal curve\n",
    "        \"\"\"\n",
    "        return pd.Series(range(len(self.__index.index)), index=dates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection d'action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_entreprise = {\n",
    "    # Entreprises du CAC 40\n",
    "    \"Accor\": \"AC.PA\",\n",
    "    \"Air Liquide\": \"AI.PA\",\n",
    "    \"ArcelorMittal\": \"MT.AS\",\n",
    "    \"BNP Paribas\": \"BNP.PA\",\n",
    "    \n",
    "    # Entreprises défensives\n",
    "    \"Nestlé\": \"NESN.SW\",\n",
    "    \"Sanofi\": \"SAN.PA\",\n",
    "    \"Novo Nordisk\": \"NOVO-B.CO\",\n",
    "    \"GlaxoSmithKline\": \"GSK.L\",\n",
    "    \n",
    "    # Entreprises cycliques\n",
    "    \"Volkswagen\": \"VOW3.DE\",\n",
    "    \"BMW\": \"BMW.DE\",\n",
    "    \"LVMH\": \"MC.PA\",\n",
    "    \"Hermès\": \"RMS.PA\",\n",
    "    \n",
    "    # Entreprises de valeur\n",
    "    \"TotalEnergies\": \"TTE.PA\",\n",
    "    \"Schneider Electric\": \"SU.PA\",\n",
    "    \"Airbus\": \"AIR.PA\",\n",
    "    \"L'Oréal\": \"OR.PA\",\n",
    "    \n",
    "    # Entreprises avec corrélations probables avec des indices macro-économiques ou techniques\n",
    "    \"SAP\": \"SAP.DE\",\n",
    "    \"ASML\": \"ASML.AS\",\n",
    "    \"Siemens\": \"SIE.DE\",\n",
    "    \"Danone\": \"BN.PA\",\n",
    "    \"Kering\": \"KER.PA\",\n",
    "    \"Orange\": \"ORA.PA\",\n",
    "    \"Publicis\": \"PUB.PA\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/stow/conda/miniforge3/envs/cremi/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: RuntimeWarning: invalid value encountered in accumulate\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Air Liquide AI.PA\n",
      "Publicis PUB.PA\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty DataFrames for historical actions and indices\n",
    "hist_action = pd.DataFrame()\n",
    "hist_index = pd.DataFrame()\n",
    "\n",
    "# Iterate over each company in the dictionary\n",
    "for company, ticker in list_entreprise.items():\n",
    "    # Instantiate an Index object for the company\n",
    "    index_obj = Index(name=company, ticker_symbol=ticker)\n",
    "    \n",
    "    # Update the index to calculate technical indices\n",
    "    index_obj.update()\n",
    "    \n",
    "    # Get the historical action data\n",
    "    action_data = index_obj.get_data()\n",
    "    \n",
    "    # Create a MultiIndex for the columns in the format ('Company', 'Price')\n",
    "    action_data.columns = pd.MultiIndex.from_product([[company], action_data.columns], names=['Company', 'Price'])\n",
    "    action_data.set_index(action_data.index, inplace=True)  # Ensure the index remains the DateTimeIndex\n",
    "    \n",
    "    # Concatenate the historical action data into hist_action\n",
    "    hist_action = pd.concat([hist_action, action_data], axis=1)\n",
    "    \n",
    "    # Get the historical index data\n",
    "    index_data = index_obj.get_index()\n",
    "    \n",
    "    # Create a MultiIndex for the columns in the format ('Company', 'Index')\n",
    "    index_data.columns = pd.MultiIndex.from_product([[company], index_data.columns], names=['Company', 'Index'])\n",
    "    index_data.set_index(index_data.index, inplace=True)  # Ensure the index remains the DateTimeIndex\n",
    "    \n",
    "    # Concatenate the index data into hist_index\n",
    "    hist_index = pd.concat([hist_index, index_data], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Company</th>\n",
       "      <th colspan=\"5\" halign=\"left\">Accor</th>\n",
       "      <th colspan=\"5\" halign=\"left\">Air Liquide</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"5\" halign=\"left\">Orange</th>\n",
       "      <th colspan=\"5\" halign=\"left\">Publicis</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>...</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-01-01</th>\n",
       "      <td>4.646613</td>\n",
       "      <td>4.646613</td>\n",
       "      <td>4.646613</td>\n",
       "      <td>4.646613</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>15.310338</td>\n",
       "      <td>15.310338</td>\n",
       "      <td>15.310338</td>\n",
       "      <td>15.310338</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>11.522519</td>\n",
       "      <td>11.522519</td>\n",
       "      <td>11.522519</td>\n",
       "      <td>11.522519</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>15.785186</td>\n",
       "      <td>15.785186</td>\n",
       "      <td>15.785186</td>\n",
       "      <td>15.785186</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-02-01</th>\n",
       "      <td>4.734243</td>\n",
       "      <td>4.734243</td>\n",
       "      <td>4.618163</td>\n",
       "      <td>4.648890</td>\n",
       "      <td>1.148837e+06</td>\n",
       "      <td>15.514610</td>\n",
       "      <td>15.611880</td>\n",
       "      <td>15.300615</td>\n",
       "      <td>15.417339</td>\n",
       "      <td>2.017343e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>9.572158</td>\n",
       "      <td>9.610653</td>\n",
       "      <td>8.943424</td>\n",
       "      <td>8.971653</td>\n",
       "      <td>6.914240e+06</td>\n",
       "      <td>14.936230</td>\n",
       "      <td>15.413765</td>\n",
       "      <td>14.936230</td>\n",
       "      <td>15.328870</td>\n",
       "      <td>484905.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-03-01</th>\n",
       "      <td>4.830975</td>\n",
       "      <td>4.895843</td>\n",
       "      <td>4.795696</td>\n",
       "      <td>4.862840</td>\n",
       "      <td>1.319025e+06</td>\n",
       "      <td>15.923143</td>\n",
       "      <td>16.370586</td>\n",
       "      <td>15.757784</td>\n",
       "      <td>16.253862</td>\n",
       "      <td>1.923018e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>7.698786</td>\n",
       "      <td>7.916919</td>\n",
       "      <td>7.698786</td>\n",
       "      <td>7.852762</td>\n",
       "      <td>4.716427e+06</td>\n",
       "      <td>16.161898</td>\n",
       "      <td>16.660657</td>\n",
       "      <td>16.077003</td>\n",
       "      <td>16.453726</td>\n",
       "      <td>393082.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-04-01</th>\n",
       "      <td>5.120037</td>\n",
       "      <td>5.120037</td>\n",
       "      <td>5.120037</td>\n",
       "      <td>5.120037</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>16.020416</td>\n",
       "      <td>16.020416</td>\n",
       "      <td>16.020416</td>\n",
       "      <td>16.020416</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>8.904932</td>\n",
       "      <td>8.904932</td>\n",
       "      <td>8.904932</td>\n",
       "      <td>8.904932</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>20.390736</td>\n",
       "      <td>20.390736</td>\n",
       "      <td>20.390736</td>\n",
       "      <td>20.390736</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-05-01</th>\n",
       "      <td>5.105242</td>\n",
       "      <td>5.105242</td>\n",
       "      <td>5.105242</td>\n",
       "      <td>5.105242</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>16.711031</td>\n",
       "      <td>16.711031</td>\n",
       "      <td>16.711031</td>\n",
       "      <td>16.711031</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>6.918643</td>\n",
       "      <td>6.918643</td>\n",
       "      <td>6.918643</td>\n",
       "      <td>6.918643</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>17.987152</td>\n",
       "      <td>17.987152</td>\n",
       "      <td>17.987152</td>\n",
       "      <td>17.987152</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-01</th>\n",
       "      <td>36.300782</td>\n",
       "      <td>36.237238</td>\n",
       "      <td>34.918575</td>\n",
       "      <td>35.264210</td>\n",
       "      <td>1.248972e+06</td>\n",
       "      <td>172.543858</td>\n",
       "      <td>172.457424</td>\n",
       "      <td>169.212728</td>\n",
       "      <td>169.877726</td>\n",
       "      <td>7.082982e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>10.128504</td>\n",
       "      <td>10.178257</td>\n",
       "      <td>10.091507</td>\n",
       "      <td>10.219860</td>\n",
       "      <td>5.506408e+06</td>\n",
       "      <td>97.115961</td>\n",
       "      <td>97.069237</td>\n",
       "      <td>95.096323</td>\n",
       "      <td>95.316574</td>\n",
       "      <td>609924.483516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01</th>\n",
       "      <td>39.169998</td>\n",
       "      <td>39.230000</td>\n",
       "      <td>37.849998</td>\n",
       "      <td>38.320000</td>\n",
       "      <td>1.122141e+06</td>\n",
       "      <td>173.720001</td>\n",
       "      <td>173.820007</td>\n",
       "      <td>169.839996</td>\n",
       "      <td>170.940002</td>\n",
       "      <td>6.621300e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>10.015405</td>\n",
       "      <td>10.059056</td>\n",
       "      <td>9.957205</td>\n",
       "      <td>10.020255</td>\n",
       "      <td>7.110345e+06</td>\n",
       "      <td>98.580002</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>96.779999</td>\n",
       "      <td>96.879997</td>\n",
       "      <td>697233.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-01</th>\n",
       "      <td>41.520000</td>\n",
       "      <td>41.919998</td>\n",
       "      <td>41.279999</td>\n",
       "      <td>41.849998</td>\n",
       "      <td>3.448940e+05</td>\n",
       "      <td>164.880005</td>\n",
       "      <td>166.759995</td>\n",
       "      <td>164.279999</td>\n",
       "      <td>166.300003</td>\n",
       "      <td>4.285810e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>9.753501</td>\n",
       "      <td>9.860202</td>\n",
       "      <td>9.753501</td>\n",
       "      <td>9.758351</td>\n",
       "      <td>4.762588e+06</td>\n",
       "      <td>97.800003</td>\n",
       "      <td>98.059998</td>\n",
       "      <td>97.480003</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>254459.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>277 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Company         Accor                                                 \\\n",
       "Price            Open       High        Low      Close        Volume   \n",
       "2002-01-01   4.646613   4.646613   4.646613   4.646613  0.000000e+00   \n",
       "2002-02-01   4.734243   4.734243   4.618163   4.648890  1.148837e+06   \n",
       "2002-03-01   4.830975   4.895843   4.795696   4.862840  1.319025e+06   \n",
       "2002-04-01   5.120037   5.120037   5.120037   5.120037  0.000000e+00   \n",
       "2002-05-01   5.105242   5.105242   5.105242   5.105242  0.000000e+00   \n",
       "...               ...        ...        ...        ...           ...   \n",
       "2024-09-01  36.300782  36.237238  34.918575  35.264210  1.248972e+06   \n",
       "2024-10-01  39.169998  39.230000  37.849998  38.320000  1.122141e+06   \n",
       "2024-11-01  41.520000  41.919998  41.279999  41.849998  3.448940e+05   \n",
       "2024-12-01        NaN        NaN        NaN        NaN           NaN   \n",
       "2025-01-01        NaN        NaN        NaN        NaN           NaN   \n",
       "\n",
       "Company    Air Liquide                                                    ...  \\\n",
       "Price             Open        High         Low       Close        Volume  ...   \n",
       "2002-01-01   15.310338   15.310338   15.310338   15.310338  0.000000e+00  ...   \n",
       "2002-02-01   15.514610   15.611880   15.300615   15.417339  2.017343e+06  ...   \n",
       "2002-03-01   15.923143   16.370586   15.757784   16.253862  1.923018e+06  ...   \n",
       "2002-04-01   16.020416   16.020416   16.020416   16.020416  0.000000e+00  ...   \n",
       "2002-05-01   16.711031   16.711031   16.711031   16.711031  0.000000e+00  ...   \n",
       "...                ...         ...         ...         ...           ...  ...   \n",
       "2024-09-01  172.543858  172.457424  169.212728  169.877726  7.082982e+05  ...   \n",
       "2024-10-01  173.720001  173.820007  169.839996  170.940002  6.621300e+05  ...   \n",
       "2024-11-01  164.880005  166.759995  164.279999  166.300003  4.285810e+05  ...   \n",
       "2024-12-01         NaN         NaN         NaN         NaN           NaN  ...   \n",
       "2025-01-01         NaN         NaN         NaN         NaN           NaN  ...   \n",
       "\n",
       "Company        Orange                                                 \\\n",
       "Price            Open       High        Low      Close        Volume   \n",
       "2002-01-01  11.522519  11.522519  11.522519  11.522519  0.000000e+00   \n",
       "2002-02-01   9.572158   9.610653   8.943424   8.971653  6.914240e+06   \n",
       "2002-03-01   7.698786   7.916919   7.698786   7.852762  4.716427e+06   \n",
       "2002-04-01   8.904932   8.904932   8.904932   8.904932  0.000000e+00   \n",
       "2002-05-01   6.918643   6.918643   6.918643   6.918643  0.000000e+00   \n",
       "...               ...        ...        ...        ...           ...   \n",
       "2024-09-01  10.128504  10.178257  10.091507  10.219860  5.506408e+06   \n",
       "2024-10-01  10.015405  10.059056   9.957205  10.020255  7.110345e+06   \n",
       "2024-11-01   9.753501   9.860202   9.753501   9.758351  4.762588e+06   \n",
       "2024-12-01        NaN        NaN        NaN        NaN           NaN   \n",
       "2025-01-01        NaN        NaN        NaN        NaN           NaN   \n",
       "\n",
       "Company      Publicis                                                  \n",
       "Price            Open       High        Low      Close         Volume  \n",
       "2002-01-01  15.785186  15.785186  15.785186  15.785186       0.000000  \n",
       "2002-02-01  14.936230  15.413765  14.936230  15.328870  484905.000000  \n",
       "2002-03-01  16.161898  16.660657  16.077003  16.453726  393082.000000  \n",
       "2002-04-01  20.390736  20.390736  20.390736  20.390736       0.000000  \n",
       "2002-05-01  17.987152  17.987152  17.987152  17.987152       0.000000  \n",
       "...               ...        ...        ...        ...            ...  \n",
       "2024-09-01  97.115961  97.069237  95.096323  95.316574  609924.483516  \n",
       "2024-10-01  98.580002  99.000000  96.779999  96.879997  697233.000000  \n",
       "2024-11-01  97.800003  98.059998  97.480003  98.000000  254459.000000  \n",
       "2024-12-01        NaN        NaN        NaN        NaN            NaN  \n",
       "2025-01-01        NaN        NaN        NaN        NaN            NaN  \n",
       "\n",
       "[277 rows x 115 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_action.loc['2002':, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion des dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toutes les dates du 01/01/2013       au 01/01/2023 sont présentes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3306940/3471759507.py:40: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n",
      "  data = data.groupby('Type', axis = 1)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m hist_index \u001b[38;5;241m=\u001b[39m hist_index[hist_index\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39misin(common_dates)]\n\u001b[1;32m     39\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat((dette_publique, chomage, ipch, pib), axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, join \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mType\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/local/stow/conda/miniforge3/envs/cremi/lib/python3.12/site-packages/pandas/core/frame.py:9183\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   9180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   9181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 9183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9186\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9189\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/local/stow/conda/miniforge3/envs/cremi/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/local/stow/conda/miniforge3/envs/cremi/lib/python3.12/site-packages/pandas/core/groupby/grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Type'"
     ]
    }
   ],
   "source": [
    "common_countries = ipch.columns.intersection(\\\n",
    "                   dette_publique.columns.intersection(\\\n",
    "                   chomage.columns.intersection(\\\n",
    "                    pib.columns)))\n",
    "\n",
    "common_dates = ipch.index.intersection(\\\n",
    "                   dette_publique.index.intersection(\\\n",
    "                   chomage.index.intersection(\\\n",
    "                    pib.index)))\n",
    "\n",
    "expected_dates = pd.date_range(start=common_dates[0], end=common_dates[-1], freq='MS')\n",
    "all_dates_present = expected_dates.isin(common_dates).all()\n",
    "\n",
    "print(f'Toutes les dates du {common_dates[0].strftime('%d/%m/%Y')}\\\n",
    "       au {common_dates[-1].strftime('%d/%m/%Y')} sont présentes' \\\n",
    "        if all_dates_present else 'Il manque des dates')\n",
    "\n",
    "def set_index(df, index_name):\n",
    "    df = df.loc[common_dates, common_countries]\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # Ajouter le nouvel index au niveau supérieur du MultiIndex existant\n",
    "        new_index = pd.MultiIndex.from_tuples([(index_name, *idx) \\\n",
    "                        for idx in df.columns], names=['Type'] + df.columns.names)\n",
    "    else:\n",
    "        # Créer un MultiIndex en juxtaposant le nouvel index et l'index existant\n",
    "        new_index = pd.MultiIndex.from_tuples([(index_name, idx) \n",
    "                        for idx in df.columns], names=['Type', df.columns.names[0]])\n",
    "    df.columns = new_index\n",
    "    return df\n",
    "# dette_publique = set_index(dette_publique, 'Dette publique')\n",
    "# chomage = set_index(chomage, 'Chomage')\n",
    "# ipch = set_index(ipch, 'IPCH')\n",
    "# pib = set_index(pib, 'PIB')\n",
    "\n",
    "hist_action = hist_action[hist_action.index.isin(common_dates)]\n",
    "hist_index = hist_index[hist_index.index.isin(common_dates)]\n",
    "\n",
    "\n",
    "data = pd.concat((dette_publique, chomage, ipch, pib), axis = 1, join = 'inner')\n",
    "data = data.groupby('Type', axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichage\n",
    "\n",
    "L'affichage des graphiques se fait trois fois, je ne sais pas pourquoi.\\\n",
    "Pour l'affichage de la carte, j'ai commencé à coder une fonction tout en bas. Il manque la traduction des noms des pays. Je ne sais pas si ça va fonctionner après ça, à voir.\n",
    "\n",
    "## Fonction d'affichage de graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(countries, start, end, data, data_type):\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    for country in countries.split(', '):\n",
    "        data.loc[start:end, (data_type, country)].plot(label=f'{country}')\n",
    "\n",
    "    plt.title(f'{data_type} ({start.strftime(\"%m/%Y\")} - {end.strftime(\"%m/%Y\")})')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction d'affichage de carte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GeoJSON file into a GeoDataFrame\n",
    "url = \"https://raw.githubusercontent.com/leakyMirror/map-of-europe/master/GeoJSON/europe.geojson\"\n",
    "europe = gpd.read_file(url)\n",
    "\n",
    "def plot_map(date: str, data: pd.core.groupby.DataFrameGroupBy, data_type: str):\n",
    "    \"\"\"\n",
    "    Plot a map of Europe for the specified date and data type.\n",
    "    \n",
    "    Args:\n",
    "        date (str): Date in 'YYYY-MM' format for which to filter the data.\n",
    "        data (pd.core.groupby.DataFrameGroupBy): A grouped DataFrame with MultiIndex columns\n",
    "            ('Type', 'Country') and a DateTimeIndex.\n",
    "        data_type (str): The type of data to display (e.g., 'IPCH').\n",
    "    \"\"\"\n",
    "    # Prepare the data\n",
    "    df = data.get_group(data_type).copy()\n",
    "    df.columns = df.columns.droplevel('Type')\n",
    "    df.rename(columns={\n",
    "        \"Albanie\": \"Albania\",\n",
    "        \"Allemagne\": \"Germany\",\n",
    "        \"Andorre\": \"Andorra\",\n",
    "        \"Autriche\": \"Austria\",\n",
    "        \"Belgique\": \"Belgium\",\n",
    "        \"Biélorussie\": \"Belarus\",\n",
    "        \"Bosnie-Herzégovine\": \"Bosnia and Herzegovina\",\n",
    "        \"Bulgarie\": \"Bulgaria\",\n",
    "        \"Croatie\": \"Croatia\",\n",
    "        \"Danemark\": \"Denmark\",\n",
    "        \"Espagne\": \"Spain\",\n",
    "        \"Estonie\": \"Estonia\",\n",
    "        \"Finlande\": \"Finland\",\n",
    "        \"France\": \"France\",\n",
    "        \"Grèce\": \"Greece\",\n",
    "        \"Hongrie\": \"Hungary\",\n",
    "        \"Irlande\": \"Ireland\",\n",
    "        \"Islande\": \"Iceland\",\n",
    "        \"Italie\": \"Italy\",\n",
    "        \"Kosovo\": \"Kosovo\",\n",
    "        \"Lettonie\": \"Latvia\",\n",
    "        \"Liechtenstein\": \"Liechtenstein\",\n",
    "        \"Lituanie\": \"Lithuania\",\n",
    "        \"Luxembourg\": \"Luxembourg\",\n",
    "        \"Malte\": \"Malta\",\n",
    "        \"Moldavie\": \"Moldova\",\n",
    "        \"Monaco\": \"Monaco\",\n",
    "        \"Monténégro\": \"Montenegro\",\n",
    "        \"Norvège\": \"Norway\",\n",
    "        \"Pays-Bas\": \"Netherlands\",\n",
    "        \"Pologne\": \"Poland\",\n",
    "        \"Portugal\": \"Portugal\",\n",
    "        \"République tchèque\": \"Czech Republic\",\n",
    "        \"Roumanie\": \"Romania\",\n",
    "        \"Royaume-Uni\": \"United Kingdom\",\n",
    "        \"Russie\": \"Russia\",\n",
    "        \"Saint-Marin\": \"San Marino\",\n",
    "        \"Serbie\": \"Serbia\",\n",
    "        \"Slovaquie\": \"Slovakia\",\n",
    "        \"Slovénie\": \"Slovenia\",\n",
    "        \"Suède\": \"Sweden\",\n",
    "        \"Suisse\": \"Switzerland\",\n",
    "        \"Ukraine\": \"Ukraine\",\n",
    "        \"Vatican\": \"Vatican City\"\n",
    "    }, inplace=True)\n",
    "\n",
    "    df.index.name = None\n",
    "    df.columns.name = 'Material'\n",
    "\n",
    "    # Melt the DataFrame to long format for merging\n",
    "    df_melted = df.reset_index().melt(id_vars='index', var_name='Country', value_name='Value')\n",
    "    df_melted.rename(columns={'index': 'Date'}, inplace=True)\n",
    "\n",
    "    # Merge GeoJSON data with DataFrame\n",
    "    europe_merged = europe.merge(df_melted, left_on='NAME', right_on='Country', how='left')\n",
    "\n",
    "    # Plot the data\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    europe_merged.plot(column='Value', ax=ax, legend=True, cmap='viridis', \n",
    "                       missing_kwds={\"color\": \"lightgrey\"},\n",
    "                       legend_kwds={'label': data_type})\n",
    "\n",
    "    plt.title(f\"Map of {data_type} in Europe in {pd.to_datetime(date).strftime('%B %Y')}\", fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widgets pour sélectionner les pays et les dates\n",
    "countries_widget = widgets.Text(\n",
    "    value='France, Allemagne, Italie',\n",
    "    description='Countries:',\n",
    "    placeholder='Enter countries separated by commas'\n",
    ")\n",
    "\n",
    "start_date_widget = widgets.DatePicker(\n",
    "    value=pd.to_datetime('2015-1', format='%Y-%m'),\n",
    "    description='Start Date'\n",
    ")\n",
    "\n",
    "end_date_widget = widgets.DatePicker(\n",
    "    value=pd.to_datetime('2020-3', format='%Y-%m'),\n",
    "    description='End Date'\n",
    ")\n",
    "\n",
    "# Widget pour choix multiple des données à afficher\n",
    "multi_choice_widget = widgets.SelectMultiple(\n",
    "    options=['IPCH', 'Dette publique', 'PIB', 'Chomage'],\n",
    "    value=['IPCH'],\n",
    "    description='Select Data',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Widget case à cocher\n",
    "checkbox_widget = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Map',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Fonction générale pour tracer les données\n",
    "@interact_manual(countries=countries_widget, \\\n",
    "                 start_date=start_date_widget, end_date=end_date_widget, \\\n",
    "                 type=multi_choice_widget, map=checkbox_widget)\n",
    "def plot_G(countries, start_date, end_date, type, map = True):\n",
    "    if map: plot_map(date = start_date, data = data, data_type= type[0])\n",
    "    else:\n",
    "        for t in type:\n",
    "            plot_graph(countries, start_date, end_date, data.get_group(t), t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage matières premières"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the data\n",
    "def plot_data(start_date, end_date, show_or, show_petrol):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    start_date, end_date = pd.to_datetime(start_date), pd.to_datetime(end_date)\n",
    "    \n",
    "    # Filter the data based on the selected dates\n",
    "    filtered_material = material[(material.index >= start_date) & (material.index <= end_date)]\n",
    "    \n",
    "    if show_or:\n",
    "        sb.lineplot(data=filtered_material, x=filtered_material.index, y='Or', label='Or', marker='o')\n",
    "        # If 'Or' checkbox is checked, plot the data for 'Or'\n",
    "\n",
    "    if show_petrol:\n",
    "        sb.lineplot(data=filtered_material, x=filtered_material.index, y='Petrol', label='Pétrole', marker='o')\n",
    "        # If 'Petrol' checkbox is checked, plot the data for 'Petrol'\n",
    "\n",
    "    # Configure the plot with a title, legend, and grid\n",
    "    plt.title(f'Cour de l\\'or et du pétrole en euros ({start_date.strftime(\"%m/%Y\")} - {end_date.strftime(\"%m/%Y\")})')\n",
    "    plt.legend()  # The legend is automatically updated based on the checked datasets\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Widgets for selecting the start and end dates, and options to display data\n",
    "start_date_widget = widgets.DatePicker(description='Début', value=pd.to_datetime('2013-01-01'))\n",
    "end_date_widget = widgets.DatePicker(description='Fin', value=pd.to_datetime('2023-12-31'))\n",
    "show_or_widget = widgets.Checkbox(description='Or', value=True)  # Checkbox for showing 'Or' data\n",
    "show_petrol_widget = widgets.Checkbox(description='Pétrole', value=True)  # Checkbox for showing 'Petrol' data\n",
    "\n",
    "# Interactive interface to control the plot function with widgets\n",
    "interact(\n",
    "    plot_data,  # The function to interact with\n",
    "    start_date=start_date_widget,  # Start date widget\n",
    "    end_date=end_date_widget,  # End date widget\n",
    "    show_or=show_or_widget,  # 'Or' checkbox widget\n",
    "    show_petrol=show_petrol_widget  # 'Petrol' checkbox widget\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage des devises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot currency data\n",
    "def plot_devise_data(select_all, start_date, end_date, **currency_checkboxes):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # List of selected currencies\n",
    "    if select_all:\n",
    "        selected_currencies = devise.columns.tolist()  # If \"select all\" is checked, include all currencies\n",
    "    else:\n",
    "        selected_currencies = [currency for currency, is_selected in currency_checkboxes.items() if is_selected]\n",
    "        # If not, include only the selected currencies based on the checkboxes\n",
    "\n",
    "    # Filter the data based on the selected date range and currencies\n",
    "    filtered_data = devise[(devise.index >= start_date) & (devise.index <= end_date)]\n",
    "    filtered_data = filtered_data[selected_currencies]\n",
    "\n",
    "    # Plot the time series for each selected currency\n",
    "    for currency in selected_currencies:\n",
    "        sb.lineplot(data=filtered_data, x=filtered_data.index, y=currency, label=currency, marker='o')\n",
    "\n",
    "    # Configure the plot with a title, x and y labels, and a legend\n",
    "    plt.title(f'Valeurs des devises (équivalent en euros) ({start_date.strftime(\"%m/%Y\")} - {end_date.strftime(\"%m/%Y\")})')\n",
    "    plt.xlabel('TIME_PERIOD')\n",
    "    plt.ylabel('OBS_VALUE')\n",
    "    plt.legend(title='Currency')  # Currency legend\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Widgets for selecting the start and end dates\n",
    "start_date_widget = widgets.DatePicker(description='Début', value=pd.to_datetime('2013-01-01'))\n",
    "end_date_widget = widgets.DatePicker(description='Fin', value=pd.to_datetime('2023-12-31'))\n",
    "\n",
    "# Dynamically generate checkboxes for each currency\n",
    "currency_checkboxes = {\n",
    "    currency: widgets.Checkbox(description=currency, value=False)  # Default value is False (unchecked)\n",
    "    for currency in devise.columns\n",
    "}\n",
    "\n",
    "# Checkbox to \"Select All\" currencies\n",
    "select_all_widget = widgets.Checkbox(description='Tout sélectionner', value=False)\n",
    "\n",
    "# Function to dynamically update checkboxes based on \"Select All\"\n",
    "def update_checkboxes(change):\n",
    "    for checkbox in currency_checkboxes.values():\n",
    "        checkbox.value = change['new']  # Update the state of checkboxes based on the \"Select All\" checkbox\n",
    "\n",
    "select_all_widget.observe(update_checkboxes, names='value')  # Observe changes to the \"Select All\" checkbox\n",
    "\n",
    "# Create a container for all the checkboxes\n",
    "checkbox_container = VBox([select_all_widget] + list(currency_checkboxes.values()))\n",
    "\n",
    "# Interactive interface to control the plot function with widgets\n",
    "interact(\n",
    "    plot_devise_data,  # The function to interact with\n",
    "    select_all=select_all_widget,  # \"Select All\" widget\n",
    "    start_date=start_date_widget,  # Start date widget\n",
    "    end_date=end_date_widget,  # End date widget\n",
    "    **currency_checkboxes  # Pass each currency checkbox widget as a parameter\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrélations\n",
    "\n",
    "Argument: data, type, historique (pd.Series), countries\\\n",
    "Calcule la correlation entre (colonne et historique, pour colonne dans data.get_group(type).loc[;, countries])\\\n",
    "Fais une moyenne des correlation.\\\n",
    "Retourne la correlation moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(data, variables):\n",
    "  \"\"\"Return the correlation matrix for specified variables.\"\"\"\n",
    "  data_flat = data.apply(lambda x: x.droplevel(0, axis=1))  # Flatten MultiIndex\n",
    "  return data_flat[variables].corr()  # Compute and return correlation matrix\n",
    "\n",
    "#test\n",
    "variables = ['PIB', 'Chomage','IPCH']\n",
    "correlation_matrix(data, variables).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_correlation(data, hist_action, country, data_type, company):\n",
    "    \"\"\"\n",
    "    Compute avg correlation between a company's data in hist_action and a country's data in the provided dataset.\n",
    "    \"\"\"\n",
    "    # Flatten the MultiIndex structure in the dataset to simplify access to columns.\n",
    "    ungrouped_data = data.apply(lambda df: df.droplevel(0, axis=1))\n",
    "\n",
    "    # Check if the specified company and country exist in their respective datasets.\n",
    "    if company not in hist_action.columns or country not in ungrouped_data[data_type].columns:\n",
    "        return None  # Return None if either is not present.\n",
    "    \n",
    "    # Extract a specific column for the company.\n",
    "    # If the company's data is a DataFrame (multi-column), select the 'Close' column by default.\n",
    "    if isinstance(hist_action[company], pd.DataFrame):\n",
    "        col_company = hist_action[company]['Close']  # Use the 'Close' column for the company's data.\n",
    "    else:\n",
    "        col_company = hist_action[company]  # Use the entire series if no multi-column structure exists.\n",
    "    \n",
    "    # Extract the country's data as a Series.\n",
    "    col_country = ungrouped_data[data_type][country]\n",
    "    \n",
    "    # Ensure both columns have valid (non-NaN) data by dropping null values.\n",
    "    col_company = col_company.dropna()\n",
    "    col_country = col_country.dropna()\n",
    "\n",
    "    # Align the two Series by finding the intersection of their indices (dates).\n",
    "    common_index = col_company.index.intersection(col_country.index)\n",
    "    col_company = col_company.loc[common_index]\n",
    "    col_country = col_country.loc[common_index]\n",
    "    \n",
    "    # Compute the correlation between the two aligned Series if there are valid data points.\n",
    "    if len(col_company) > 0 and len(col_country) > 0:\n",
    "        return col_company.corr(col_country)  # Return the correlation coefficient.\n",
    "    else:\n",
    "        return None  # Return None if there are no valid data points.\n",
    "\n",
    "\n",
    "  \n",
    "result = compute_avg_correlation(data, hist_action, 'France', 'PIB', 'Air Liquide')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_material_correlation(material, hist_action, commodity, company):\n",
    "  \"\"\"compute avg correlation between commodity in material and company in hist_action\"\"\"\n",
    "  col_material = material[commodity]  # select commodity column\n",
    "  col_company = hist_action.xs(company, axis=1, level='Company')  # select company column\n",
    "  correlations = [col_material.corr(col_company[c]) for c in col_company.columns]  # compute correlations\n",
    "  return sum(correlations) / len(correlations)  # return mean correlation\n",
    "\n",
    "avg_material_correlation(material, hist_action, 'Or', 'Air Liquide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_currency_correlation(devise, hist_action, currency, company):\n",
    "  \"\"\"compute avg correlation between company in hist_action and currency in devise\"\"\"\n",
    "  col_company = hist_action.xs(company, axis=1, level='Company')  # select company column\n",
    "  col_currency = devise[currency]  # select currency column\n",
    "  correlations = [col_company[c].corr(col_currency) for c in col_company.columns]  # compute correlations\n",
    "  return sum(correlations) / len(correlations)  # return mean correlation\n",
    "\n",
    "compute_avg_currency_correlation(devise, hist_action, 'US dollar', 'Air Liquide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
