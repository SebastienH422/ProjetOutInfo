{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PIB :** https://donnees.banquemondiale.org/indicateur/NY.GDP.MKTP.CD  \n",
    "**Taux de chômage :** https://ec.europa.eu/eurostat/databrowser/view/UNE_RT_M__custom_14826434/default/table?lang=fr  \n",
    "**IPCH :**  https://ec.europa.eu/eurostat/databrowser/view/PRC_HICP_MANR__custom_14819170/default/table?lang=fr  \n",
    "**Historique des actions :**  \n",
    "**Devise:** https://ec.europa.eu/eurostat/databrowser/view/tec00033/default/table?lang=en&category=t_ert  \n",
    "**Matières premières:** https://bdm.insee.fr/series/sdmx/data/SERIES_BDM/010002100 **et** https://bdm.insee.fr/series/sdmx/data/SERIES_BDM/010002091  \n",
    "**Dette publique:** https://ec.europa.eu/eurostat/databrowser/view/sdg_17_40/default/table?lang=fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type d'analyses prévus et résultats attendus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses prévues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Corrélations entre les différentes données  \n",
    "- Etude d'indices boursiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultats attendus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIB\n",
    "Un PIB croissant est souvent associé à une économie forte, ce qui peut influencer positivement les marchés boursiers. L'analyse cherchera à quantifier cette relation.  \n",
    "\n",
    "### Taux de chômage\n",
    "Un faible taux de chômage peut refléter une économie robuste et un climat favorable aux entreprises, impactant ainsi les actions. Les corrélations entre ces données et les performances boursières seront examinées.   \n",
    "\n",
    "### IPCH (Indice des Prix à la Consommation Harmonisé)\n",
    "L'inflation, mesurée ici par l'IPCH, est un facteur clé pour comprendre les ajustements des marchés financiers aux variations des taux d'intérêt et des prix.  \n",
    "\n",
    "### Historique des actions\n",
    "L'analyse des tendances passées dans les cours des actions permettra d'évaluer la réactivité des marchés aux changements des indicateurs économiques.  \n",
    "\n",
    "### Devise\n",
    "Les fluctuations des taux de change peuvent avoir un impact direct, notamment pour les entreprises opérant à l'international. Les relations entre les cours des actions et les variations des devises seront explorées.  \n",
    "\n",
    "### Matières premières\n",
    "Certains secteurs boursiers sont fortement dépendants des prix des matières premières. L'étude analysera les corrélations spécifiques entre ces prix et les performances des actions dans les secteurs concernés.  \n",
    "\n",
    "### Dette intérieure\n",
    "Le niveau d'endettement d'un pays peut influencer la confiance des investisseurs et, par conséquent, le comportement des marchés. L'étude des corrélations dans ce contexte sera essentielle.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Début du code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import warnings\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "\n",
    "import ipywidgets as  widgets\n",
    "from ipywidgets import interact, widgets, VBox, HBox\n",
    "from ipywidgets import interact_manual\n",
    "import geopandas as gpd\n",
    "\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dette publique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data from the Excel file\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "# Importer les données depuis l'URL\n",
    "dette_publique_url = 'https://sebastien-hein.emi.u-bordeaux.fr/OI-sbzrthstrm/DATA/dette_pub.xlsx'\n",
    "code_url = 'https://sebastien-hein.emi.u-bordeaux.fr/OI-sbzrthstrm/DATA/code.tsv'\n",
    "\n",
    "# Charger les fichiers directement depuis l'URL\n",
    "dette_publique = pd.read_excel(dette_publique_url, sheet_name='Feuille 1')\n",
    "code = pd.read_csv(code_url, sep='\\t')\n",
    "\n",
    "# Define column names for code and label\n",
    "col_code = 'CODE'\n",
    "col_label = 'Label - French'\n",
    "\n",
    "def find_value(x):\n",
    "    \"\"\"Find the label value based on the code.\"\"\"\n",
    "    matched = not code[code[col_code] == x].empty\n",
    "    if matched:\n",
    "        return code[code[col_code] == x].loc[:, col_label].iloc[0]\n",
    "    elif x == 'TIME':\n",
    "        return 'Country'\n",
    "    return None\n",
    "\n",
    "# Modify the index of the dataframe\n",
    "dette_publique.index = dette_publique.iloc[:, 0].apply(find_value)\n",
    "dette_publique.index.name = None\n",
    "\n",
    "# Set the column names based on the 'Country' row\n",
    "dette_publique.columns = dette_publique.loc['Country']\n",
    "\n",
    "# Filter the dataframe to include only rows from 'Belgique' to 'Suède' onwards and exclude the 'TIME' column\n",
    "# Indeed, only the countries interest us,\n",
    "# and the datas are missing for Islande, Norvège, Suisse and United Kingdom.\n",
    "dette_publique = dette_publique.loc['Belgique':'Suède', dette_publique.columns != 'TIME']\n",
    "\n",
    "# Filter the dataframe to include only columns from the year 2002 onwards\n",
    "dette_publique = dette_publique.loc[:,2002:] # Problème non résolu: Si l'on prend une date inférieure à 2002, \n",
    "                                             #                      l'interpolation ne fonctionne nul part.\n",
    "\n",
    "def to_date(x):\n",
    "    \"\"\"Convert a value to datetime.\"\"\"\n",
    "    return pd.to_datetime(x, format='%Y')\n",
    "\n",
    "# Vectorize the to_date function\n",
    "vect_to_date = np.vectorize(to_date)\n",
    "\n",
    "# Convert the columns to datetime\n",
    "dette_publique.columns = vect_to_date(dette_publique.columns.values)\n",
    "\n",
    "monthly_dates = pd.date_range(start=dette_publique.columns.values[0], end=dette_publique.columns.values[-1], freq='MS')\n",
    "\n",
    "# Add columns for each month from 2013 to 2023\n",
    "dette_publique = dette_publique.reindex(columns=dette_publique.columns.union(monthly_dates))\n",
    "\n",
    "def fill_val(x):\n",
    "    \"\"\"Fill missing values by resampling and interpolating.\"\"\"\n",
    "    return x.resample('MS').interpolate(method='quadratic')\n",
    "\n",
    "# Apply the fill_val function to each row\n",
    "dette_publique = dette_publique.apply(func=fill_val, axis=1).T\n",
    "dette_publique.columns.name = 'Country'\n",
    "\n",
    "dette_publique.isna().sum().sum() # Number of missing values (0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albanie: [('2010-12', '2016-11')]\n",
      "Monténégro: [('2010-12', '2015-11')]\n",
      "United Kingdom: [('2020-12', '2024-11')]\n",
      "Kosovo*: [('2010-12', '2016-11')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define URLs for the data sources\n",
    "ipch_url = 'https://sebastien-hein.emi.u-bordeaux.fr/OI-sbzrthstrm/DATA/ipch.tsv'\n",
    "code_cp_url = 'https://sebastien-hein.emi.u-bordeaux.fr/OI-sbzrthstrm/DATA/code_cp.tsv'\n",
    "\n",
    "# Load the data directly from the URLs\n",
    "ipch = pd.read_csv(ipch_url, sep='\\t')  # Load the ipch data using tab as a separator\n",
    "code_cp = pd.read_csv(code_cp_url, sep='\\t')  # Load the code_cp data using tab as a separator\n",
    "\n",
    "\n",
    "# Set index\n",
    "def get_CP(x): return x[8:12]  # Extract CP code\n",
    "def get_id(x): return x[13:]  # Extract id\n",
    "vect_get_cp = np.vectorize(get_CP)\n",
    "vect_get_id = np.vectorize(get_id)\n",
    "ipch['CP'] = vect_get_cp(ipch.iloc[:, 0])  # Apply CP extraction\n",
    "ipch['id'] = vect_get_id(ipch.iloc[:, 0])  # Apply id extraction\n",
    "\n",
    "ipch = ipch[ipch['CP'] == 'CP00']\n",
    "ipch.drop(columns = 'CP', inplace = True)\n",
    "\n",
    "ipch.drop(columns='freq,unit,coicop,geo\\\\TIME_PERIOD', inplace=True)  # Drop unnecessary columns\n",
    "ipch['country'] = ipch.loc[:, 'id'].apply(find_value)  # Find country names\n",
    "ipch.set_index(['country'], inplace=True)  # Set index\n",
    "ipch.drop(columns='id', inplace=True)  # Drop id column\n",
    "\n",
    "# Convert the columns to datetime\n",
    "def to_date_M(x):\n",
    "    \"\"\"Convert a value to datetime.\"\"\"\n",
    "    try:\n",
    "        return pd.to_datetime(x[:-1], format='%Y-%m')\n",
    "    except:\n",
    "        print('fail')\n",
    "        return x\n",
    "\n",
    "vect_to_date_M = np.vectorize(to_date_M)\n",
    "ipch.columns = vect_to_date_M(ipch.columns.values)  # Apply datetime conversion\n",
    "\n",
    "# Filter data\n",
    "ipch = ipch[~ipch.index.str.startswith(('Union', 'Zone', 'Espace'))]  # Exclude certain countries\n",
    "\n",
    "# Clean and convert to numeric\n",
    "ipch = ipch.map(lambda x: pd.to_numeric(\n",
    "    str(x).replace(' ', '').replace('d', ''), errors='coerce'))\n",
    "ipch = ipch.T\n",
    "ipch.columns.name = 'Country'\n",
    "\n",
    "# Missing values\n",
    "# Initialiser le dictionnaire pour stocker les plages de dates manquantes\n",
    "missing_ranges = defaultdict(list)\n",
    "\n",
    "# Identifier les valeurs manquantes\n",
    "missing_values = ipch.isna()\n",
    "\n",
    "# Parcourir chaque pays (colonne) pour trouver les plages de dates manquantes\n",
    "for country in ipch.columns:\n",
    "    country_missing = missing_values[country]\n",
    "    if not country_missing.empty:\n",
    "        # Trouver les plages de dates manquantes\n",
    "        missing_dates = country_missing[country_missing].index\n",
    "        start_date = None\n",
    "        for date in missing_dates:\n",
    "            if start_date is None:\n",
    "                start_date = date\n",
    "            if (date + pd.DateOffset(months=1)) not in missing_dates:\n",
    "                end_date = date\n",
    "                missing_ranges[country].append((start_date.strftime('%Y-%m'), end_date.strftime('%Y-%m')))\n",
    "                start_date = None\n",
    "\n",
    "# Convertir le defaultdict en dict\n",
    "missing_ranges = dict(missing_ranges)\n",
    "\n",
    "for country, dates in missing_ranges.items():\n",
    "    print(f'{country}: {dates}')\n",
    "\n",
    "# Delete the country for which the missing data is on a bigger period than 4 years\n",
    "ipch.drop(columns = 'Albanie, Kosovo*, Monténégro'.split(', '), inplace = True)\n",
    "\n",
    "# Fill missing values using interpolation as the most consistent method for long gaps\n",
    "ipch.interpolate(method='time', inplace=True, limit_direction='both')  # Interpolate linearly by date for smoother transitions\n",
    "\n",
    "# Optionally fill remaining missing values (if interpolation failed for some edge cases) with column mean\n",
    "ipch.fillna(ipch.mean(), inplace=True)\n",
    "\n",
    "ipch.isna().sum().sum() # Number of missing values (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chomage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_xml(file_url):\n",
    "    \"\"\"Parse XML file and extract data into a DataFrame.\"\"\"\n",
    "    \n",
    "    # Download the XML file content using requests\n",
    "    response = requests.get(file_url)\n",
    "    xml_content = response.text  # Get the content of the XML file as a string\n",
    "    \n",
    "    # Parse the XML content with ElementTree\n",
    "    root = ET.fromstring(xml_content)  # Parse the XML string directly\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    data = []\n",
    "    columns = set()\n",
    "    rows = set()\n",
    "\n",
    "    # Extract data from <Series> and <Obs> tags\n",
    "    for series in root.findall('.//Series'):\n",
    "        geo = series.attrib.get('geo')  # Get \"geo\" attribute\n",
    "        if geo:\n",
    "            rows.add(geo)\n",
    "            for obs in series.findall('Obs'):\n",
    "                time_period = obs.attrib.get('TIME_PERIOD')  # Get \"TIME_PERIOD\" attribute\n",
    "                obs_value = obs.attrib.get('OBS_VALUE')  # Get \"OBS_VALUE\" attribute\n",
    "                if time_period and obs_value:\n",
    "                    columns.add(time_period)\n",
    "                    data.append((geo, time_period, obs_value))\n",
    "\n",
    "    # Create DataFrame with appropriate indices\n",
    "    df = pd.DataFrame(index=sorted(rows), columns=sorted(columns))\n",
    "\n",
    "    # Fill DataFrame with extracted values\n",
    "    for geo, time_period, obs_value in data:\n",
    "        df.at[geo, time_period] = obs_value\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for the XML data\n",
    "file_url = 'https://sebastien-hein.emi.u-bordeaux.fr/OI-sbzrthstrm/DATA/chomage.xml'\n",
    "\n",
    "# Load the data\n",
    "chomage = parse_xml(file_url)\n",
    "\n",
    "# Format the data\n",
    "chomage.columns = chomage.columns.map(lambda x: \\\n",
    "                                      pd.to_datetime(x, format='%Y-%m'))  # Convert columns to datetime\n",
    "chomage.index = chomage.index.map(lambda x: \\\n",
    "                code.loc[code.loc[:, 'CODE'] == x, 'Label - French'].iloc[0])  # Map index to labels\n",
    "chomage.drop('Zone euro - 20 pays (à partir de 2023)', inplace=True)  # Drop specific rows\n",
    "chomage.drop('Union européenne - 27 pays (à partir de 2020)', inplace=True)  # Drop specific rows\n",
    "chomage = chomage.apply(pd.to_numeric).T  # Convert data to numeric\n",
    "chomage.columns.name = 'Country'\n",
    "\n",
    "\n",
    "# Missing values\n",
    "missing_ranges = defaultdict(list) # Initialiser le dictionnaire pour stocker les plages de dates manquantes\n",
    "missing_values = chomage.isna() # Identifier les valeurs manquantes\n",
    "\n",
    "for country in chomage.columns: # Parcourir chaque pays (colonne) pour trouver les plages de dates manquantes\n",
    "    country_missing = missing_values[country]\n",
    "    if country_missing.any():\n",
    "        # Trouver les plages de dates manquantes\n",
    "        missing_dates = country_missing[country_missing].index\n",
    "        start_date = None\n",
    "        for date in missing_dates:\n",
    "            if start_date is None:\n",
    "                start_date = date\n",
    "            if date + pd.DateOffset(months=1) not in missing_dates:\n",
    "                end_date = date\n",
    "                missing_ranges[country].append((start_date.strftime('%Y-%m'), \n",
    "                                                end_date.strftime('%Y-%m')))\n",
    "                start_date = None\n",
    "\n",
    "# Fill missing values using interpolation as the most consistent method for long gaps\n",
    "chomage.interpolate(method='time', inplace=True, limit_direction='both')  # Interpolate linearly by date for smoother transitions\n",
    "\n",
    "# Optionally fill remaining missing values (if interpolation failed for some edge cases) with column mean\n",
    "chomage.fillna(chomage.mean(), inplace=True)\n",
    "\n",
    "ipch.isna().sum().sum() # Number of missing values (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml_pib(file_url):\n",
    "    \"\"\"Parse XML file and extract data into a DataFrame.\"\"\"\n",
    "    \n",
    "    # Download the XML file content using requests\n",
    "    response = requests.get(file_url)\n",
    "    xml_content = response.text  # Get the content of the XML file as a string\n",
    "    \n",
    "    # Parse the XML content with ElementTree\n",
    "    root = ET.fromstring(xml_content)  # Parse the XML string directly\n",
    "    \n",
    "    # Initialize a list to store data\n",
    "    data = []\n",
    "\n",
    "    # Extract data\n",
    "    for record in root.findall('.//record'):\n",
    "        record_data = {}\n",
    "        for field in record.findall('field'):\n",
    "            name = field.attrib.get('name')\n",
    "            text = field.text\n",
    "            match name:\n",
    "                case \"Country or Area\": record_data['country'] = text\n",
    "                case \"Value\": \n",
    "                    try: record_data['value'] = float(text)\n",
    "                    except: record_data['value'] = None\n",
    "                case \"Year\": record_data['year'] = pd.to_datetime(str(text))\n",
    "        data.append(record_data)\n",
    "\n",
    "    # Create DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates(subset=['year', 'country'])\n",
    "\n",
    "    # Pivot the DataFrame to get the desired format\n",
    "    df = df.pivot(index='year', columns='country', values='value')\n",
    "\n",
    "    return df\n",
    "\n",
    "url = 'https://julie-sclaunich.emi.u-bordeaux.fr/DATA/API_NY.GDP.MKTP.CD_DS2_fr_xml_v2_38351.xml'\n",
    "pib = parse_xml_pib(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reindex monthly\n",
    "monthly_dates = pd.date_range(start=pib.index.values[0], end=pib.index.values[-1], freq='MS')\n",
    "\n",
    "# Add columns for each month from 2013 to 2023\n",
    "pib = pib.reindex(index=pib.index.union(monthly_dates))\n",
    "pib.columns.name = 'Country'\n",
    "\n",
    "# Select only same dates and countries as dette_publique\n",
    "pib = pib.loc[dette_publique.index, dette_publique.columns.intersection(pib.columns)]\n",
    "\n",
    "def fill_val(x):\n",
    "    \"\"\"Fill missing values by resampling and interpolating.\"\"\"\n",
    "    return x.resample('MS').interpolate(method='quadratic')\n",
    "\n",
    "# Apply the fill_val function to each row\n",
    "pib = pib.apply(func=fill_val, axis=0)\n",
    "\n",
    "pib.isna().sum().sum() # Number of missing values (0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Currency</th>\n",
       "      <th>Bosnia and Herzegovina convertible mark</th>\n",
       "      <th>Bulgarian lev</th>\n",
       "      <th>Canadian dollar</th>\n",
       "      <th>Czech koruna</th>\n",
       "      <th>Danish krone</th>\n",
       "      <th>Hungarian forint</th>\n",
       "      <th>Icelandic króna</th>\n",
       "      <th>Japanese yen</th>\n",
       "      <th>North Macedonian denar</th>\n",
       "      <th>Norwegian krone</th>\n",
       "      <th>Polish zloty</th>\n",
       "      <th>Pound sterling</th>\n",
       "      <th>Romanian leu</th>\n",
       "      <th>Russian rouble</th>\n",
       "      <th>Serbian dinar</th>\n",
       "      <th>Swedish krona</th>\n",
       "      <th>Swiss franc</th>\n",
       "      <th>Turkish lira</th>\n",
       "      <th>US dollar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>1.95583</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>1.368400</td>\n",
       "      <td>25.980000</td>\n",
       "      <td>7.457900</td>\n",
       "      <td>296.870000</td>\n",
       "      <td>162.380000</td>\n",
       "      <td>129.660000</td>\n",
       "      <td>61.585000</td>\n",
       "      <td>7.806700</td>\n",
       "      <td>4.197500</td>\n",
       "      <td>0.849260</td>\n",
       "      <td>4.419000</td>\n",
       "      <td>42.337000</td>\n",
       "      <td>113.136900</td>\n",
       "      <td>8.651500</td>\n",
       "      <td>1.231100</td>\n",
       "      <td>2.533500</td>\n",
       "      <td>1.328100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-01</th>\n",
       "      <td>1.95583</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>1.384134</td>\n",
       "      <td>26.195389</td>\n",
       "      <td>7.457178</td>\n",
       "      <td>298.365029</td>\n",
       "      <td>161.761335</td>\n",
       "      <td>131.231491</td>\n",
       "      <td>61.590730</td>\n",
       "      <td>7.849537</td>\n",
       "      <td>4.197594</td>\n",
       "      <td>0.848684</td>\n",
       "      <td>4.422440</td>\n",
       "      <td>42.618453</td>\n",
       "      <td>113.521815</td>\n",
       "      <td>8.697162</td>\n",
       "      <td>1.236956</td>\n",
       "      <td>2.577914</td>\n",
       "      <td>1.339790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-01</th>\n",
       "      <td>1.95583</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>1.397158</td>\n",
       "      <td>26.376653</td>\n",
       "      <td>7.456598</td>\n",
       "      <td>299.637286</td>\n",
       "      <td>161.199346</td>\n",
       "      <td>132.544486</td>\n",
       "      <td>61.595525</td>\n",
       "      <td>7.888816</td>\n",
       "      <td>4.197485</td>\n",
       "      <td>0.847671</td>\n",
       "      <td>4.425333</td>\n",
       "      <td>42.944501</td>\n",
       "      <td>113.864562</td>\n",
       "      <td>8.737177</td>\n",
       "      <td>1.241087</td>\n",
       "      <td>2.615999</td>\n",
       "      <td>1.348489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-01</th>\n",
       "      <td>1.95583</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>1.410265</td>\n",
       "      <td>26.562635</td>\n",
       "      <td>7.456038</td>\n",
       "      <td>300.959399</td>\n",
       "      <td>160.573608</td>\n",
       "      <td>133.880340</td>\n",
       "      <td>61.600413</td>\n",
       "      <td>7.932953</td>\n",
       "      <td>4.197150</td>\n",
       "      <td>0.846004</td>\n",
       "      <td>4.428299</td>\n",
       "      <td>43.385011</td>\n",
       "      <td>114.238586</td>\n",
       "      <td>8.780119</td>\n",
       "      <td>1.244379</td>\n",
       "      <td>2.655914</td>\n",
       "      <td>1.356062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-01</th>\n",
       "      <td>1.95583</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>1.421635</td>\n",
       "      <td>26.727907</td>\n",
       "      <td>7.455577</td>\n",
       "      <td>302.152358</td>\n",
       "      <td>159.964517</td>\n",
       "      <td>135.055220</td>\n",
       "      <td>61.604722</td>\n",
       "      <td>7.976317</td>\n",
       "      <td>4.196610</td>\n",
       "      <td>0.843845</td>\n",
       "      <td>4.430932</td>\n",
       "      <td>43.890885</td>\n",
       "      <td>114.595096</td>\n",
       "      <td>8.820316</td>\n",
       "      <td>1.246282</td>\n",
       "      <td>2.692291</td>\n",
       "      <td>1.361329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Currency    Bosnia and Herzegovina convertible mark  Bulgarian lev  \\\n",
       "2013-01-01                                  1.95583         1.9558   \n",
       "2013-02-01                                  1.95583         1.9558   \n",
       "2013-03-01                                  1.95583         1.9558   \n",
       "2013-04-01                                  1.95583         1.9558   \n",
       "2013-05-01                                  1.95583         1.9558   \n",
       "\n",
       "Currency    Canadian dollar  Czech koruna  Danish krone  Hungarian forint  \\\n",
       "2013-01-01         1.368400     25.980000      7.457900        296.870000   \n",
       "2013-02-01         1.384134     26.195389      7.457178        298.365029   \n",
       "2013-03-01         1.397158     26.376653      7.456598        299.637286   \n",
       "2013-04-01         1.410265     26.562635      7.456038        300.959399   \n",
       "2013-05-01         1.421635     26.727907      7.455577        302.152358   \n",
       "\n",
       "Currency    Icelandic króna  Japanese yen  North Macedonian denar  \\\n",
       "2013-01-01       162.380000    129.660000               61.585000   \n",
       "2013-02-01       161.761335    131.231491               61.590730   \n",
       "2013-03-01       161.199346    132.544486               61.595525   \n",
       "2013-04-01       160.573608    133.880340               61.600413   \n",
       "2013-05-01       159.964517    135.055220               61.604722   \n",
       "\n",
       "Currency    Norwegian krone  Polish zloty  Pound sterling  Romanian leu  \\\n",
       "2013-01-01         7.806700      4.197500        0.849260      4.419000   \n",
       "2013-02-01         7.849537      4.197594        0.848684      4.422440   \n",
       "2013-03-01         7.888816      4.197485        0.847671      4.425333   \n",
       "2013-04-01         7.932953      4.197150        0.846004      4.428299   \n",
       "2013-05-01         7.976317      4.196610        0.843845      4.430932   \n",
       "\n",
       "Currency    Russian rouble  Serbian dinar  Swedish krona  Swiss franc  \\\n",
       "2013-01-01       42.337000     113.136900       8.651500     1.231100   \n",
       "2013-02-01       42.618453     113.521815       8.697162     1.236956   \n",
       "2013-03-01       42.944501     113.864562       8.737177     1.241087   \n",
       "2013-04-01       43.385011     114.238586       8.780119     1.244379   \n",
       "2013-05-01       43.890885     114.595096       8.820316     1.246282   \n",
       "\n",
       "Currency    Turkish lira  US dollar  \n",
       "2013-01-01      2.533500   1.328100  \n",
       "2013-02-01      2.577914   1.339790  \n",
       "2013-03-01      2.615999   1.348489  \n",
       "2013-04-01      2.655914   1.356062  \n",
       "2013-05-01      2.692291   1.361329  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_avg_currency_correlation(devise, hist_action, currency, company):\n",
    "  \"\"\"compute avg correlation between company in hist_action and currency in devise\"\"\"\n",
    "  col_company = hist_action.xs(company, axis=1, level='Company')  # select company column\n",
    "  col_currency = devise[currency]  # select currency column\n",
    "  correlations = [col_company[c].corr(col_currency) for c in col_company.columns]  # compute correlations\n",
    "  return sum(correlations) / len(correlations)  # return mean correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matières premières\n",
    "## Or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Or</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-01</th>\n",
       "      <td>201.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-01</th>\n",
       "      <td>198.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-01</th>\n",
       "      <td>195.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-01</th>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-01</th>\n",
       "      <td>190.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Or\n",
       "2023-12-01  201.5\n",
       "2023-11-01  198.6\n",
       "2023-10-01  195.8\n",
       "2023-09-01  194.0\n",
       "2023-08-01  190.2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_or = \"https://bdm.insee.fr/series/sdmx/data/SERIES_BDM/010002100\" # URL of the serie\n",
    "\n",
    "response = requests.get(url_or) # Retrieve XML data\n",
    "response.raise_for_status() # Checks that the request is successful\n",
    "xml_content = response.content\n",
    "\n",
    "\n",
    "root = ET.fromstring(xml_content) # Parse XML content\n",
    "\n",
    "root = ET.fromstring(xml_content) # Load XML content\n",
    "\n",
    "\n",
    "data = [] # Initialize a list to store the data\n",
    "\n",
    "\n",
    "for series in root.findall(\".//{*}Series\"): # Browse each series\n",
    "\n",
    "    for obs in series.findall(\".//{*}Obs\"): # Browse the observation in  each series\n",
    "\n",
    "        # Extract relevant \n",
    "        time_period = obs.attrib.get(\"TIME_PERIOD\")\n",
    "        obs_value = obs.attrib.get(\"OBS_VALUE\")\n",
    "        # Add the data at the list\n",
    "        data.append({\"TIME_PERIOD\": time_period, \"OBS_VALUE\": obs_value})\n",
    "\n",
    "\n",
    "df_or = pd.DataFrame(data) # Create a DataFrame from the extracted data\n",
    "\n",
    "\n",
    "# Convert columns to appropriate types\n",
    "df_or[\"TIME_PERIOD\"] = pd.to_datetime(df_or[\"TIME_PERIOD\"], format=\"%Y-%m\")\n",
    "df_or[\"OBS_VALUE\"] = pd.to_numeric(df_or[\"OBS_VALUE\"])\n",
    "\n",
    "\n",
    "\n",
    "# Convert TIME_PERIOD to datetime for easier filtering\n",
    "df_or['TIME_PERIOD'] = pd.to_datetime(df_or['TIME_PERIOD'], format='%Y-%m')\n",
    "\n",
    "# Filter years between 2013 and 2023\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2023-12-31'\n",
    "df_or = df_or[(df_or['TIME_PERIOD'] >= start_date) & (df_or['TIME_PERIOD'] <= end_date)]\n",
    "\n",
    "df_or.set_index('TIME_PERIOD', inplace=True) #indexes the years\n",
    "df_or.index.name = None\n",
    "df_or.columns.name = None\n",
    "df_or.rename(columns = {'OBS_VALUE': 'Or'}, inplace = True)\n",
    "print(df_or.isna().sum().sum()) # Number of missing values (0)\n",
    "# show the 5 first rows\n",
    "df_or.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pétrole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Material</th>\n",
       "      <th>Petrol</th>\n",
       "      <th>Or</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-01</th>\n",
       "      <td>118.1</td>\n",
       "      <td>201.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-01</th>\n",
       "      <td>127.3</td>\n",
       "      <td>198.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-01</th>\n",
       "      <td>142.3</td>\n",
       "      <td>195.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-01</th>\n",
       "      <td>145.2</td>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-01</th>\n",
       "      <td>130.9</td>\n",
       "      <td>190.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Material    Petrol     Or\n",
       "2023-12-01   118.1  201.5\n",
       "2023-11-01   127.3  198.6\n",
       "2023-10-01   142.3  195.8\n",
       "2023-09-01   145.2  194.0\n",
       "2023-08-01   130.9  190.2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_petrol = \"https://bdm.insee.fr/series/sdmx/data/SERIES_BDM/010002091\" # URL of the serie\n",
    "\n",
    "response = requests.get(url_petrol) # Retrieve XML data\n",
    "response.raise_for_status()   # Checks that the request is successful\n",
    "xml_content = response.content\n",
    "\n",
    "\n",
    "root = ET.fromstring(xml_content) # Parse XML content\n",
    "\n",
    "root = ET.fromstring(xml_content) # Load XML content\n",
    "\n",
    "# Initialiser une liste pour stocker les données\n",
    "data = []\n",
    "\n",
    "\n",
    "for series in root.findall(\".//{*}Series\"): # Browse each series\n",
    "   \n",
    "    for obs in series.findall(\".//{*}Obs\"):  # Browse the observation in  each series\n",
    "\n",
    "       # Extract relevant attributes\n",
    "        time_period = obs.attrib.get(\"TIME_PERIOD\")\n",
    "        obs_value = obs.attrib.get(\"OBS_VALUE\")\n",
    "         # Add the data at the list\n",
    "        data.append({\"TIME_PERIOD\": time_period, \"OBS_VALUE\": obs_value})\n",
    "\n",
    "\n",
    "df_petrol = pd.DataFrame(data)  # Create a DataFrame from the extracted data\n",
    "\n",
    "# Convert columns to appropriate types\n",
    "df_petrol[\"TIME_PERIOD\"] = pd.to_datetime(df_petrol[\"TIME_PERIOD\"], format=\"%Y-%m\")\n",
    "df_petrol[\"OBS_VALUE\"] = pd.to_numeric(df_petrol[\"OBS_VALUE\"])\n",
    "\n",
    "\n",
    "# Convert TIME_PERIOD to datetime for easier filtering\n",
    "df_petrol['TIME_PERIOD'] = pd.to_datetime(df_petrol['TIME_PERIOD'], format='%Y-%m')\n",
    "\n",
    "# Filter years between 2013 and 2023\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2023-12-31'\n",
    "df_petrol = df_petrol[(df_petrol['TIME_PERIOD'] >= start_date) & (df_petrol['TIME_PERIOD'] <= end_date)]\n",
    "\n",
    "df_petrol.set_index('TIME_PERIOD', inplace=True) #indexes the years\n",
    "df_petrol.index.name = None\n",
    "df_petrol.columns.name = None\n",
    "df_petrol.rename(columns = {'OBS_VALUE': 'Petrol'}, inplace = True)\n",
    "\n",
    "print(ipch.isna().sum().sum()) # Number of missing values (0)\n",
    "# show the 5 first rows\n",
    "df_petrol.head()\n",
    "material = pd.concat((df_petrol, df_or), axis = 1, join = 'inner')\n",
    "material.columns.name = 'Material'\n",
    "material.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action\n",
    "\n",
    "## Présentation de la classe ```Indice``` pour importer les données d'un actif et calculer les indices\n",
    "\n",
    "La classe importe les données depuis YahooFinance.\n",
    "\n",
    "L'argument `ticker_symbol` suffit lors de l'instanciation. Il s'agit du symbole boursier de l'action dont les données seront téléchargées.\\\n",
    "L'argument `data` lors de l'instanciation permet de donner directement les données si elles sont téléchargées.\n",
    "\n",
    "La méthode `update` calcule les différents indices qui seront affichés par la méthode `affichage`. Elle est exécutée lors de l'instanciation.\n",
    "\n",
    "L'historique du prix de l'action est accessible via la méthode `get_data`.\\\n",
    "Les calculs des indices OBV, ADLine, ADX et Aroon sont implémentés et accessibles via la méthode `get_index`.\\\n",
    "\n",
    "## Présentation des indices\n",
    "\n",
    "### OBV\n",
    "Il s'agit d'un indicateur de momentum qui mesure les flux de volume positifs et négatifs. \n",
    "\n",
    "Si la courbe de l'OBV augmente (ou diminue) de façon prononcée, sans changement significatif du prix de l'actif, cela indique qu'à un moment, le prix devrait sauter vers le haut (ou vers le bas).\n",
    "\n",
    "Lorsque les institutions commencent à acheter un actif que les particuliers continuent de vendre, le prix est encore légèrement en baisse ou se stabilise, alors que le volume augmente. Le phénomène inverse se produit également. \n",
    "\n",
    "### ADLine\n",
    "L'ADLine (*Accumulative Distribution Line*) est un indicateur qui mesure le flux d'argent pour un actif en prenant en compte à la fois les variations de prix et les volumes.\n",
    "\n",
    "Une ADLine en hausse indique une pression d'achat accrue, souvent interprétée comme une accumulation de la part des investisseurs, tandis qu'une ADLine en baisse révèle une pression de vente ou une distribution.\n",
    "\n",
    "Une divergence entre l'ADLine et le prix de l'actif peut être utilisée pour anticiper un retournement potentiel de tendance. Par exemple, si le prix monte mais que l'ADLine chute, cela pourrait signaler un affaiblissement de la tendance haussière.\n",
    "\n",
    "### ADX\n",
    "L'ADX identifie une tendance forte lorsqu'il est au-dessus de 25 et une tendance faible lorsqu'il est en-dessous de 20. \\\n",
    "On peut également utiliser le franchissement des lignes $-DI$ et $+DI$ pour générer des signaux de trade: \n",
    "- Lorsque $+DI$ passe au-dessus de $-DI$ et que l'ADX est supérieur à 20 (idéalement à 25), alors il s'agit d'un potentiel signal pour acheter.\n",
    "- Inversement, lorsque $-DI$ passe au-dessus de $+DI$ et que l'ADX est supérieur à 20 (ou 25), il s'agit d'un potentiel signal pour vendre.\n",
    "\n",
    "### Aroon\n",
    "Indique si le prix maximal ou minimal a été atteint depuis longtemps ou non sur les dernières périodes (25 par défaut). Il peut s'agir du prix d'ouverture, de clôture, le prix maximal ou minimal sur la période. S'il est à 100, c'est que le prix maximal a été atteint la veille et que le prix minimal a été atteint avant toutes les périodes étudiées. S'il est à -100 dans le cas contraire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBV</th>\n",
       "      <th>ADLine</th>\n",
       "      <th>ADX</th>\n",
       "      <th>pDI</th>\n",
       "      <th>mDI</th>\n",
       "      <th>Aroon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>2.571420e+08</td>\n",
       "      <td>-1.238096e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>5.203304e+08</td>\n",
       "      <td>-8.057093e+07</td>\n",
       "      <td>-7.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.736432</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>6.807540e+08</td>\n",
       "      <td>-1.640083e+07</td>\n",
       "      <td>-6.414136</td>\n",
       "      <td>1.763985</td>\n",
       "      <td>1.659260</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-08</th>\n",
       "      <td>9.182120e+08</td>\n",
       "      <td>1.852661e+08</td>\n",
       "      <td>-0.613184</td>\n",
       "      <td>10.382046</td>\n",
       "      <td>1.496780</td>\n",
       "      <td>-8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-09</th>\n",
       "      <td>1.133010e+09</td>\n",
       "      <td>2.248346e+08</td>\n",
       "      <td>5.100351</td>\n",
       "      <td>12.134276</td>\n",
       "      <td>1.395131</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     OBV        ADLine       ADX        pDI       mDI  Aroon\n",
       "2015-01-05  2.571420e+08 -1.238096e+08  0.000000   0.000000  0.000000    0.0\n",
       "2015-01-06  5.203304e+08 -8.057093e+07 -7.142857   0.000000  1.736432    0.0\n",
       "2015-01-07  6.807540e+08 -1.640083e+07 -6.414136   1.763985  1.659260   -4.0\n",
       "2015-01-08  9.182120e+08  1.852661e+08 -0.613184  10.382046  1.496780   -8.0\n",
       "2015-01-09  1.133010e+09  2.248346e+08  5.100351  12.134276  1.395131  -12.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Index():\n",
    "    \"\"\"\n",
    "    A class to represent and calculate various financial indices for a given stock.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data(ticker_symbol='AAPL', period='10y'):\n",
    "        \"\"\"\n",
    "        Import stock price history.\n",
    "        \n",
    "        IN: ticker_symbol: <str> Stock identifier\n",
    "            period: <str> Period over which data is downloaded\n",
    "                    arg: '1d', '5d', '1mo', '3mo', '6mo', '1y', '2y', '5y', '10y', 'ytd', 'max'\n",
    "        OUT: <pd.Series>: Stock history\n",
    "        \"\"\"\n",
    "        ticker = yf.Ticker(ticker_symbol)  # Create a Ticker object\n",
    "        data_hist = ticker.history(period=period)  # Download historical data\n",
    "        dates = pd.Series(data_hist['Open'].index).apply(lambda x: pd.to_datetime(x.strftime('%Y-%m-%d')))  # Convert dates\n",
    "        data = data_hist.reset_index(drop=True).set_index(dates).drop(['Dividends', 'Stock Splits'], axis=1)  # Prepare data\n",
    "        data.index.name = None\n",
    "        return data\n",
    "\n",
    "    @classmethod\n",
    "    def smooth(cls, series, period=14, method='simple', fill='NoFill', alpha=1/14):\n",
    "        \"\"\"\n",
    "        Calculate moving average.\n",
    "        \n",
    "        IN: series: <Pandas Series> Series for which the moving average is calculated\n",
    "            period: <int> Number of periods used for the moving average\n",
    "            method: <str> Method used for calculating the average\n",
    "                    'simple': Calculate arithmetic average\n",
    "                    'exp': Calculate exponential average\n",
    "                    'weight': Assign increasing weights to series and calculate arithmetic average\n",
    "            fill: <str> If the first period values are initialized or left empty (fill='NoFill')\n",
    "                  'constant': Fill with the first non-null value (at position period)\n",
    "                  'data': Fill with the first period values of the series\n",
    "                  'smooth': Fill value i with the moving average of the first i+1 values of the series over a period of i+1, \n",
    "                            for i from 1 to period - 1. The same method of calculating the average is used.\n",
    "            alpha: <int> Argument for calculating the average by the 'exp' method. Must be between 0 and 1.\n",
    "        OUT: <Pandas Series>: Smoothed series\n",
    "        \"\"\"\n",
    "        match method:\n",
    "            case 'simple':\n",
    "                smoothed = series.rolling(period).sum().copy() / period  # Simple moving average\n",
    "            case 'exp':\n",
    "                smoothed = [series.iloc[0]]  # Initialize with the first value\n",
    "                for i in range(1, len(series)):\n",
    "                    smoothed += [alpha * series.iloc[i] + (1 - alpha) * smoothed[i - 1]]  # Exponential moving average\n",
    "                smoothed = pd.Series(smoothed)\n",
    "            case 'weight':\n",
    "                weight = np.array([k for k in range(1, period + 1)])  # Weights for weighted moving average\n",
    "                smoothed = series.rolling(period).apply(lambda x: np.dot(x, weight).sum() / weight.sum())  # Weighted moving average\n",
    "        \n",
    "        match fill:\n",
    "            case 'constant':\n",
    "                smoothed.iloc[:period] = smoothed.iloc[period - 1]  # Fill with constant value\n",
    "            case 'data':\n",
    "                smoothed.iloc[:period - 1] = series.iloc[:period - 1]  # Fill with initial data values\n",
    "            case 'smooth':\n",
    "                smoothed.iloc[0] = series.iloc[0]  # Initialize with the first value\n",
    "                for i in range(1, period):\n",
    "                    smoothed.iloc[i] = cls.smooth(series=series.iloc[:i + 1], period=i + 1, method=method, fill='NoFill').iloc[i]  # Smooth fill\n",
    "            case 'NoFill':\n",
    "                pass  # No fill\n",
    "\n",
    "        return smoothed\n",
    "\n",
    "    def __init__(self, name = 'Apple', ticker_symbol='AAPL', data=False) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Index class.\n",
    "        \n",
    "        IN: ticker_symbol: <str> Stock ticker symbol to study\n",
    "            data: <bool or Pandas DataFrame> If data is False, the history will be downloaded from Yahoo Finance. \n",
    "                                             Otherwise, data must contain the data downloaded from Yahoo Finance and \n",
    "                                             transformed as in the static method load_data.\n",
    "        \"\"\"\n",
    "        self.__name = name\n",
    "        self.__TICKER_SYMBOL = ticker_symbol  # Stock ticker symbol\n",
    "        self.__data = None  # Data placeholder\n",
    "        self.__index = None  # Index placeholder\n",
    "        self.__date_limits = {'Start': None, 'End': None}  # Date limits\n",
    "\n",
    "        self.__start(data)  # Initialize data\n",
    "    \n",
    "    def __start(self, data):\n",
    "        \"\"\"Start the data initialization.\"\"\"\n",
    "        if data is not False:\n",
    "            self.__data = data  # Use provided data\n",
    "        else:\n",
    "            self.__data = self.load_data(ticker_symbol = self.__TICKER_SYMBOL)  # Load data from Yahoo Finance\n",
    "\n",
    "        self.__index = pd.DataFrame(index = self.__data.index)  # Initialize index DataFrame\n",
    "        self.__date_limits['Start'] = self.__data.index[0]  # Set start date\n",
    "        self.__date_limits['End'] = self.__data.index[-1]  # Set end date\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.__name\n",
    " \n",
    "    def get_data(self):\n",
    "        \"\"\"Return the data.\"\"\"\n",
    "        return self.__data\n",
    "    \n",
    "    def get_index(self):\n",
    "        \"\"\"Return the index.\"\"\"\n",
    "        return self.__index\n",
    "   \n",
    "    def update(self, **kwargs):\n",
    "        \"\"\"Update the index with calculated indicators.\"\"\"\n",
    "        self.__index = pd.DataFrame(index=self.__data.index)  # Reset index DataFrame\n",
    "        self.OBV(**kwargs)\n",
    "        self.ADLine(**kwargs)\n",
    "        self.ADX(**kwargs)\n",
    "        self.Aroon(**kwargs)\n",
    "    \n",
    "    def add(self, indicator, dates, **kwargs):\n",
    "        \"\"\"Add a specific indicator to the index.\"\"\"\n",
    "        match indicator:\n",
    "            case 'C': return self.C(dates=dates, **kwargs)  # Add indicator 'C'\n",
    "\n",
    "    # Display methods\n",
    "    def display(self, start_date, end_date, list_indices, **kwargs):\n",
    "        \"\"\"\n",
    "        Display the graphs of the specified indices in list_indices.\n",
    "        \n",
    "        IN: start_date, end_date: <str> Start and end dates for displaying the indices\n",
    "            list_indices: <list of str> List of indices in string format to display. \n",
    "                         Multiple lists will give multiple graphs. \n",
    "                         Different indices within a single list will be displayed on the same graph.\n",
    "                         Index names: 'OBV', 'ADLine', 'ADX'\n",
    "            **kwargs: Arguments to pass to the index calculation methods\n",
    "        \"\"\"\n",
    "        self.update(**kwargs)  # Update indices\n",
    "\n",
    "        # Convert start and end date\n",
    "        start = pd.to_datetime(start_date)\n",
    "        end = pd.to_datetime(end_date)\n",
    "\n",
    "        if start < self.__date_limits['Start']: start = self.__date_limits['Start']  # Adjust start date\n",
    "        else: pass\n",
    "        if end > self.__date_limits['End']: end = self.__date_limits['End']  # Adjust end date\n",
    "        else: pass\n",
    "\n",
    "        data = self.__index.loc[start:end]  # Filter data by date range\n",
    "        x = pd.Series(data.index).apply(lambda x: x.strftime('%d/%m/%y'))  # Format dates for x-axis\n",
    "\n",
    "        fig, ax = plt.subplots(len(list_indices), 1, figsize=(20, 3 * len(list_indices)))  # Create subplots\n",
    "        fig.suptitle(f'Period {start.strftime(\"%d/%m/%y\")} - {end.strftime(\"%d/%m/%y\")}')  # Set title\n",
    "        for i, indices in enumerate(list_indices):\n",
    "            for indicator in indices:\n",
    "                if indicator in data:\n",
    "                    y = data[indicator]  # Get data for the index if present\n",
    "                else:\n",
    "                    y = self.__add(indicator, dates=data.index, **kwargs)  # Add index data if not present\n",
    "                style = kwargs.get(f'{indicator}_style', '')  # Get style for the index\n",
    "                ax[i].plot(x, y, style, label=indicator)  # Plot the index\n",
    "            ax[i].legend()\n",
    "            ax[i].set_xticks(ticks=x.iloc[np.linspace(0, 1, 20) * (len(x) - 1)])\n",
    "    \n",
    "    # Index calculation methods\n",
    "    def OBV(self, start='Close', end=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Calculate the On-Balance Volume (OBV) indicator.\n",
    "        \n",
    "        IN: start, end: <str> Among 'Open' and 'Close', the directions for adding volumes will be calculated according \n",
    "                        to the opening price ('Open') or closing price ('Close') of day n for the start and day n+1 for the end.\n",
    "            **kwargs: Exists for compatibility\n",
    "        OUT: <Pandas Series>: OBV indicator\n",
    "        \"\"\"\n",
    "        if start == 'Open' and end == 'Close':\n",
    "            direction = self.__data[end] - self.__data[start]  # Calculate direction based on Open and Close\n",
    "        elif end:\n",
    "            direction = self.__data[end] - self.__data[start].shift(1)  # Calculate direction based on shifted Close\n",
    "        else: direction = self.__data[start] - self.__data[start].shift(1)  # Calculate direction based on shifted Open\n",
    "\n",
    "        direction.iloc[0] = 1  # Set initial direction\n",
    "        OBV = (self.__data['Volume'] * direction / abs(direction)).cumsum()  # Calculate OBV\n",
    "        self.__index['OBV'] = np.where(np.isnan(OBV), OBV.shift(1), OBV)  # Handle NaN values and store OBV\n",
    "\n",
    "        return self.__index['OBV']\n",
    "        \n",
    "    def ADLine(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Calculate the Accumulation/Distribution Line (ADLine) indicator.\n",
    "        \n",
    "        IN: **kwargs: Exists for compatibility\n",
    "        OUT: <Pandas Series>: ADLine indicator\n",
    "        \"\"\"\n",
    "        MFM = ((self.__data['High'] - 2 * self.__data['Close'] + self.__data['Low'])) / \\\n",
    "              (self.__data['Low'] - self.__data['High'])  # Money Flow Multiplier\n",
    "        MFV = MFM * self.__data['Volume']  # Money Flow Volume\n",
    "\n",
    "        self.__index['ADLine'] = MFV.cumsum()  # Calculate and store ADLine\n",
    "\n",
    "        return self.__index['ADLine']\n",
    "    \n",
    "    def ADX(self, period_ADX=14, method='exp', fill='NoFill', alpha=1/14, **kwargs):\n",
    "        \"\"\"\n",
    "        Calculate the Average Directional Index (ADX) indicator.\n",
    "        \n",
    "        IN: period_ADX: <int> Number of periods used for the ADX calculation\n",
    "            method: <str> Method used for calculating the average\n",
    "                    'simple': Calculate arithmetic average\n",
    "                    'exp': Calculate exponential average\n",
    "                    'weight': Assign increasing weights to series and calculate arithmetic average\n",
    "            fill: <str> If the first period values are initialized or left empty (fill='NoFill')\n",
    "                  'constant': Fill with the first non-null value (at position period)\n",
    "                  'data': Fill with the first period values of the series\n",
    "                  'smooth': Fill value i with the moving average of the first i+1 values of the series over a period of i+1, \n",
    "                            for i from 1 to period - 1. The same method of calculating the average is used.\n",
    "            alpha: <int> Argument for calculating the average by the 'exp' method. Must be between 0 and 1.\n",
    "            **kwargs: Exists for compatibility\n",
    "        OUT: <Pandas Series>: ADX indicator\n",
    "        \"\"\"\n",
    "        DMp = np.where(self.__data['High'] - self.__data['High'].shift(1) > self.__data['Low'].shift(1) - self.__data['Low'],\n",
    "                       self.__data['High'] - self.__data['High'].shift(1), 0)  # Positive Directional Movement\n",
    "        DMm = np.where(self.__data['High'] - self.__data['High'].shift(1) <= self.__data['Low'].shift(1) - self.__data['Low'],\n",
    "                       self.__data['Low'].shift(1) - self.__data['Low'], 0)  # Negative Directional Movement\n",
    "\n",
    "        TR = pd.DataFrame([self.__data['High'] - self.__data['Low'], \n",
    "                           self.__data['High'] - self.__data['Close'].shift(1), \n",
    "                           self.__data['Close'].shift(1) - self.__data['Low']]).apply(max)  # Set True Range\n",
    "\n",
    "        # Smoothed Positive DM, Negative DM and True Range\n",
    "        DMpsmooth = self.smooth(series=pd.Series(DMp), period=period_ADX, method=method, fill=fill, alpha=alpha)\n",
    "        DMmsmooth = self.smooth(series=pd.Series(DMm), period=period_ADX, method=method, fill=fill, alpha=alpha)\n",
    "        TRsmooth = self.smooth(series=pd.Series(TR), period=period_ADX, method=method, fill=fill, alpha=alpha)\n",
    "\n",
    "        DIp = 100 * DMpsmooth / TRsmooth  # Positive Directional Indicator\n",
    "        DIm = 100 * DMmsmooth / TRsmooth  # Negative Directional Indicator\n",
    "\n",
    "        DX = 100 * (DIp - DIm) / (DIp + DIm)  # Directional Movement Index\n",
    "        DX.iloc[0] = 0  # Set initial DX\n",
    "        ADX = self.smooth(DX, period=period_ADX, method=method, fill=fill, alpha=alpha)  # Average Directional Index\n",
    "\n",
    "        # Set index for ADX, DIp, DIm\n",
    "        ADX.index = self.__index.index\n",
    "        DIp.index = self.__index.index\n",
    "        DIm.index = self.__index.index\n",
    "        \n",
    "        # Store ADX, Positive DI and Negative DI in index\n",
    "        self.__index['ADX'] = ADX\n",
    "        self.__index['pDI'] = DIp\n",
    "        self.__index['mDI'] = DIm\n",
    "        \n",
    "        return self.__index['ADX']\n",
    "    \n",
    "    def Aroon(self, period_Aroon=25, event='Close', **kwargs):\n",
    "        \"\"\"\n",
    "        Calculate the Aroon indicator.\n",
    "        \n",
    "        IN: period_Aroon: <int> Number of periods over which the index is calculated\n",
    "            event: <str> Among 'Open', 'Close', 'Low', and 'High', the Aroon index will be calculated based on the variations of these events.\n",
    "            **kwargs: Exists for compatibility\n",
    "        OUT: <Pandas Series>: Aroon indicator\n",
    "        \"\"\"\n",
    "        def indMin(i):\n",
    "            if i < period_Aroon: start, end = 0, max(i, 1)\n",
    "            else: start, end = i - period_Aroon, i\n",
    "            return np.argmin(self.__data[event].iloc[start: end])  # Index of minimum value\n",
    "        def indMax(i):\n",
    "            if i < period_Aroon: start, end = 0, max(i, 1)\n",
    "            else: start, end = i - period_Aroon, i\n",
    "            return np.argmax(self.__data[event].iloc[start: end])  # Index of maximum value\n",
    "\n",
    "        # Calculate argMin, argMax and Aroon indicator for each period\n",
    "        argMin = pd.Series(range(len(self.__data))).apply(indMin)\n",
    "        argMax = pd.Series(range(len(self.__data))).apply(indMax)\n",
    "        Aroon = 100 * (argMin - argMax) / period_Aroon\n",
    "        \n",
    "        # Store Aroon in index\n",
    "        Aroon.index = self.__index.index\n",
    "        self.__index['Aroon'] = Aroon\n",
    "\n",
    "        return self.__index['Aroon']\n",
    "    \n",
    "    # Other lines\n",
    "    def C(self, dates, c=25):\n",
    "        \"\"\"\n",
    "        Generate a horizontal curve.\n",
    "        \n",
    "        IN: dates: <Pandas Index> Dates for the curve\n",
    "            c: <int> Height of the curve\n",
    "        OUT: <Pandas Series>: Horizontal curve\n",
    "        \"\"\"\n",
    "        return pd.Series(range(len(self.__index.index)), index=dates)\n",
    "    \n",
    "apple = Index()\n",
    "apple.update()\n",
    "apple.get_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection d'action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_entreprise = {\n",
    "    # Entreprises du CAC 40\n",
    "    \"Accor\": \"AC.PA\",\n",
    "    \"Air Liquide\": \"AI.PA\",\n",
    "    \"Airbus\": \"AIR.PA\",\n",
    "    \"ArcelorMittal\": \"MT.AS\",\n",
    "    \"BNP Paribas\": \"BNP.PA\",\n",
    "    \n",
    "    # Entreprises défensives\n",
    "    \"Nestlé\": \"NESN.SW\",\n",
    "    \"Unilever\": \"ULVR.L\",\n",
    "    \"Sanofi\": \"SAN.PA\",\n",
    "    \"Novo Nordisk\": \"NOVO-B.CO\",\n",
    "    \"GlaxoSmithKline\": \"GSK.L\",\n",
    "    \n",
    "    # Entreprises cycliques\n",
    "    \"Volkswagen\": \"VOW3.DE\",\n",
    "    \"BMW\": \"BMW.DE\",\n",
    "    \"Daimler\": \"DAI.DE\",\n",
    "    \"LVMH\": \"MC.PA\",\n",
    "    \"Hermès\": \"RMS.PA\",\n",
    "    \n",
    "    # Entreprises de valeur\n",
    "    \"TotalEnergies\": \"TTE.PA\",\n",
    "    \"Schneider Electric\": \"SU.PA\",\n",
    "    \"Airbus\": \"AIR.PA\",\n",
    "    \"Sanofi\": \"SAN.PA\",\n",
    "    \"L'Oréal\": \"OR.PA\",\n",
    "    \n",
    "    # Entreprises avec corrélations probables avec des indices macro-économiques ou techniques\n",
    "    \"SAP\": \"SAP.DE\",\n",
    "    \"ASML\": \"ASML.AS\",\n",
    "    \"Siemens\": \"SIE.DE\",\n",
    "    \"Schneider Electric\": \"SU.PA\",\n",
    "    \"Air Liquide\": \"AI.PA\",\n",
    "    \"L'Oréal\": \"OR.PA\",\n",
    "    \"Danone\": \"BN.PA\",\n",
    "    \"Kering\": \"KER.PA\",\n",
    "    \"Orange\": \"ORA.PA\",\n",
    "    \"Publicis\": \"PUB.PA\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$DAI.DE: possibly delisted; no price data found  (period=10y) (Yahoo error = \"No data found, symbol may be delisted\")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Dividends', 'Stock Splits'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Iterate over each company in the dictionary\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m company, ticker \u001b[38;5;129;01min\u001b[39;00m list_entreprise\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Instantiate an Index object for the company\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     index_obj \u001b[38;5;241m=\u001b[39m \u001b[43mIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Update the index to calculate technical indices\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     index_obj\u001b[38;5;241m.\u001b[39mupdate()\n",
      "Cell \u001b[0;32mIn[10], line 83\u001b[0m, in \u001b[0;36mIndex.__init__\u001b[0;34m(self, name, ticker_symbol, data)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Index placeholder\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__date_limits \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnd\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}  \u001b[38;5;66;03m# Date limits\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__start\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 90\u001b[0m, in \u001b[0;36mIndex.__start\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data \u001b[38;5;241m=\u001b[39m data  \u001b[38;5;66;03m# Use provided data\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker_symbol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__TICKER_SYMBOL\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load data from Yahoo Finance\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__index \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data\u001b[38;5;241m.\u001b[39mindex)  \u001b[38;5;66;03m# Initialize index DataFrame\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__date_limits[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Set start date\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m, in \u001b[0;36mIndex.load_data\u001b[0;34m(ticker_symbol, period)\u001b[0m\n\u001b[1;32m     17\u001b[0m data_hist \u001b[38;5;241m=\u001b[39m ticker\u001b[38;5;241m.\u001b[39mhistory(period\u001b[38;5;241m=\u001b[39mperiod)  \u001b[38;5;66;03m# Download historical data\u001b[39;00m\n\u001b[1;32m     18\u001b[0m dates \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(data_hist[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpen\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: pd\u001b[38;5;241m.\u001b[39mto_datetime(x\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)))  \u001b[38;5;66;03m# Convert dates\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata_hist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdates\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDividends\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStock Splits\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Prepare data\u001b[39;00m\n\u001b[1;32m     20\u001b[0m data\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/local/stow/conda/miniforge3/envs/cremi/lib/python3.12/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/local/stow/conda/miniforge3/envs/cremi/lib/python3.12/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m/opt/local/stow/conda/miniforge3/envs/cremi/lib/python3.12/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/local/stow/conda/miniforge3/envs/cremi/lib/python3.12/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Dividends', 'Stock Splits'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Initialize empty DataFrames for historical actions and indices\n",
    "hist_action = pd.DataFrame()\n",
    "hist_index = pd.DataFrame()\n",
    "\n",
    "# Iterate over each company in the dictionary\n",
    "for company, ticker in list_entreprise.items():\n",
    "    # Instantiate an Index object for the company\n",
    "    index_obj = Index(company, ticker)\n",
    "    \n",
    "    # Update the index to calculate technical indices\n",
    "    index_obj.update()\n",
    "    \n",
    "    # Get the historical action data and append it to hist_action\n",
    "    action_data = index_obj.get_data()\n",
    "    action_data['Company'] = company  # Add a column for the company name\n",
    "    hist_action = pd.concat([hist_action, action_data])\n",
    "    \n",
    "    # Get the historical index data\n",
    "    index_data = index_obj.get_index()\n",
    "    \n",
    "    # Create a MultiIndex for the columns\n",
    "    index_data.columns = pd.MultiIndex.from_product([[company], index_data.columns], names=['Company', 'Index'])\n",
    "    \n",
    "    # Concatenate the index data to hist_index\n",
    "    hist_index = pd.concat([hist_index, index_data], axis=1)\n",
    "\n",
    "# Set the index name for hist_action\n",
    "hist_action.set_index('Company', inplace=True)\n",
    "\n",
    "# Display the resulting DataFrames\n",
    "print(hist_action)\n",
    "print(hist_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion des dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_countries = ipch.columns.intersection(\\\n",
    "                   dette_publique.columns.intersection(\\\n",
    "                   chomage.columns.intersection(\\\n",
    "                    pib.columns)))\n",
    "\n",
    "common_dates = ipch.index.intersection(\\\n",
    "                   dette_publique.index.intersection(\\\n",
    "                   chomage.index.intersection(\\\n",
    "                    pib.index)))\n",
    "\n",
    "expected_dates = pd.date_range(start='2013-01-01', end='2023-01-01', freq='MS')\n",
    "all_dates_present = expected_dates.isin(common_dates).all()\n",
    "\n",
    "print(f'Toutes les dates du {common_dates[-1].strftime('%d/%m/%Y')}\\\n",
    "       au {common_dates[0].strftime('%d/%m/%Y')} sont présentes' \\\n",
    "        if all_dates_present else 'Il manque des dates')\n",
    "\n",
    "def set_index(df, index_name):\n",
    "    df = df.loc[common_dates, common_countries]\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # Ajouter le nouvel index au niveau supérieur du MultiIndex existant\n",
    "        new_index = pd.MultiIndex.from_tuples([(index_name, *idx) \\\n",
    "                        for idx in df.columns], names=['Type'] + df.columns.names)\n",
    "    else:\n",
    "        # Créer un MultiIndex en juxtaposant le nouvel index et l'index existant\n",
    "        new_index = pd.MultiIndex.from_tuples([(index_name, idx) \n",
    "                        for idx in df.columns], names=['Type', df.columns.names[0]])\n",
    "    df.columns = new_index\n",
    "    return df\n",
    "dette_publique = set_index(dette_publique, 'Dette publique')\n",
    "chomage = set_index(chomage, 'Chomage')\n",
    "ipch = set_index(ipch, 'IPCH')\n",
    "pib = set_index(pib, 'PIB')\n",
    "\n",
    "data = pd.concat((dette_publique, chomage, ipch, pib), axis = 1, join = 'inner')\n",
    "data = data.groupby('Type', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.get_group('IPCH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichage\n",
    "\n",
    "L'affichage des graphiques se fait trois fois, je ne sais pas pourquoi.\\\n",
    "Pour l'affichage de la carte, j'ai commencé à coder une fonction tout en bas. Il manque la traduction des noms des pays. Je ne sais pas si ça va fonctionner après ça, à voir.\n",
    "\n",
    "## Fonction d'affichage de graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(countries, start, end, data, data_type):\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    for country in countries.split(', '):\n",
    "        data.loc[start:end, (data_type, country)].plot(label=f'{country}')\n",
    "\n",
    "    plt.title(f'{data_type} ({start.strftime(\"%m/%Y\")} - {end.strftime(\"%m/%Y\")})')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction d'affichage de carte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GeoJSON file into a GeoDataFrame\n",
    "url = \"https://raw.githubusercontent.com/leakyMirror/map-of-europe/master/GeoJSON/europe.geojson\"\n",
    "europe = gpd.read_file(url)\n",
    "\n",
    "def plot_map(date: str, data: pd.core.groupby.DataFrameGroupBy, data_type: str):\n",
    "    \"\"\"\n",
    "    Plot a map of Europe for the specified date and data type.\n",
    "    \n",
    "    Args:\n",
    "        date (str): Date in 'YYYY-MM' format for which to filter the data.\n",
    "        data (pd.core.groupby.DataFrameGroupBy): A grouped DataFrame with MultiIndex columns\n",
    "            ('Type', 'Country') and a DateTimeIndex.\n",
    "        data_type (str): The type of data to display (e.g., 'IPCH').\n",
    "    \"\"\"\n",
    "    # Prepare the data\n",
    "    df = data.get_group(data_type).copy()\n",
    "    df.columns = df.columns.droplevel('Type')\n",
    "    df.rename(columns={\n",
    "        \"Albanie\": \"Albania\",\n",
    "        \"Allemagne\": \"Germany\",\n",
    "        \"Andorre\": \"Andorra\",\n",
    "        \"Autriche\": \"Austria\",\n",
    "        \"Belgique\": \"Belgium\",\n",
    "        \"Biélorussie\": \"Belarus\",\n",
    "        \"Bosnie-Herzégovine\": \"Bosnia and Herzegovina\",\n",
    "        \"Bulgarie\": \"Bulgaria\",\n",
    "        \"Croatie\": \"Croatia\",\n",
    "        \"Danemark\": \"Denmark\",\n",
    "        \"Espagne\": \"Spain\",\n",
    "        \"Estonie\": \"Estonia\",\n",
    "        \"Finlande\": \"Finland\",\n",
    "        \"France\": \"France\",\n",
    "        \"Grèce\": \"Greece\",\n",
    "        \"Hongrie\": \"Hungary\",\n",
    "        \"Irlande\": \"Ireland\",\n",
    "        \"Islande\": \"Iceland\",\n",
    "        \"Italie\": \"Italy\",\n",
    "        \"Kosovo\": \"Kosovo\",\n",
    "        \"Lettonie\": \"Latvia\",\n",
    "        \"Liechtenstein\": \"Liechtenstein\",\n",
    "        \"Lituanie\": \"Lithuania\",\n",
    "        \"Luxembourg\": \"Luxembourg\",\n",
    "        \"Malte\": \"Malta\",\n",
    "        \"Moldavie\": \"Moldova\",\n",
    "        \"Monaco\": \"Monaco\",\n",
    "        \"Monténégro\": \"Montenegro\",\n",
    "        \"Norvège\": \"Norway\",\n",
    "        \"Pays-Bas\": \"Netherlands\",\n",
    "        \"Pologne\": \"Poland\",\n",
    "        \"Portugal\": \"Portugal\",\n",
    "        \"République tchèque\": \"Czech Republic\",\n",
    "        \"Roumanie\": \"Romania\",\n",
    "        \"Royaume-Uni\": \"United Kingdom\",\n",
    "        \"Russie\": \"Russia\",\n",
    "        \"Saint-Marin\": \"San Marino\",\n",
    "        \"Serbie\": \"Serbia\",\n",
    "        \"Slovaquie\": \"Slovakia\",\n",
    "        \"Slovénie\": \"Slovenia\",\n",
    "        \"Suède\": \"Sweden\",\n",
    "        \"Suisse\": \"Switzerland\",\n",
    "        \"Ukraine\": \"Ukraine\",\n",
    "        \"Vatican\": \"Vatican City\"\n",
    "    }, inplace=True)\n",
    "\n",
    "    df.index.name = None\n",
    "    df.columns.name = 'Material'\n",
    "\n",
    "    # Melt the DataFrame to long format for merging\n",
    "    df_melted = df.reset_index().melt(id_vars='index', var_name='Country', value_name='Value')\n",
    "    df_melted.rename(columns={'index': 'Date'}, inplace=True)\n",
    "\n",
    "    # Merge GeoJSON data with DataFrame\n",
    "    europe_merged = europe.merge(df_melted, left_on='NAME', right_on='Country', how='left')\n",
    "\n",
    "    # Plot the data\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    europe_merged.plot(column='Value', ax=ax, legend=True, cmap='viridis', \n",
    "                       missing_kwds={\"color\": \"lightgrey\"},\n",
    "                       legend_kwds={'label': data_type})\n",
    "\n",
    "    plt.title(f\"Map of {data_type} in Europe in {pd.to_datetime(date).strftime('%B %Y')}\", fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widgets pour sélectionner les pays et les dates\n",
    "countries_widget = widgets.Text(\n",
    "    value='France, Allemagne, Italie',\n",
    "    description='Countries:',\n",
    "    placeholder='Enter countries separated by commas'\n",
    ")\n",
    "\n",
    "start_date_widget = widgets.DatePicker(\n",
    "    value=pd.to_datetime('2015-1', format='%Y-%m'),\n",
    "    description='Start Date'\n",
    ")\n",
    "\n",
    "end_date_widget = widgets.DatePicker(\n",
    "    value=pd.to_datetime('2020-3', format='%Y-%m'),\n",
    "    description='End Date'\n",
    ")\n",
    "\n",
    "# Widget pour choix multiple des données à afficher\n",
    "multi_choice_widget = widgets.SelectMultiple(\n",
    "    options=['IPCH', 'Dette publique', 'PIB', 'Chomage'],\n",
    "    value=['IPCH'],\n",
    "    description='Select Data',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Widget case à cocher\n",
    "checkbox_widget = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Map',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Fonction générale pour tracer les données\n",
    "@interact_manual(countries=countries_widget, \\\n",
    "                 start_date=start_date_widget, end_date=end_date_widget, \\\n",
    "                 type=multi_choice_widget, map=checkbox_widget)\n",
    "def plot_G(countries, start_date, end_date, type, map = True):\n",
    "    if map: plot_map(date = start_date, data = data, data_type= type[0])\n",
    "    else:\n",
    "        for t in type:\n",
    "            plot_graph(countries, start_date, end_date, data.get_group(t), t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage matières premières"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the data\n",
    "def plot_data(start_date, end_date, show_or, show_petrol):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    start_date, end_date = pd.to_datetime(start_date), pd.to_datetime(end_date)\n",
    "    \n",
    "    # Filter the data based on the selected dates\n",
    "    filtered_material = material[(material.index >= start_date) & (material.index <= end_date)]\n",
    "    \n",
    "    if show_or:\n",
    "        sb.lineplot(data=filtered_material, x=filtered_material.index, y='Or', label='Or', marker='o')\n",
    "        # If 'Or' checkbox is checked, plot the data for 'Or'\n",
    "\n",
    "    if show_petrol:\n",
    "        sb.lineplot(data=filtered_material, x=filtered_material.index, y='Petrol', label='Pétrole', marker='o')\n",
    "        # If 'Petrol' checkbox is checked, plot the data for 'Petrol'\n",
    "\n",
    "    # Configure the plot with a title, legend, and grid\n",
    "    plt.title(f'Cour de l\\'or et du pétrole en euros ({start_date.strftime(\"%m/%Y\")} - {end_date.strftime(\"%m/%Y\")})')\n",
    "    plt.legend()  # The legend is automatically updated based on the checked datasets\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Widgets for selecting the start and end dates, and options to display data\n",
    "start_date_widget = widgets.DatePicker(description='Début', value=pd.to_datetime('2013-01-01'))\n",
    "end_date_widget = widgets.DatePicker(description='Fin', value=pd.to_datetime('2023-12-31'))\n",
    "show_or_widget = widgets.Checkbox(description='Or', value=True)  # Checkbox for showing 'Or' data\n",
    "show_petrol_widget = widgets.Checkbox(description='Pétrole', value=True)  # Checkbox for showing 'Petrol' data\n",
    "\n",
    "# Interactive interface to control the plot function with widgets\n",
    "interact(\n",
    "    plot_data,  # The function to interact with\n",
    "    start_date=start_date_widget,  # Start date widget\n",
    "    end_date=end_date_widget,  # End date widget\n",
    "    show_or=show_or_widget,  # 'Or' checkbox widget\n",
    "    show_petrol=show_petrol_widget  # 'Petrol' checkbox widget\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage des devises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot currency data\n",
    "def plot_devise_data(select_all, start_date, end_date, **currency_checkboxes):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # List of selected currencies\n",
    "    if select_all:\n",
    "        selected_currencies = devise.columns.tolist()  # If \"select all\" is checked, include all currencies\n",
    "    else:\n",
    "        selected_currencies = [currency for currency, is_selected in currency_checkboxes.items() if is_selected]\n",
    "        # If not, include only the selected currencies based on the checkboxes\n",
    "\n",
    "    # Filter the data based on the selected date range and currencies\n",
    "    filtered_data = devise[(devise.index >= start_date) & (devise.index <= end_date)]\n",
    "    filtered_data = filtered_data[selected_currencies]\n",
    "\n",
    "    # Plot the time series for each selected currency\n",
    "    for currency in selected_currencies:\n",
    "        sb.lineplot(data=filtered_data, x=filtered_data.index, y=currency, label=currency, marker='o')\n",
    "\n",
    "    # Configure the plot with a title, x and y labels, and a legend\n",
    "    plt.title(f'Valeurs des devises (équivalent en euros) ({start_date.strftime(\"%m/%Y\")} - {end_date.strftime(\"%m/%Y\")})')\n",
    "    plt.xlabel('TIME_PERIOD')\n",
    "    plt.ylabel('OBS_VALUE')\n",
    "    plt.legend(title='Currency')  # Currency legend\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Widgets for selecting the start and end dates\n",
    "start_date_widget = widgets.DatePicker(description='Début', value=pd.to_datetime('2013-01-01'))\n",
    "end_date_widget = widgets.DatePicker(description='Fin', value=pd.to_datetime('2023-12-31'))\n",
    "\n",
    "# Dynamically generate checkboxes for each currency\n",
    "currency_checkboxes = {\n",
    "    currency: widgets.Checkbox(description=currency, value=False)  # Default value is False (unchecked)\n",
    "    for currency in devise.columns\n",
    "}\n",
    "\n",
    "# Checkbox to \"Select All\" currencies\n",
    "select_all_widget = widgets.Checkbox(description='Tout sélectionner', value=False)\n",
    "\n",
    "# Function to dynamically update checkboxes based on \"Select All\"\n",
    "def update_checkboxes(change):\n",
    "    for checkbox in currency_checkboxes.values():\n",
    "        checkbox.value = change['new']  # Update the state of checkboxes based on the \"Select All\" checkbox\n",
    "\n",
    "select_all_widget.observe(update_checkboxes, names='value')  # Observe changes to the \"Select All\" checkbox\n",
    "\n",
    "# Create a container for all the checkboxes\n",
    "checkbox_container = VBox([select_all_widget] + list(currency_checkboxes.values()))\n",
    "\n",
    "# Interactive interface to control the plot function with widgets\n",
    "interact(\n",
    "    plot_devise_data,  # The function to interact with\n",
    "    select_all=select_all_widget,  # \"Select All\" widget\n",
    "    start_date=start_date_widget,  # Start date widget\n",
    "    end_date=end_date_widget,  # End date widget\n",
    "    **currency_checkboxes  # Pass each currency checkbox widget as a parameter\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrélations\n",
    "\n",
    "Argument: data, type, historique (pd.Series), countries\\\n",
    "Calcule la correlation entre (colonne et historique, pour colonne dans data.get_group(type).loc[;, countries])\\\n",
    "Fais une moyenne des correlation.\\\n",
    "Retourne la correlation moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(data, variables):\n",
    "  \"\"\"Return the correlation matrix for specified variables.\"\"\"\n",
    "  data_flat = data.apply(lambda x: x.droplevel(0, axis=1))  # Flatten MultiIndex\n",
    "  return data_flat[variables].corr()  # Compute and return correlation matrix\n",
    "\n",
    "#test\n",
    "variables = ['PIB', 'Chomage','IPCH']\n",
    "correlation_matrix(data, variables).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_correlation(data, hist_action, country, data_type, company):\n",
    "  \"\"\"compute avg correlation between company in hist_action and country in data\"\"\"\n",
    "  col_company = hist_action.xs(company, axis=1, level='Company')  # select company column\n",
    "  col_country = data.xs(data_type, axis=1, level='Type')[country]  # select country column\n",
    "  correlations = [col_company[c].corr(col_country[c]) for c in col_company.columns]  # compute correlations\n",
    "  return sum(correlations) / len(correlations)  # return mean correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_material_correlation(material, hist_action, commodity, company):\n",
    "  \"\"\"compute avg correlation between commodity in material and company in hist_action\"\"\"\n",
    "  col_material = material[commodity]  # select commodity column\n",
    "  col_company = hist_action.xs(company, axis=1, level='Company')  # select company column\n",
    "  correlations = [col_material.corr(col_company[c]) for c in col_company.columns]  # compute correlations\n",
    "  return sum(correlations) / len(correlations)  # return mean correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_currency_correlation(devise, hist_action, currency, company):\n",
    "  \"\"\"compute avg correlation between company in hist_action and currency in devise\"\"\"\n",
    "  col_company = hist_action.xs(company, axis=1, level='Company')  # select company column\n",
    "  col_currency = devise[currency]  # select currency column\n",
    "  correlations = [col_company[c].corr(col_currency) for c in col_company.columns]  # compute correlations\n",
    "  return sum(correlations) / len(correlations)  # return mean correlation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
