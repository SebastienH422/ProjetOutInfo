{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PIB :** https://donnees.banquemondiale.org/indicateur/NY.GDP.MKTP.CD  \n",
    "**Taux de chômage :** https://ec.europa.eu/eurostat/databrowser/view/UNE_RT_M__custom_14826434/default/table?lang=fr\n",
    "**IPCH :**  https://ec.europa.eu/eurostat/databrowser/view/PRC_HICP_MANR__custom_14819170/default/table?lang=fr\n",
    "**Historique des actions :**  \n",
    "**Devise:** https://ec.europa.eu/eurostat/databrowser/view/tec00033/default/table?lang=en&category=t_ert  \n",
    "**Matières premières:** https://bdm.insee.fr/series/sdmx/data/SERIES_BDM/010002100 **et** https://bdm.insee.fr/series/sdmx/data/SERIES_BDM/010002091  \n",
    "**Dette publique:** https://ec.europa.eu/eurostat/databrowser/view/sdg_17_40/default/table?lang=fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type d'analyses prévus et résultats attendus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses prévues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Corrélations entre les différentes données  \n",
    "- Etude d'indices boursiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultats attendus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIB\n",
    "Un PIB croissant est souvent associé à une économie forte, ce qui peut influencer positivement les marchés boursiers. L'analyse cherchera à quantifier cette relation.  \n",
    "\n",
    "### Taux de chômage\n",
    "Un faible taux de chômage peut refléter une économie robuste et un climat favorable aux entreprises, impactant ainsi les actions. Les corrélations entre ces données et les performances boursières seront examinées.   \n",
    "\n",
    "### IPCH (Indice des Prix à la Consommation Harmonisé)\n",
    "L'inflation, mesurée ici par l'IPCH, est un facteur clé pour comprendre les ajustements des marchés financiers aux variations des taux d'intérêt et des prix.  \n",
    "\n",
    "### Historique des actions\n",
    "L'analyse des tendances passées dans les cours des actions permettra d'évaluer la réactivité des marchés aux changements des indicateurs économiques.  \n",
    "\n",
    "### Devise\n",
    "Les fluctuations des taux de change peuvent avoir un impact direct, notamment pour les entreprises opérant à l'international. Les relations entre les cours des actions et les variations des devises seront explorées.  \n",
    "\n",
    "### Matières premières\n",
    "Certains secteurs boursiers sont fortement dépendants des prix des matières premières. L'étude analysera les corrélations spécifiques entre ces prix et les performances des actions dans les secteurs concernés.  \n",
    "\n",
    "### Dette intérieure\n",
    "Le niveau d'endettement d'un pays peut influencer la confiance des investisseurs et, par conséquent, le comportement des marchés. L'étude des corrélations dans ce contexte sera essentielle.  \n",
    "\n",
    "## Résultats\n",
    "Nous avons cinq dataframe:\n",
    "- `data`: Un multiIndex en colonne: `('Type', 'Country')` avec `'Type'` décrivant les différents indices macro-économiques (PIB, IPCH, dette publique, ...) et `'Country'` décrivant le pays. Elle contient l'historique des différents indices macro-économiques.\n",
    "- `hist_action`, `hist_index`: Pour `'hist_action'`, un multiIndex en colonne: `('Company', 'Price')` avec `'Company'` qui correspond au nom de l'entreprise, et `'Price'` qui correspond aux différents prix du jour (prix d'ouverture, de cloture, volume d'échange, ...). Pour `'hist_index'`, un multiIndex en colonne: `'('Company', 'Index')'` avec `'Company'` qui correspond au nom de l'entreprise et `'Index'` qui correspond à l'indice technique calculé. Elles contiennent l'historique des actions étudiées et des indices techniques calculés (qui ont été expliqués plus bas).\n",
    "- `devise` qui a en colonne un index contenant des strings correspondant au nom des devises étudiées.\n",
    "- `'material'` qui a en colonne un index contenant les strings `'Or'` et `'Petrol'`. Elle contient l'historique des prix en euros de ces deux matières premières.\n",
    "Toutes les dataframes ont un index sur les lignes au format DateTimeIndex de pandas.\n",
    "\n",
    "Toutes les dataframes ont des fonctions d'affichage. Pour les indices techniques et les actions, les fonctions d'affichages sont présents dans la classe `Index` avec la méthode `display`.\n",
    "De plus, des premières fonctions d'analyse ont été implémentées pour calculer la corrélation entre le prix des actions et les autres données (indices macro-économiques et indices techniques)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Début du code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import warnings\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "\n",
    "import ipywidgets as  widgets\n",
    "from ipywidgets import interact, widgets, VBox, HBox\n",
    "from ipywidgets import interact_manual\n",
    "import geopandas as gpd\n",
    "\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dette publique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data from the Excel file\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "# Importer les données depuis l'URL\n",
    "dette_publique_url = 'https://sebastien-hein.emi.u-bordeaux.fr/OI-sbzrthstrm/DATA/dette_pub.xlsx'\n",
    "code_url = 'https://sebastien-hein.emi.u-bordeaux.fr/OI-sbzrthstrm/DATA/code.tsv'\n",
    "\n",
    "# Charger les fichiers directement depuis l'URL\n",
    "dette_publique = pd.read_excel(dette_publique_url, sheet_name='Feuille 1')\n",
    "code = pd.read_csv(code_url, sep='\\t')\n",
    "\n",
    "# Define column names for code and label\n",
    "col_code = 'CODE'\n",
    "col_label = 'Label - French'\n",
    "\n",
    "def find_value(x):\n",
    "    \"\"\"Find the label value based on the code.\"\"\"\n",
    "    matched = not code[code[col_code] == x].empty\n",
    "    if matched:\n",
    "        return code[code[col_code] == x].loc[:, col_label].iloc[0]\n",
    "    elif x == 'TIME':\n",
    "        return 'Country'\n",
    "    return None\n",
    "\n",
    "# Modify the index of the dataframe\n",
    "dette_publique.index = dette_publique.iloc[:, 0].apply(find_value)\n",
    "dette_publique.index.name = None\n",
    "\n",
    "# Set the column names based on the 'Country' row\n",
    "dette_publique.columns = dette_publique.loc['Country']\n",
    "\n",
    "# Filter the dataframe to include only rows from 'Belgique' to 'Suède' onwards and exclude the 'TIME' column\n",
    "# Indeed, only the countries interest us,\n",
    "# and the datas are missing for Islande, Norvège, Suisse and United Kingdom.\n",
    "dette_publique = dette_publique.loc['Belgique':'Suède', dette_publique.columns != 'TIME']\n",
    "\n",
    "# Filter the dataframe to include only columns from the year 2002 onwards\n",
    "dette_publique = dette_publique.loc[:,2002:] # Problème non résolu: Si l'on prend une date inférieure à 2002, \n",
    "                                             #                      l'interpolation ne fonctionne nul part.\n",
    "\n",
    "def to_date(x):\n",
    "    \"\"\"Convert a value to datetime.\"\"\"\n",
    "    return pd.to_datetime(x, format='%Y')\n",
    "\n",
    "# Vectorize the to_date function\n",
    "vect_to_date = np.vectorize(to_date)\n",
    "\n",
    "# Convert the columns to datetime\n",
    "dette_publique.columns = vect_to_date(dette_publique.columns.values)\n",
    "\n",
    "monthly_dates = pd.date_range(start=dette_publique.columns.values[0], end=dette_publique.columns.values[-1], freq='MS')\n",
    "\n",
    "# Add columns for each month from 2013 to 2023\n",
    "dette_publique = dette_publique.reindex(columns=dette_publique.columns.union(monthly_dates))\n",
    "\n",
    "def fill_val(x):\n",
    "    \"\"\"Fill missing values by resampling and interpolating.\"\"\"\n",
    "    return x.resample('MS').interpolate(method='quadratic')\n",
    "\n",
    "# Apply the fill_val function to each row\n",
    "dette_publique = dette_publique.apply(func=fill_val, axis=1).T\n",
    "dette_publique.columns.name = 'Country'\n",
    "\n",
    "dette_publique.isna().sum().sum() # Number of missing values (0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albanie: [('2010-12', '2016-11')]\n",
      "Monténégro: [('2010-12', '2015-11')]\n",
      "United Kingdom: [('2020-12', '2024-11')]\n",
      "Kosovo*: [('2010-12', '2016-11')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define URLs for the data sources\n",
    "ipch_url = 'https://sebastien-hein.emi.u-bordeaux.fr/OI-sbzrthstrm/DATA/ipch.tsv'\n",
    "code_cp_url = 'https://sebastien-hein.emi.u-bordeaux.fr/OI-sbzrthstrm/DATA/code_cp.tsv'\n",
    "\n",
    "# Load the data directly from the URLs\n",
    "ipch = pd.read_csv(ipch_url, sep='\\t')  # Load the ipch data using tab as a separator\n",
    "code_cp = pd.read_csv(code_cp_url, sep='\\t')  # Load the code_cp data using tab as a separator\n",
    "\n",
    "\n",
    "# Set index\n",
    "def get_CP(x): return x[8:12]  # Extract CP code\n",
    "def get_id(x): return x[13:]  # Extract id\n",
    "vect_get_cp = np.vectorize(get_CP)\n",
    "vect_get_id = np.vectorize(get_id)\n",
    "ipch['CP'] = vect_get_cp(ipch.iloc[:, 0])  # Apply CP extraction\n",
    "ipch['id'] = vect_get_id(ipch.iloc[:, 0])  # Apply id extraction\n",
    "\n",
    "ipch = ipch[ipch['CP'] == 'CP00']\n",
    "ipch.drop(columns = 'CP', inplace = True)\n",
    "\n",
    "ipch.drop(columns='freq,unit,coicop,geo\\\\TIME_PERIOD', inplace=True)  # Drop unnecessary columns\n",
    "ipch['country'] = ipch.loc[:, 'id'].apply(find_value)  # Find country names\n",
    "ipch.set_index(['country'], inplace=True)  # Set index\n",
    "ipch.drop(columns='id', inplace=True)  # Drop id column\n",
    "\n",
    "# Convert the columns to datetime\n",
    "def to_date_M(x):\n",
    "    \"\"\"Convert a value to datetime.\"\"\"\n",
    "    try:\n",
    "        return pd.to_datetime(x[:-1], format='%Y-%m')\n",
    "    except:\n",
    "        print('fail')\n",
    "        return x\n",
    "\n",
    "vect_to_date_M = np.vectorize(to_date_M)\n",
    "ipch.columns = vect_to_date_M(ipch.columns.values)  # Apply datetime conversion\n",
    "\n",
    "# Filter data\n",
    "ipch = ipch[~ipch.index.str.startswith(('Union', 'Zone', 'Espace'))]  # Exclude certain countries\n",
    "\n",
    "# Clean and convert to numeric\n",
    "ipch = ipch.map(lambda x: pd.to_numeric(\n",
    "    str(x).replace(' ', '').replace('d', ''), errors='coerce'))\n",
    "ipch = ipch.T\n",
    "ipch.columns.name = 'Country'\n",
    "\n",
    "# Missing values\n",
    "# Initialiser le dictionnaire pour stocker les plages de dates manquantes\n",
    "missing_ranges = defaultdict(list)\n",
    "\n",
    "# Identifier les valeurs manquantes\n",
    "missing_values = ipch.isna()\n",
    "\n",
    "# Parcourir chaque pays (colonne) pour trouver les plages de dates manquantes\n",
    "for country in ipch.columns:\n",
    "    country_missing = missing_values[country]\n",
    "    if not country_missing.empty:\n",
    "        # Trouver les plages de dates manquantes\n",
    "        missing_dates = country_missing[country_missing].index\n",
    "        start_date = None\n",
    "        for date in missing_dates:\n",
    "            if start_date is None:\n",
    "                start_date = date\n",
    "            if (date + pd.DateOffset(months=1)) not in missing_dates:\n",
    "                end_date = date\n",
    "                missing_ranges[country].append((start_date.strftime('%Y-%m'), end_date.strftime('%Y-%m')))\n",
    "                start_date = None\n",
    "\n",
    "# Convertir le defaultdict en dict\n",
    "missing_ranges = dict(missing_ranges)\n",
    "\n",
    "for country, dates in missing_ranges.items():\n",
    "    print(f'{country}: {dates}')\n",
    "\n",
    "# Delete the country for which the missing data is on a bigger period than 4 years\n",
    "ipch.drop(columns = 'Albanie, Kosovo*, Monténégro'.split(', '), inplace = True)\n",
    "\n",
    "# Fill missing values using interpolation as the most consistent method for long gaps\n",
    "ipch.interpolate(method='time', inplace=True, limit_direction='both')  # Interpolate linearly by date for smoother transitions\n",
    "\n",
    "# Optionally fill remaining missing values (if interpolation failed for some edge cases) with column mean\n",
    "ipch.fillna(ipch.mean(), inplace=True)\n",
    "\n",
    "ipch.isna().sum().sum() # Number of missing values (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chomage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_xml(file_url):\n",
    "    \"\"\"Parse XML file and extract data into a DataFrame.\"\"\"\n",
    "    \n",
    "    # Download the XML file content using requests\n",
    "    response = requests.get(file_url)\n",
    "    xml_content = response.text  # Get the content of the XML file as a string\n",
    "    \n",
    "    # Parse the XML content with ElementTree\n",
    "    root = ET.fromstring(xml_content)  # Parse the XML string directly\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    data = []\n",
    "    columns = set()\n",
    "    rows = set()\n",
    "\n",
    "    # Extract data from <Series> and <Obs> tags\n",
    "    for series in root.findall('.//Series'):\n",
    "        geo = series.attrib.get('geo')  # Get \"geo\" attribute\n",
    "        if geo:\n",
    "            rows.add(geo)\n",
    "            for obs in series.findall('Obs'):\n",
    "                time_period = obs.attrib.get('TIME_PERIOD')  # Get \"TIME_PERIOD\" attribute\n",
    "                obs_value = obs.attrib.get('OBS_VALUE')  # Get \"OBS_VALUE\" attribute\n",
    "                if time_period and obs_value:\n",
    "                    columns.add(time_period)\n",
    "                    data.append((geo, time_period, obs_value))\n",
    "\n",
    "    # Create DataFrame with appropriate indices\n",
    "    df = pd.DataFrame(index=sorted(rows), columns=sorted(columns))\n",
    "\n",
    "    # Fill DataFrame with extracted values\n",
    "    for geo, time_period, obs_value in data:\n",
    "        df.at[geo, time_period] = obs_value\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for the XML data\n",
    "file_url = 'https://sebastien-hein.emi.u-bordeaux.fr/OI-sbzrthstrm/DATA/chomage.xml'\n",
    "\n",
    "# Load the data\n",
    "chomage = parse_xml(file_url)\n",
    "\n",
    "# Format the data\n",
    "chomage.columns = chomage.columns.map(lambda x: \\\n",
    "                                      pd.to_datetime(x, format='%Y-%m'))  # Convert columns to datetime\n",
    "chomage.index = chomage.index.map(lambda x: \\\n",
    "                code.loc[code.loc[:, 'CODE'] == x, 'Label - French'].iloc[0])  # Map index to labels\n",
    "chomage.drop('Zone euro - 20 pays (à partir de 2023)', inplace=True)  # Drop specific rows\n",
    "chomage.drop('Union européenne - 27 pays (à partir de 2020)', inplace=True)  # Drop specific rows\n",
    "chomage = chomage.apply(pd.to_numeric).T  # Convert data to numeric\n",
    "chomage.columns.name = 'Country'\n",
    "\n",
    "\n",
    "# Missing values\n",
    "missing_ranges = defaultdict(list) # Initialize dictionary to store missing date ranges\n",
    "missing_values = chomage.isna() # Identify missing values\n",
    "\n",
    "for country in chomage.columns: # Loop through each country (column) to find missing date ranges\n",
    "    country_missing = missing_values[country]\n",
    "    if country_missing.any():\n",
    "        # Find the ranges of missing dates\n",
    "        missing_dates = country_missing[country_missing].index\n",
    "        start_date = None\n",
    "        for date in missing_dates:\n",
    "            if start_date is None:\n",
    "                start_date = date\n",
    "            if date + pd.DateOffset(months=1) not in missing_dates:\n",
    "                end_date = date\n",
    "                missing_ranges[country].append((start_date.strftime('%Y-%m'), \n",
    "                                                end_date.strftime('%Y-%m')))\n",
    "                start_date = None\n",
    "\n",
    "# Fill missing values using interpolation as the most consistent method for long gaps\n",
    "chomage.interpolate(method='time', inplace=True, limit_direction='both')  # Interpolate linearly by date for smoother transitions\n",
    "\n",
    "# Optionally fill remaining missing values (if interpolation failed for some edge cases) with column mean\n",
    "chomage.fillna(chomage.mean(), inplace=True)\n",
    "\n",
    "ipch.isna().sum().sum() # Number of missing values (0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml_pib(file_url):\n",
    "    \"\"\"Parse XML file and extract data into a DataFrame.\"\"\"\n",
    "    \n",
    "    # Download the XML file content using requests\n",
    "    response = requests.get(file_url)\n",
    "    xml_content = response.text  # Get the content of the XML file as a string\n",
    "    \n",
    "    # Parse the XML content with ElementTree\n",
    "    root = ET.fromstring(xml_content)  # Parse the XML string directly\n",
    "    \n",
    "    # Initialize a list to store data\n",
    "    data = []\n",
    "\n",
    "    # Extract data\n",
    "    for record in root.findall('.//record'):\n",
    "        record_data = {}\n",
    "        for field in record.findall('field'):\n",
    "            name = field.attrib.get('name')\n",
    "            text = field.text\n",
    "            match name:\n",
    "                case \"Country or Area\": record_data['country'] = text\n",
    "                case \"Value\": \n",
    "                    try: record_data['value'] = float(text)\n",
    "                    except: record_data['value'] = None\n",
    "                case \"Year\": record_data['year'] = pd.to_datetime(str(text))\n",
    "        data.append(record_data)\n",
    "\n",
    "    # Create DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates(subset=['year', 'country'])\n",
    "\n",
    "    # Pivot the DataFrame to get the desired format\n",
    "    df = df.pivot(index='year', columns='country', values='value')\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for the XML data\n",
    "url = 'https://julie-sclaunich.emi.u-bordeaux.fr/DATA/API_NY.GDP.MKTP.CD_DS2_fr_xml_v2_38351.xml'\n",
    "\n",
    "# Load the data\n",
    "pib = parse_xml_pib(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reindex monthly\n",
    "monthly_dates = pd.date_range(start=pib.index.values[0], end=pib.index.values[-1], freq='MS')\n",
    "\n",
    "# Add columns for each month from 2013 to 2023\n",
    "pib = pib.reindex(index=pib.index.union(monthly_dates))\n",
    "pib.columns.name = 'Country'\n",
    "\n",
    "# Select only same dates and countries as dette_publique\n",
    "pib = pib.loc[dette_publique.index, dette_publique.columns.intersection(pib.columns)]\n",
    "\n",
    "def fill_val(x):\n",
    "    \"\"\"Fill missing values by resampling and interpolating.\"\"\"\n",
    "    return x.resample('MS').interpolate(method='quadratic')\n",
    "\n",
    "# Apply the fill_val function to each row\n",
    "pib = pib.apply(func=fill_val, axis=0)\n",
    "\n",
    "pib.isna().sum().sum() # Number of missing values (0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Currency</th>\n",
       "      <th>Bosnia and Herzegovina convertible mark</th>\n",
       "      <th>Bulgarian lev</th>\n",
       "      <th>Canadian dollar</th>\n",
       "      <th>Czech koruna</th>\n",
       "      <th>Danish krone</th>\n",
       "      <th>Hungarian forint</th>\n",
       "      <th>Icelandic króna</th>\n",
       "      <th>Japanese yen</th>\n",
       "      <th>North Macedonian denar</th>\n",
       "      <th>Norwegian krone</th>\n",
       "      <th>Polish zloty</th>\n",
       "      <th>Pound sterling</th>\n",
       "      <th>Romanian leu</th>\n",
       "      <th>Russian rouble</th>\n",
       "      <th>Serbian dinar</th>\n",
       "      <th>Swedish krona</th>\n",
       "      <th>Swiss franc</th>\n",
       "      <th>Turkish lira</th>\n",
       "      <th>US dollar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>1.95583</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>1.368400</td>\n",
       "      <td>25.980000</td>\n",
       "      <td>7.457900</td>\n",
       "      <td>296.870000</td>\n",
       "      <td>162.380000</td>\n",
       "      <td>129.660000</td>\n",
       "      <td>61.585000</td>\n",
       "      <td>7.806700</td>\n",
       "      <td>4.197500</td>\n",
       "      <td>0.849260</td>\n",
       "      <td>4.419000</td>\n",
       "      <td>42.337000</td>\n",
       "      <td>113.136900</td>\n",
       "      <td>8.651500</td>\n",
       "      <td>1.231100</td>\n",
       "      <td>2.533500</td>\n",
       "      <td>1.328100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-01</th>\n",
       "      <td>1.95583</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>1.384134</td>\n",
       "      <td>26.195389</td>\n",
       "      <td>7.457178</td>\n",
       "      <td>298.365029</td>\n",
       "      <td>161.761335</td>\n",
       "      <td>131.231491</td>\n",
       "      <td>61.590730</td>\n",
       "      <td>7.849537</td>\n",
       "      <td>4.197594</td>\n",
       "      <td>0.848684</td>\n",
       "      <td>4.422440</td>\n",
       "      <td>42.618453</td>\n",
       "      <td>113.521815</td>\n",
       "      <td>8.697162</td>\n",
       "      <td>1.236956</td>\n",
       "      <td>2.577914</td>\n",
       "      <td>1.339790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-01</th>\n",
       "      <td>1.95583</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>1.397158</td>\n",
       "      <td>26.376653</td>\n",
       "      <td>7.456598</td>\n",
       "      <td>299.637286</td>\n",
       "      <td>161.199346</td>\n",
       "      <td>132.544486</td>\n",
       "      <td>61.595525</td>\n",
       "      <td>7.888816</td>\n",
       "      <td>4.197485</td>\n",
       "      <td>0.847671</td>\n",
       "      <td>4.425333</td>\n",
       "      <td>42.944501</td>\n",
       "      <td>113.864562</td>\n",
       "      <td>8.737177</td>\n",
       "      <td>1.241087</td>\n",
       "      <td>2.615999</td>\n",
       "      <td>1.348489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-01</th>\n",
       "      <td>1.95583</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>1.410265</td>\n",
       "      <td>26.562635</td>\n",
       "      <td>7.456038</td>\n",
       "      <td>300.959399</td>\n",
       "      <td>160.573608</td>\n",
       "      <td>133.880340</td>\n",
       "      <td>61.600413</td>\n",
       "      <td>7.932953</td>\n",
       "      <td>4.197150</td>\n",
       "      <td>0.846004</td>\n",
       "      <td>4.428299</td>\n",
       "      <td>43.385011</td>\n",
       "      <td>114.238586</td>\n",
       "      <td>8.780119</td>\n",
       "      <td>1.244379</td>\n",
       "      <td>2.655914</td>\n",
       "      <td>1.356062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-01</th>\n",
       "      <td>1.95583</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>1.421635</td>\n",
       "      <td>26.727907</td>\n",
       "      <td>7.455577</td>\n",
       "      <td>302.152358</td>\n",
       "      <td>159.964517</td>\n",
       "      <td>135.055220</td>\n",
       "      <td>61.604722</td>\n",
       "      <td>7.976317</td>\n",
       "      <td>4.196610</td>\n",
       "      <td>0.843845</td>\n",
       "      <td>4.430932</td>\n",
       "      <td>43.890885</td>\n",
       "      <td>114.595096</td>\n",
       "      <td>8.820316</td>\n",
       "      <td>1.246282</td>\n",
       "      <td>2.692291</td>\n",
       "      <td>1.361329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Currency    Bosnia and Herzegovina convertible mark  Bulgarian lev  \\\n",
       "2013-01-01                                  1.95583         1.9558   \n",
       "2013-02-01                                  1.95583         1.9558   \n",
       "2013-03-01                                  1.95583         1.9558   \n",
       "2013-04-01                                  1.95583         1.9558   \n",
       "2013-05-01                                  1.95583         1.9558   \n",
       "\n",
       "Currency    Canadian dollar  Czech koruna  Danish krone  Hungarian forint  \\\n",
       "2013-01-01         1.368400     25.980000      7.457900        296.870000   \n",
       "2013-02-01         1.384134     26.195389      7.457178        298.365029   \n",
       "2013-03-01         1.397158     26.376653      7.456598        299.637286   \n",
       "2013-04-01         1.410265     26.562635      7.456038        300.959399   \n",
       "2013-05-01         1.421635     26.727907      7.455577        302.152358   \n",
       "\n",
       "Currency    Icelandic króna  Japanese yen  North Macedonian denar  \\\n",
       "2013-01-01       162.380000    129.660000               61.585000   \n",
       "2013-02-01       161.761335    131.231491               61.590730   \n",
       "2013-03-01       161.199346    132.544486               61.595525   \n",
       "2013-04-01       160.573608    133.880340               61.600413   \n",
       "2013-05-01       159.964517    135.055220               61.604722   \n",
       "\n",
       "Currency    Norwegian krone  Polish zloty  Pound sterling  Romanian leu  \\\n",
       "2013-01-01         7.806700      4.197500        0.849260      4.419000   \n",
       "2013-02-01         7.849537      4.197594        0.848684      4.422440   \n",
       "2013-03-01         7.888816      4.197485        0.847671      4.425333   \n",
       "2013-04-01         7.932953      4.197150        0.846004      4.428299   \n",
       "2013-05-01         7.976317      4.196610        0.843845      4.430932   \n",
       "\n",
       "Currency    Russian rouble  Serbian dinar  Swedish krona  Swiss franc  \\\n",
       "2013-01-01       42.337000     113.136900       8.651500     1.231100   \n",
       "2013-02-01       42.618453     113.521815       8.697162     1.236956   \n",
       "2013-03-01       42.944501     113.864562       8.737177     1.241087   \n",
       "2013-04-01       43.385011     114.238586       8.780119     1.244379   \n",
       "2013-05-01       43.890885     114.595096       8.820316     1.246282   \n",
       "\n",
       "Currency    Turkish lira  US dollar  \n",
       "2013-01-01      2.533500   1.328100  \n",
       "2013-02-01      2.577914   1.339790  \n",
       "2013-03-01      2.615999   1.348489  \n",
       "2013-04-01      2.655914   1.356062  \n",
       "2013-05-01      2.692291   1.361329  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL of the CSV file\n",
    "url_3 = 'https://julie-sclaunich.emi.u-bordeaux.fr/DATA/estat_tec00033_filtered_en.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "devise = pd.read_csv(url_3)\n",
    "\n",
    "# Remove unnecessary columns to clean up the data\n",
    "columns_to_delete = ['DATAFLOW', 'LAST UPDATE', 'freq', 'statinfo', 'unit', 'OBS_FLAG']\n",
    "devise.drop(columns=columns_to_delete, inplace=True)\n",
    "\n",
    "# Convert 'TIME_PERIOD' to datetime format and filter rows after 2012\n",
    "devise['TIME_PERIOD'] = pd.to_datetime(devise['TIME_PERIOD'], format='%Y', errors='coerce')  # Convert to datetime\n",
    "devise = devise[devise['TIME_PERIOD'] > '2012-12-31']  # Keep rows with dates after 2012\n",
    "devise['TIME_PERIOD'] = devise['TIME_PERIOD'].dt.strftime('%Y/%m')  # Format as YYYY/MM\n",
    "\n",
    "# Reshape the DataFrame to have 'TIME_PERIOD' as row index and 'currency' as columns\n",
    "devise = devise.pivot(index='TIME_PERIOD', columns='currency', values='OBS_VALUE')\n",
    "\n",
    "# Set the index to be a DatetimeIndex for resampling\n",
    "devise.index = pd.to_datetime(devise.index, format='%Y/%m', errors='coerce')\n",
    "devise.index.name = None\n",
    "devise.columns.name = 'Currency'\n",
    "\n",
    "# Define a function to fill missing values by resampling and interpolating\n",
    "def fill_val(x):\n",
    "    \"\"\"Fill missing values by resampling to monthly frequency and using quadratic interpolation.\"\"\"\n",
    "    return x.resample('MS').interpolate(method='quadratic')\n",
    "\n",
    "# Apply the interpolation function to fill missing values\n",
    "devise = fill_val(devise)\n",
    "print(devise.isna().sum().sum()) # Number of missing values (24 a voir pourquoi)\n",
    "# Display a sample of the corrected data\n",
    "devise.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matières premières\n",
    "## Or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Or</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-01</th>\n",
       "      <td>201.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-01</th>\n",
       "      <td>198.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-01</th>\n",
       "      <td>195.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-01</th>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-01</th>\n",
       "      <td>190.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Or\n",
       "2023-12-01  201.5\n",
       "2023-11-01  198.6\n",
       "2023-10-01  195.8\n",
       "2023-09-01  194.0\n",
       "2023-08-01  190.2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_or = \"https://bdm.insee.fr/series/sdmx/data/SERIES_BDM/010002100\" # URL of the serie\n",
    "\n",
    "response = requests.get(url_or) # Retrieve XML data\n",
    "response.raise_for_status() # Checks that the request is successful\n",
    "xml_content = response.content\n",
    "\n",
    "\n",
    "root = ET.fromstring(xml_content) # Parse XML content\n",
    "\n",
    "root = ET.fromstring(xml_content) # Load XML content\n",
    "\n",
    "\n",
    "data = [] # Initialize a list to store the data\n",
    "\n",
    "\n",
    "for series in root.findall(\".//{*}Series\"): # Browse each series\n",
    "\n",
    "    for obs in series.findall(\".//{*}Obs\"): # Browse the observation in  each series\n",
    "\n",
    "        # Extract relevant \n",
    "        time_period = obs.attrib.get(\"TIME_PERIOD\")\n",
    "        obs_value = obs.attrib.get(\"OBS_VALUE\")\n",
    "        # Add the data at the list\n",
    "        data.append({\"TIME_PERIOD\": time_period, \"OBS_VALUE\": obs_value})\n",
    "\n",
    "\n",
    "df_or = pd.DataFrame(data) # Create a DataFrame from the extracted data\n",
    "\n",
    "\n",
    "# Convert columns to appropriate types\n",
    "df_or[\"TIME_PERIOD\"] = pd.to_datetime(df_or[\"TIME_PERIOD\"], format=\"%Y-%m\")\n",
    "df_or[\"OBS_VALUE\"] = pd.to_numeric(df_or[\"OBS_VALUE\"])\n",
    "\n",
    "\n",
    "\n",
    "# Convert TIME_PERIOD to datetime for easier filtering\n",
    "df_or['TIME_PERIOD'] = pd.to_datetime(df_or['TIME_PERIOD'], format='%Y-%m')\n",
    "\n",
    "# Filter years between 2013 and 2023\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2023-12-31'\n",
    "df_or = df_or[(df_or['TIME_PERIOD'] >= start_date) & (df_or['TIME_PERIOD'] <= end_date)]\n",
    "\n",
    "df_or.set_index('TIME_PERIOD', inplace=True) #indexes the years\n",
    "df_or.index.name = None\n",
    "df_or.columns.name = None\n",
    "df_or.rename(columns = {'OBS_VALUE': 'Or'}, inplace = True)\n",
    "print(df_or.isna().sum().sum()) # Number of missing values (0)\n",
    "# show the 5 first rows\n",
    "df_or.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pétrole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Material</th>\n",
       "      <th>Petrol</th>\n",
       "      <th>Or</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-01</th>\n",
       "      <td>118.1</td>\n",
       "      <td>201.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-01</th>\n",
       "      <td>127.3</td>\n",
       "      <td>198.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-01</th>\n",
       "      <td>142.3</td>\n",
       "      <td>195.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-01</th>\n",
       "      <td>145.2</td>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-01</th>\n",
       "      <td>130.9</td>\n",
       "      <td>190.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Material    Petrol     Or\n",
       "2023-12-01   118.1  201.5\n",
       "2023-11-01   127.3  198.6\n",
       "2023-10-01   142.3  195.8\n",
       "2023-09-01   145.2  194.0\n",
       "2023-08-01   130.9  190.2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_petrol = \"https://bdm.insee.fr/series/sdmx/data/SERIES_BDM/010002091\" # URL of the serie\n",
    "\n",
    "response = requests.get(url_petrol) # Retrieve XML data\n",
    "response.raise_for_status()   # Checks that the request is successful\n",
    "xml_content = response.content\n",
    "\n",
    "\n",
    "root = ET.fromstring(xml_content) # Parse XML content\n",
    "\n",
    "root = ET.fromstring(xml_content) # Load XML content\n",
    "\n",
    "# Initialiser une liste pour stocker les données\n",
    "data = []\n",
    "\n",
    "\n",
    "for series in root.findall(\".//{*}Series\"): # Browse each series\n",
    "   \n",
    "    for obs in series.findall(\".//{*}Obs\"):  # Browse the observation in  each series\n",
    "\n",
    "       # Extract relevant attributes\n",
    "        time_period = obs.attrib.get(\"TIME_PERIOD\")\n",
    "        obs_value = obs.attrib.get(\"OBS_VALUE\")\n",
    "         # Add the data at the list\n",
    "        data.append({\"TIME_PERIOD\": time_period, \"OBS_VALUE\": obs_value})\n",
    "\n",
    "\n",
    "df_petrol = pd.DataFrame(data)  # Create a DataFrame from the extracted data\n",
    "\n",
    "# Convert columns to appropriate types\n",
    "df_petrol[\"TIME_PERIOD\"] = pd.to_datetime(df_petrol[\"TIME_PERIOD\"], format=\"%Y-%m\")\n",
    "df_petrol[\"OBS_VALUE\"] = pd.to_numeric(df_petrol[\"OBS_VALUE\"])\n",
    "\n",
    "\n",
    "# Convert TIME_PERIOD to datetime for easier filtering\n",
    "df_petrol['TIME_PERIOD'] = pd.to_datetime(df_petrol['TIME_PERIOD'], format='%Y-%m')\n",
    "\n",
    "# Filter years between 2013 and 2023\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2023-12-31'\n",
    "df_petrol = df_petrol[(df_petrol['TIME_PERIOD'] >= start_date) & (df_petrol['TIME_PERIOD'] <= end_date)]\n",
    "\n",
    "df_petrol.set_index('TIME_PERIOD', inplace=True) #indexes the years\n",
    "df_petrol.index.name = None\n",
    "df_petrol.columns.name = None\n",
    "df_petrol.rename(columns = {'OBS_VALUE': 'Petrol'}, inplace = True)\n",
    "\n",
    "print(ipch.isna().sum().sum()) # Number of missing values (0)\n",
    "# show the 5 first rows\n",
    "df_petrol.head()\n",
    "material = pd.concat((df_petrol, df_or), axis = 1, join = 'inner')\n",
    "material.columns.name = 'Material'\n",
    "material.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action\n",
    "\n",
    "## Présentation de la classe ```Indice``` pour importer les données d'un actif et calculer les indices\n",
    "\n",
    "La classe importe les données depuis YahooFinance.\n",
    "\n",
    "L'argument `ticker_symbol` suffit lors de l'instanciation. Il s'agit du symbole boursier de l'action dont les données seront téléchargées.\\\n",
    "L'argument `data` lors de l'instanciation permet de donner directement les données si elles sont téléchargées.\n",
    "\n",
    "La méthode `update` calcule les différents indices qui seront affichés par la méthode `affichage`. Elle est exécutée lors de l'instanciation.\n",
    "\n",
    "L'historique du prix de l'action est accessible via la méthode `get_data`.\\\n",
    "Les calculs des indices OBV, ADLine, ADX et Aroon sont implémentés et accessibles via la méthode `get_index`.\\\n",
    "\n",
    "## Présentation des indices\n",
    "\n",
    "### OBV\n",
    "Il s'agit d'un indicateur de momentum qui mesure les flux de volume positifs et négatifs. \n",
    "\n",
    "Si la courbe de l'OBV augmente (ou diminue) de façon prononcée, sans changement significatif du prix de l'actif, cela indique qu'à un moment, le prix devrait sauter vers le haut (ou vers le bas).\n",
    "\n",
    "Lorsque les institutions commencent à acheter un actif que les particuliers continuent de vendre, le prix est encore légèrement en baisse ou se stabilise, alors que le volume augmente. Le phénomène inverse se produit également. \n",
    "\n",
    "### ADLine\n",
    "L'ADLine (*Accumulative Distribution Line*) est un indicateur qui mesure le flux d'argent pour un actif en prenant en compte à la fois les variations de prix et les volumes.\n",
    "\n",
    "Une ADLine en hausse indique une pression d'achat accrue, souvent interprétée comme une accumulation de la part des investisseurs, tandis qu'une ADLine en baisse révèle une pression de vente ou une distribution.\n",
    "\n",
    "Une divergence entre l'ADLine et le prix de l'actif peut être utilisée pour anticiper un retournement potentiel de tendance. Par exemple, si le prix monte mais que l'ADLine chute, cela pourrait signaler un affaiblissement de la tendance haussière.\n",
    "\n",
    "### ADX\n",
    "L'ADX identifie une tendance forte lorsqu'il est au-dessus de 25 et une tendance faible lorsqu'il est en-dessous de 20. \\\n",
    "On peut également utiliser le franchissement des lignes $-DI$ et $+DI$ pour générer des signaux de trade: \n",
    "- Lorsque $+DI$ passe au-dessus de $-DI$ et que l'ADX est supérieur à 20 (idéalement à 25), alors il s'agit d'un potentiel signal pour acheter.\n",
    "- Inversement, lorsque $-DI$ passe au-dessus de $+DI$ et que l'ADX est supérieur à 20 (ou 25), il s'agit d'un potentiel signal pour vendre.\n",
    "\n",
    "### Aroon\n",
    "Indique si le prix maximal ou minimal a été atteint depuis longtemps ou non sur les dernières périodes (25 par défaut). Il peut s'agir du prix d'ouverture, de clôture, le prix maximal ou minimal sur la période. S'il est à 100, c'est que le prix maximal a été atteint la veille et que le prix minimal a été atteint avant toutes les périodes étudiées. S'il est à -100 dans le cas contraire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index():\n",
    "    \"\"\"\n",
    "    A class to represent and calculate various financial indices for a given stock.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data(ticker_symbol='AAPL', period='max'):\n",
    "        \"\"\"\n",
    "        Import stock price history.\n",
    "        \n",
    "        IN: ticker_symbol: <str> Stock identifier\n",
    "            period: <str> Period over which data is downloaded\n",
    "                    arg: '1d', '5d', '1mo', '3mo', '6mo', '1y', '2y', '5y', '10y', 'ytd', 'max'\n",
    "        OUT: <pd.Series>: Stock history\n",
    "        \"\"\"\n",
    "        ticker = yf.Ticker(ticker_symbol)  # Create a Ticker object\n",
    "        data_hist = ticker.history(period=period)  # Download historical data\n",
    "        dates = pd.Series(data_hist['Open'].index).apply(lambda x: pd.to_datetime(x.strftime('%Y-%m-%d')))  # Convert dates\n",
    "        data = data_hist.reset_index(drop=True).set_index(dates).drop(['Dividends', 'Stock Splits'], axis=1)  # Prepare data\n",
    "        data.index.name = None\n",
    "        return data\n",
    "\n",
    "    @classmethod\n",
    "    def smooth(cls, series, period=14, method='simple', fill='NoFill', alpha=1/14):\n",
    "        \"\"\"\n",
    "        Calculate moving average.\n",
    "        \n",
    "        IN: series: <Pandas Series> Series for which the moving average is calculated\n",
    "            period: <int> Number of periods used for the moving average\n",
    "            method: <str> Method used for calculating the average\n",
    "                    'simple': Calculate arithmetic average\n",
    "                    'exp': Calculate exponential average\n",
    "                    'weight': Assign increasing weights to series and calculate arithmetic average\n",
    "            fill: <str> If the first period values are initialized or left empty (fill='NoFill')\n",
    "                  'constant': Fill with the first non-null value (at position period)\n",
    "                  'data': Fill with the first period values of the series\n",
    "                  'smooth': Fill value i with the moving average of the first i+1 values of the series over a period of i+1, \n",
    "                            for i from 1 to period - 1. The same method of calculating the average is used.\n",
    "            alpha: <int> Argument for calculating the average by the 'exp' method. Must be between 0 and 1.\n",
    "        OUT: <Pandas Series>: Smoothed series\n",
    "        \"\"\"\n",
    "        match method:\n",
    "            case 'simple':\n",
    "                smoothed = series.rolling(period).sum().copy() / period  # Simple moving average\n",
    "            case 'exp':\n",
    "                smoothed = [series.iloc[0]]  # Initialize with the first value\n",
    "                for i in range(1, len(series)):\n",
    "                    smoothed += [alpha * series.iloc[i] + (1 - alpha) * smoothed[i - 1]]  # Exponential moving average\n",
    "                smoothed = pd.Series(smoothed)\n",
    "            case 'weight':\n",
    "                weight = np.array([k for k in range(1, period + 1)])  # Weights for weighted moving average\n",
    "                smoothed = series.rolling(period).apply(lambda x: np.dot(x, weight).sum() / weight.sum())  # Weighted moving average\n",
    "        \n",
    "        match fill:\n",
    "            case 'constant':\n",
    "                smoothed.iloc[:period] = smoothed.iloc[period - 1]  # Fill with constant value\n",
    "            case 'data':\n",
    "                smoothed.iloc[:period - 1] = series.iloc[:period - 1]  # Fill with initial data values\n",
    "            case 'smooth':\n",
    "                smoothed.iloc[0] = series.iloc[0]  # Initialize with the first value\n",
    "                for i in range(1, period):\n",
    "                    smoothed.iloc[i] = cls.smooth(series=series.iloc[:i + 1], period=i + 1, method=method, fill='NoFill').iloc[i]  # Smooth fill\n",
    "            case 'NoFill':\n",
    "                pass  # No fill\n",
    "\n",
    "        return smoothed\n",
    "\n",
    "    def __init__(self, name = 'Apple', ticker_symbol='AAPL', data=False) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Index class.\n",
    "        \n",
    "        IN: ticker_symbol: <str> Stock ticker symbol to study\n",
    "            data: <bool or Pandas DataFrame> If data is False, the history will be downloaded from Yahoo Finance. \n",
    "                                             Otherwise, data must contain the data downloaded from Yahoo Finance and \n",
    "                                             transformed as in the static method load_data.\n",
    "        \"\"\"\n",
    "        self.__name = name\n",
    "        self.__TICKER_SYMBOL = ticker_symbol  # Stock ticker symbol\n",
    "        self.__data = None  # Data placeholder\n",
    "        self.__index = None  # Index placeholder\n",
    "        self.__date_limits = {'Start': None, 'End': None}  # Date limits\n",
    "\n",
    "        self.__start(data)  # Initialize data\n",
    "\n",
    "    def __start(self, data):\n",
    "        \"\"\"Start the data initialization.\"\"\"\n",
    "        if data is not False:\n",
    "            self.__data = data  # Use provided data\n",
    "        else:\n",
    "            self.__data = self.load_data(ticker_symbol = self.__TICKER_SYMBOL)  # Load data from Yahoo Finance\n",
    "\n",
    "        self.__index = pd.DataFrame(index = self.__data.index)  # Initialize index DataFrame\n",
    "        self.__date_limits['Start'] = self.__data.index[0]  # Set start date\n",
    "        self.__date_limits['End'] = self.__data.index[-1]  # Set end date\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.__name\n",
    " \n",
    "    def get_data(self):\n",
    "        \"\"\"Return the data.\"\"\"\n",
    "        monthly_dates = pd.date_range(start=self.__data.index.values[0], end=self.__data.index.values[-1], freq='MS')\n",
    "\n",
    "        # Add columns for each month from 2013 to 2023\n",
    "        self.__data = self.__data.reindex(index=self.__data.index.union(monthly_dates))\n",
    "\n",
    "        def fill_val(x):\n",
    "            \"\"\"Fill missing values by resampling and interpolating.\"\"\"\n",
    "            return x.resample('MS').interpolate(method='quadratic')\n",
    "\n",
    "        # Apply the fill_val function to each row\n",
    "        self.__data = self.__data.apply(func=fill_val, axis=0)\n",
    "\n",
    "        return self.__data[self.__data.index.is_month_start]\n",
    "    \n",
    "    def get_index(self):\n",
    "        \"\"\"Return the index.\"\"\"\n",
    "        monthly_dates = pd.date_range(start=self.__index.index.values[0], end=self.__index.index.values[-1], freq='MS')\n",
    "\n",
    "        # Add columns for each month from 2013 to 2023\n",
    "        self.__index = self.__index.reindex(index=self.__index.index.union(monthly_dates))\n",
    "\n",
    "        def fill_val(x):\n",
    "            \"\"\"Fill missing values by resampling and interpolating.\"\"\"\n",
    "            return x.resample('MS').interpolate(method='quadratic')\n",
    "\n",
    "        # Apply the fill_val function to each row\n",
    "        self.__index = self.__index.apply(func=fill_val, axis=0)\n",
    "\n",
    "        return self.__index[self.__index.index.is_month_start]\n",
    "   \n",
    "    def update(self, **kwargs):\n",
    "        \"\"\"Update the index with calculated indicators.\"\"\"\n",
    "        self.__index = pd.DataFrame(index=self.__data.index)  # Reset index DataFrame\n",
    "        self.OBV(**kwargs)\n",
    "        self.ADLine(**kwargs)\n",
    "        self.ADX(**kwargs)\n",
    "        self.Aroon(**kwargs)\n",
    "    \n",
    "    def add(self, indicator, dates, **kwargs):\n",
    "        \"\"\"Add a specific indicator to the index.\"\"\"\n",
    "        match indicator:\n",
    "            case 'C': return self.C(dates=dates, **kwargs)  # Add indicator 'C'\n",
    "\n",
    "    def save(self, name):\n",
    "        \"\"\"\n",
    "        Save the data and index DataFrames to CSV files.\n",
    "\n",
    "        Parameters:\n",
    "        name (str): The base name for the CSV files. The function will create two files:\n",
    "                    one for the data DataFrame and one for the index DataFrame.\n",
    "                    The files will be named '{name}_data.csv' and '{name}_index.csv' respectively.\n",
    "        \"\"\"\n",
    "        self.__data.to_csv(f'{name}_data.csv', index=True)\n",
    "        self.__index.to_csv(f'{name}_index.csv', index=True)\n",
    "\n",
    "    def display(self, start_date, end_date, list_indices, **kwargs):\n",
    "        \"\"\"\n",
    "        Display the graphs of the specified indices in list_indices.\n",
    "        \n",
    "        IN: start_date, end_date: <str> Start and end dates for displaying the indices\n",
    "            list_indices: <list of str> List of indices in string format to display. \n",
    "                         Multiple lists will give multiple graphs. \n",
    "                         Different indices within a single list will be displayed on the same graph.\n",
    "                         Index names: 'OBV', 'ADLine', 'ADX'\n",
    "            **kwargs: Arguments to pass to the index calculation methods\n",
    "        \"\"\"\n",
    "        self.update(**kwargs)  # Update indices\n",
    "\n",
    "        # Convert start and end date\n",
    "        start = pd.to_datetime(start_date)\n",
    "        end = pd.to_datetime(end_date)\n",
    "\n",
    "        if start < self.__date_limits['Start']: start = self.__date_limits['Start']  # Adjust start date\n",
    "        else: pass\n",
    "        if end > self.__date_limits['End']: end = self.__date_limits['End']  # Adjust end date\n",
    "        else: pass\n",
    "\n",
    "        data = self.__index.loc[start:end]  # Filter data by date range\n",
    "        x = pd.Series(data.index).apply(lambda x: x.strftime('%d/%m/%y'))  # Format dates for x-axis\n",
    "\n",
    "        fig, ax = plt.subplots(len(list_indices), 1, figsize=(20, 3 * len(list_indices)))  # Create subplots\n",
    "        fig.suptitle(f'Period {start.strftime(\"%d/%m/%y\")} - {end.strftime(\"%d/%m/%y\")}')  # Set title\n",
    "        for i, indices in enumerate(list_indices):\n",
    "            for indicator in indices:\n",
    "                if indicator in data:\n",
    "                    y = data[indicator]  # Get data for the index if present\n",
    "                else:\n",
    "                    y = self.__add(indicator, dates=data.index, **kwargs)  # Add index data if not present\n",
    "                style = kwargs.get(f'{indicator}_style', '')  # Get style for the index\n",
    "                ax[i].plot(x, y, style, label=indicator)  # Plot the index\n",
    "            ax[i].legend()\n",
    "            ax[i].set_xticks(ticks=x.iloc[np.linspace(0, 1, 20) * (len(x) - 1)])\n",
    "    \n",
    "\n",
    "    # Index calculation methods\n",
    "    def OBV(self, start='Close', end=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Calculate the On-Balance Volume (OBV) indicator.\n",
    "        \n",
    "        IN: start, end: <str> Among 'Open' and 'Close', the directions for adding volumes will be calculated according \n",
    "                        to the opening price ('Open') or closing price ('Close') of day n for the start and day n+1 for the end.\n",
    "            **kwargs: Exists for compatibility\n",
    "        OUT: <Pandas Series>: OBV indicator\n",
    "        \"\"\"\n",
    "        if start == 'Open' and end == 'Close':\n",
    "            direction = self.__data[end] - self.__data[start]  # Calculate direction based on Open and Close\n",
    "        elif end:\n",
    "            direction = self.__data[end] - self.__data[start].shift(1)  # Calculate direction based on shifted Close\n",
    "        else: direction = self.__data[start] - self.__data[start].shift(1)  # Calculate direction based on shifted Open\n",
    "\n",
    "        direction.iloc[0] = 1  # Set initial direction\n",
    "        OBV = (self.__data['Volume'] * direction / abs(direction)).cumsum()  # Calculate OBV\n",
    "        self.__index['OBV'] = np.where(np.isnan(OBV), OBV.shift(1), OBV)  # Handle NaN values and store OBV\n",
    "\n",
    "        return self.__index['OBV']\n",
    "        \n",
    "    def ADLine(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Calculate the Accumulation/Distribution Line (ADLine) indicator.\n",
    "        \n",
    "        IN: **kwargs: Exists for compatibility\n",
    "        OUT: <Pandas Series>: ADLine indicator\n",
    "        \"\"\"\n",
    "        MFM = ((self.__data['High'] - 2 * self.__data['Close'] + self.__data['Low'])) / \\\n",
    "              (self.__data['Low'] - self.__data['High'])  # Money Flow Multiplier\n",
    "        MFV = MFM * self.__data['Volume']  # Money Flow Volume\n",
    "\n",
    "        self.__index['ADLine'] = MFV.cumsum()  # Calculate and store ADLine\n",
    "\n",
    "        return self.__index['ADLine']\n",
    "    \n",
    "    def ADX(self, period_ADX=14, method='exp', fill='NoFill', alpha=1/14, **kwargs):\n",
    "        \"\"\"\n",
    "        Calculate the Average Directional Index (ADX) indicator.\n",
    "        \n",
    "        IN: period_ADX: <int> Number of periods used for the ADX calculation\n",
    "            method: <str> Method used for calculating the average\n",
    "                    'simple': Calculate arithmetic average\n",
    "                    'exp': Calculate exponential average\n",
    "                    'weight': Assign increasing weights to series and calculate arithmetic average\n",
    "            fill: <str> If the first period values are initialized or left empty (fill='NoFill')\n",
    "                  'constant': Fill with the first non-null value (at position period)\n",
    "                  'data': Fill with the first period values of the series\n",
    "                  'smooth': Fill value i with the moving average of the first i+1 values of the series over a period of i+1, \n",
    "                            for i from 1 to period - 1. The same method of calculating the average is used.\n",
    "            alpha: <int> Argument for calculating the average by the 'exp' method. Must be between 0 and 1.\n",
    "            **kwargs: Exists for compatibility\n",
    "        OUT: <Pandas Series>: ADX indicator\n",
    "        \"\"\"\n",
    "        DMp = np.where(self.__data['High'] - self.__data['High'].shift(1) > self.__data['Low'].shift(1) - self.__data['Low'],\n",
    "                       self.__data['High'] - self.__data['High'].shift(1), 0)  # Positive Directional Movement\n",
    "        DMm = np.where(self.__data['High'] - self.__data['High'].shift(1) <= self.__data['Low'].shift(1) - self.__data['Low'],\n",
    "                       self.__data['Low'].shift(1) - self.__data['Low'], 0)  # Negative Directional Movement\n",
    "\n",
    "        TR = pd.DataFrame([self.__data['High'] - self.__data['Low'], \n",
    "                           self.__data['High'] - self.__data['Close'].shift(1), \n",
    "                           self.__data['Close'].shift(1) - self.__data['Low']]).apply(max)  # Set True Range\n",
    "\n",
    "        # Smoothed Positive DM, Negative DM and True Range\n",
    "        DMpsmooth = self.smooth(series=pd.Series(DMp), period=period_ADX, method=method, fill=fill, alpha=alpha)\n",
    "        DMmsmooth = self.smooth(series=pd.Series(DMm), period=period_ADX, method=method, fill=fill, alpha=alpha)\n",
    "        TRsmooth = self.smooth(series=pd.Series(TR), period=period_ADX, method=method, fill=fill, alpha=alpha)\n",
    "\n",
    "        DIp = 100 * DMpsmooth / TRsmooth  # Positive Directional Indicator\n",
    "        DIm = 100 * DMmsmooth / TRsmooth  # Negative Directional Indicator\n",
    "\n",
    "        DX = 100 * (DIp - DIm) / (DIp + DIm)  # Directional Movement Index\n",
    "        DX.iloc[0] = 0  # Set initial DX\n",
    "        ADX = self.smooth(DX, period=period_ADX, method=method, fill=fill, alpha=alpha)  # Average Directional Index\n",
    "\n",
    "        # Set index for ADX, DIp, DIm\n",
    "        ADX.index = self.__index.index\n",
    "        DIp.index = self.__index.index\n",
    "        DIm.index = self.__index.index\n",
    "        \n",
    "        # Store ADX, Positive DI and Negative DI in index\n",
    "        self.__index['ADX'] = ADX\n",
    "        self.__index['pDI'] = DIp\n",
    "        self.__index['mDI'] = DIm\n",
    "        \n",
    "        return self.__index['ADX']\n",
    "    \n",
    "    def Aroon(self, period_Aroon=25, event='Close', **kwargs):\n",
    "        \"\"\"\n",
    "        Calculate the Aroon indicator.\n",
    "        \n",
    "        IN: period_Aroon: <int> Number of periods over which the index is calculated\n",
    "            event: <str> Among 'Open', 'Close', 'Low', and 'High', the Aroon index will be calculated based on the variations of these events.\n",
    "            **kwargs: Exists for compatibility\n",
    "        OUT: <Pandas Series>: Aroon indicator\n",
    "        \"\"\"\n",
    "        def indMin(i):\n",
    "            if i < period_Aroon: start, end = 0, max(i, 1)\n",
    "            else: start, end = i - period_Aroon, i\n",
    "            return np.argmin(self.__data[event].iloc[start: end])  # Index of minimum value\n",
    "        def indMax(i):\n",
    "            if i < period_Aroon: start, end = 0, max(i, 1)\n",
    "            else: start, end = i - period_Aroon, i\n",
    "            return np.argmax(self.__data[event].iloc[start: end])  # Index of maximum value\n",
    "\n",
    "        # Calculate argMin, argMax and Aroon indicator for each period\n",
    "        argMin = pd.Series(range(len(self.__data))).apply(indMin)\n",
    "        argMax = pd.Series(range(len(self.__data))).apply(indMax)\n",
    "        Aroon = 100 * (argMin - argMax) / period_Aroon\n",
    "        \n",
    "        # Store Aroon in index\n",
    "        Aroon.index = self.__index.index\n",
    "        self.__index['Aroon'] = Aroon\n",
    "\n",
    "        return self.__index['Aroon']\n",
    "    \n",
    "    # Other lines\n",
    "    def C(self, dates, c=25):\n",
    "        \"\"\"\n",
    "        Generate a horizontal curve.\n",
    "        \n",
    "        IN: dates: <Pandas Index> Dates for the curve\n",
    "            c: <int> Height of the curve\n",
    "        OUT: <Pandas Series>: Horizontal curve\n",
    "        \"\"\"\n",
    "        return pd.Series(range(len(self.__index.index)), index=dates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection d'action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_entreprise = {\n",
    "    # CAC 40 companies\n",
    "    \"Accor\": \"AC.PA\",\n",
    "    \"Air Liquide\": \"AI.PA\",\n",
    "    \"ArcelorMittal\": \"MT.AS\",\n",
    "    \"BNP Paribas\": \"BNP.PA\",\n",
    "    \n",
    "    # Defensive companies\n",
    "    \"Nestlé\": \"NESN.SW\",\n",
    "    \"Sanofi\": \"SAN.PA\",\n",
    "    \"Novo Nordisk\": \"NOVO-B.CO\",\n",
    "    \"GlaxoSmithKline\": \"GSK.L\",\n",
    "    \n",
    "    # Cyclical companies\n",
    "    \"Volkswagen\": \"VOW3.DE\",\n",
    "    \"BMW\": \"BMW.DE\",\n",
    "    \"LVMH\": \"MC.PA\",\n",
    "    \"Hermès\": \"RMS.PA\",\n",
    "    \n",
    "    # Value companies\n",
    "    \"TotalEnergies\": \"TTE.PA\",\n",
    "    \"Schneider Electric\": \"SU.PA\",\n",
    "    \"Airbus\": \"AIR.PA\",\n",
    "    \"L'Oréal\": \"OR.PA\",\n",
    "    \n",
    "    # Companies with probable correlations with macroeconomic or technical indices\n",
    "    \"SAP\": \"SAP.DE\",\n",
    "    \"ASML\": \"ASML.AS\",\n",
    "    \"Siemens\": \"SIE.DE\",\n",
    "    \"Danone\": \"BN.PA\",\n",
    "    \"Kering\": \"KER.PA\",\n",
    "    \"Orange\": \"ORA.PA\",\n",
    "    \"Publicis\": \"PUB.PA\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction des dataframes pour les actions et les indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames for historical actions and indices\n",
    "hist_action = pd.DataFrame()\n",
    "hist_index = pd.DataFrame()\n",
    "\n",
    "# Iterate over each company in the dictionary\n",
    "for company, ticker in list_entreprise.items():\n",
    "    # Instantiate an Index object for the company\n",
    "    index_obj = Index(name=company, ticker_symbol=ticker)\n",
    "    \n",
    "    # Update the index to calculate technical indices\n",
    "    index_obj.update()\n",
    "    \n",
    "    # Get the historical action data\n",
    "    action_data = index_obj.get_data()\n",
    "    \n",
    "    # Create a MultiIndex for the columns in the format ('Company', 'Price')\n",
    "    action_data.columns = pd.MultiIndex.from_product([[company], action_data.columns], names=['Company', 'Price'])\n",
    "    action_data.set_index(action_data.index, inplace=True)  # Ensure the index remains the DateTimeIndex\n",
    "    \n",
    "    # Concatenate the historical action data into hist_action\n",
    "    hist_action = pd.concat([hist_action, action_data], axis=1)\n",
    "    \n",
    "    # Get the historical index data\n",
    "    index_data = index_obj.get_index()\n",
    "    \n",
    "    # Create a MultiIndex for the columns in the format ('Company', 'Index')\n",
    "    index_data.columns = pd.MultiIndex.from_product([[company], index_data.columns], names=['Company', 'Index'])\n",
    "    index_data.set_index(index_data.index, inplace=True)  # Ensure the index remains the DateTimeIndex\n",
    "    \n",
    "    # Concatenate the index data into hist_index\n",
    "    hist_index = pd.concat([hist_index, index_data], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion des dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dates from 01/01/2013 to 01/01/2023 are present\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/8spvhwwd3gjgp2cl9zvhb_4m0000gn/T/ipykernel_62046/1567578562.py:64: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n",
      "  data = data.groupby('Type', axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Find common countries across all DataFrames\n",
    "common_countries = ipch.columns.intersection(\n",
    "                   dette_publique.columns.intersection(\n",
    "                   chomage.columns.intersection(\n",
    "                    pib.columns)))\n",
    "\n",
    "# Find common dates across all DataFrames\n",
    "common_dates = ipch.index.intersection(\n",
    "                   dette_publique.index.intersection(\n",
    "                   chomage.index.intersection(\n",
    "                    pib.index)))\n",
    "\n",
    "# Generate a date range from the first to the last common date, with a frequency of the first day of each month\n",
    "expected_dates = pd.date_range(start=common_dates[0], end=common_dates[-1], freq='MS')\n",
    "\n",
    "# Check if all expected dates are present in the common dates\n",
    "all_dates_present = expected_dates.isin(common_dates).all()\n",
    "\n",
    "# Print whether all dates are present or if some are missing\n",
    "print(f'All dates from {common_dates[0].strftime(\"%d/%m/%Y\")} to {common_dates[-1].strftime(\"%d/%m/%Y\")} are present' \\\n",
    "        if all_dates_present else 'Some dates are missing')\n",
    "\n",
    "def set_index(df, index_name):\n",
    "    \"\"\"\n",
    "    Set a new index for the DataFrame and update its columns to include the new index.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame to update.\n",
    "    index_name (str): The name of the new index to add.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The updated DataFrame with the new index.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to include only common dates and common countries\n",
    "    df = df.loc[common_dates, common_countries]\n",
    "    \n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # Add the new index to the top level of the existing MultiIndex\n",
    "        new_index = pd.MultiIndex.from_tuples([(index_name, *idx) \\\n",
    "                        for idx in df.columns], names=['Type'] + df.columns.names)\n",
    "    else:\n",
    "        # Create a MultiIndex by combining the new index and the existing index\n",
    "        new_index = pd.MultiIndex.from_tuples([(index_name, idx) \n",
    "                        for idx in df.columns], names=['Type', df.columns.names[0]])\n",
    "    \n",
    "    # Update the columns of the DataFrame with the new MultiIndex\n",
    "    df.columns = new_index\n",
    "    return df\n",
    "\n",
    "# Set the index for each DataFrame\n",
    "dette_publique = set_index(dette_publique, 'Dette publique')\n",
    "chomage = set_index(chomage, 'Chomage')\n",
    "ipch = set_index(ipch, 'IPCH')\n",
    "pib = set_index(pib, 'PIB')\n",
    "\n",
    "# Filter hist_action and hist_index to include only common dates\n",
    "hist_action = hist_action[hist_action.index.isin(common_dates)]\n",
    "hist_index = hist_index[hist_index.index.isin(common_dates)]\n",
    "\n",
    "# Concatenate all DataFrames along the columns, keeping only the common dates\n",
    "data = pd.concat((dette_publique, chomage, ipch, pib), axis=1, join='inner')\n",
    "\n",
    "# Group the data by the 'Type' level of the columns\n",
    "data = data.groupby('Type', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichage\n",
    "\n",
    "L'affichage des graphiques se fait trois fois, je ne sais pas pourquoi.\\\n",
    "Pour l'affichage de la carte, j'ai commencé à coder une fonction tout en bas. Il manque la traduction des noms des pays. Je ne sais pas si ça va fonctionner après ça, à voir.\n",
    "\n",
    "## Fonction d'affichage de graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(countries, start, end, data, data_type):\n",
    "    \"\"\"\n",
    "    Plot the graph for the specified countries, date range, and data type.\n",
    "\n",
    "    Parameters:\n",
    "    countries (str): Comma-separated list of countries to plot.\n",
    "    start (datetime): The start date for the data to plot.\n",
    "    end (datetime): The end date for the data to plot.\n",
    "    data (DataFrame): The DataFrame containing the data to plot.\n",
    "    data_type (str): The type of data to plot (e.g., 'IPCH', 'Dette publique').\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Create a new figure with specified size\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    \n",
    "    # Plot data for each country\n",
    "    for country in countries.split(', '):\n",
    "        data.loc[start:end, (data_type, country)].plot(label=f'{country}')\n",
    "\n",
    "    # Set the title of the plot\n",
    "    plt.title(f'{data_type} ({start.strftime(\"%B %Y\")} - {end.strftime(\"%B %Y\")})')\n",
    "    \n",
    "    # Add a legend to the plot\n",
    "    plt.legend()\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction d'affichage de carte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GeoJSON file into a GeoDataFrame\n",
    "url = \"https://raw.githubusercontent.com/leakyMirror/map-of-europe/master/GeoJSON/europe.geojson\"\n",
    "europe = gpd.read_file(url)\n",
    "\n",
    "def plot_map(date: str, data: pd.core.groupby.DataFrameGroupBy, data_type: str):\n",
    "    \"\"\"\n",
    "    Plot a map of Europe for the specified date and data type.\n",
    "    \n",
    "    Args:\n",
    "        date (str): Date in 'YYYY-MM' format for which to filter the data.\n",
    "        data (pd.core.groupby.DataFrameGroupBy): A grouped DataFrame with MultiIndex columns\n",
    "            ('Type', 'Country') and a DateTimeIndex.\n",
    "        data_type (str): The type of data to display (e.g., 'IPCH').\n",
    "    \"\"\"\n",
    "    # Prepare the data\n",
    "    df = data.get_group(data_type).copy()\n",
    "    df.columns = df.columns.droplevel('Type')\n",
    "    df.rename(columns={\n",
    "        \"Albanie\": \"Albania\",\n",
    "        \"Allemagne\": \"Germany\",\n",
    "        \"Andorre\": \"Andorra\",\n",
    "        \"Autriche\": \"Austria\",\n",
    "        \"Belgique\": \"Belgium\",\n",
    "        \"Biélorussie\": \"Belarus\",\n",
    "        \"Bosnie-Herzégovine\": \"Bosnia and Herzegovina\",\n",
    "        \"Bulgarie\": \"Bulgaria\",\n",
    "        \"Croatie\": \"Croatia\",\n",
    "        \"Danemark\": \"Denmark\",\n",
    "        \"Espagne\": \"Spain\",\n",
    "        \"Estonie\": \"Estonia\",\n",
    "        \"Finlande\": \"Finland\",\n",
    "        \"France\": \"France\",\n",
    "        \"Grèce\": \"Greece\",\n",
    "        \"Hongrie\": \"Hungary\",\n",
    "        \"Irlande\": \"Ireland\",\n",
    "        \"Islande\": \"Iceland\",\n",
    "        \"Italie\": \"Italy\",\n",
    "        \"Kosovo\": \"Kosovo\",\n",
    "        \"Lettonie\": \"Latvia\",\n",
    "        \"Liechtenstein\": \"Liechtenstein\",\n",
    "        \"Lituanie\": \"Lithuania\",\n",
    "        \"Luxembourg\": \"Luxembourg\",\n",
    "        \"Malte\": \"Malta\",\n",
    "        \"Moldavie\": \"Moldova\",\n",
    "        \"Monaco\": \"Monaco\",\n",
    "        \"Monténégro\": \"Montenegro\",\n",
    "        \"Norvège\": \"Norway\",\n",
    "        \"Pays-Bas\": \"Netherlands\",\n",
    "        \"Pologne\": \"Poland\",\n",
    "        \"Portugal\": \"Portugal\",\n",
    "        \"République tchèque\": \"Czech Republic\",\n",
    "        \"Roumanie\": \"Romania\",\n",
    "        \"Royaume-Uni\": \"United Kingdom\",\n",
    "        \"Russie\": \"Russia\",\n",
    "        \"Saint-Marin\": \"San Marino\",\n",
    "        \"Serbie\": \"Serbia\",\n",
    "        \"Slovaquie\": \"Slovakia\",\n",
    "        \"Slovénie\": \"Slovenia\",\n",
    "        \"Suède\": \"Sweden\",\n",
    "        \"Suisse\": \"Switzerland\",\n",
    "        \"Ukraine\": \"Ukraine\",\n",
    "        \"Vatican\": \"Vatican City\"\n",
    "    }, inplace=True)\n",
    "\n",
    "    df.index.name = None\n",
    "    df.columns.name = 'Material'\n",
    "\n",
    "    # Melt the DataFrame to long format for merging\n",
    "    df_melted = df.reset_index().melt(id_vars='index', var_name='Country', value_name='Value')\n",
    "    df_melted.rename(columns={'index': 'Date'}, inplace=True)\n",
    "\n",
    "    # Merge GeoJSON data with DataFrame\n",
    "    europe_merged = europe.merge(df_melted, left_on='NAME', right_on='Country', how='left')\n",
    "\n",
    "    # Plot the data\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    europe_merged.plot(column='Value', ax=ax, legend=True, cmap='viridis', \n",
    "                       missing_kwds={\"color\": \"lightgrey\"},\n",
    "                       legend_kwds={'label': data_type})\n",
    "\n",
    "    plt.title(f\"Map of {data_type} in Europe in {pd.to_datetime(date).strftime('%B %Y')}\", fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650e511858e74e5da93356b41210045d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='France, Allemagne, Italie', continuous_update=False, description='Countries:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Widgets to select countries and dates\n",
    "countries_widget = widgets.Text(\n",
    "    value='France, Allemagne, Italie',\n",
    "    description='Countries:',\n",
    "    placeholder='Enter countries separated by commas'\n",
    ")\n",
    "\n",
    "start_date_widget = widgets.DatePicker(\n",
    "    value=pd.to_datetime('2015-1', format='%Y-%m'),\n",
    "    description='Start Date'\n",
    ")\n",
    "\n",
    "end_date_widget = widgets.DatePicker(\n",
    "    value=pd.to_datetime('2020-3', format='%Y-%m'),\n",
    "    description='End Date'\n",
    ")\n",
    "\n",
    "# Widget for multiple choice of data to display\n",
    "multi_choice_widget = widgets.SelectMultiple(\n",
    "    options=['IPCH', 'Dette publique', 'PIB', 'Chomage'],\n",
    "    value=['IPCH'],\n",
    "    description='Select Data',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Checkbox widget\n",
    "checkbox_widget = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Map',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# General function to plot the data\n",
    "@interact_manual(countries=countries_widget, \\\n",
    "                 start_date=start_date_widget, end_date=end_date_widget, \\\n",
    "                 type=multi_choice_widget, map=checkbox_widget)\n",
    "def plot_G(countries, start_date, end_date, type, map=True):\n",
    "    \"\"\"\n",
    "    Plot the data based on the selected countries, dates, and data types.\n",
    "\n",
    "    Parameters:\n",
    "    countries (str): Comma-separated list of countries to plot.\n",
    "    start_date (datetime): The start date for the data to plot.\n",
    "    end_date (datetime): The end date for the data to plot.\n",
    "    type (list): List of data types to plot (e.g., 'IPCH', 'Dette publique').\n",
    "    map (bool): If True, plot a map; otherwise, plot a graph.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if map:\n",
    "        plot_map(date=start_date, data=data, data_type=type[0])\n",
    "    else:\n",
    "        for t in type:\n",
    "            plot_graph(countries, start_date, end_date, data.get_group(t), t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage matières premières"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6eac0febce4aaea147ad5b3636fc74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(DatePicker(value=Timestamp('2013-01-01 00:00:00'), description='Début', step=1), DatePic…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_data(start_date, end_date, show_or, show_petrol)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to plot the data\n",
    "def plot_data(start_date, end_date, show_or, show_petrol):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    start_date, end_date = pd.to_datetime(start_date), pd.to_datetime(end_date)\n",
    "    \n",
    "    # Filter the data based on the selected dates\n",
    "    filtered_material = material[(material.index >= start_date) & (material.index <= end_date)]\n",
    "    \n",
    "    if show_or:\n",
    "        sb.lineplot(data=filtered_material, x=filtered_material.index, y='Or', label='Or', marker='o')\n",
    "        # If 'Or' checkbox is checked, plot the data for 'Or'\n",
    "\n",
    "    if show_petrol:\n",
    "        sb.lineplot(data=filtered_material, x=filtered_material.index, y='Petrol', label='Pétrole', marker='o')\n",
    "        # If 'Petrol' checkbox is checked, plot the data for 'Petrol'\n",
    "\n",
    "    # Configure the plot with a title, legend, and grid\n",
    "    plt.title(f'Cour de l\\'or et du pétrole en euros ({start_date.strftime(\"%m/%Y\")} - {end_date.strftime(\"%m/%Y\")})')\n",
    "    plt.legend()  # The legend is automatically updated based on the checked datasets\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Widgets for selecting the start and end dates, and options to display data\n",
    "start_date_widget = widgets.DatePicker(description='Début', value=pd.to_datetime('2013-01-01'))\n",
    "end_date_widget = widgets.DatePicker(description='Fin', value=pd.to_datetime('2023-12-31'))\n",
    "show_or_widget = widgets.Checkbox(description='Or', value=True)  # Checkbox for showing 'Or' data\n",
    "show_petrol_widget = widgets.Checkbox(description='Pétrole', value=True)  # Checkbox for showing 'Petrol' data\n",
    "\n",
    "# Interactive interface to control the plot function with widgets\n",
    "interact(\n",
    "    plot_data,  # The function to interact with\n",
    "    start_date=start_date_widget,  # Start date widget\n",
    "    end_date=end_date_widget,  # End date widget\n",
    "    show_or=show_or_widget,  # 'Or' checkbox widget\n",
    "    show_petrol=show_petrol_widget  # 'Petrol' checkbox widget\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage des devises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e384c86c16ad4654b6d89c2d6a0f3d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='Tout sélectionner'), DatePicker(value=Timestamp('2013…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_devise_data(select_all, start_date, end_date, **currency_checkboxes)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to plot currency data\n",
    "def plot_devise_data(select_all, start_date, end_date, **currency_checkboxes):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # List of selected currencies\n",
    "    if select_all:\n",
    "        selected_currencies = devise.columns.tolist()  # If \"select all\" is checked, include all currencies\n",
    "    else:\n",
    "        selected_currencies = [currency for currency, is_selected in currency_checkboxes.items() if is_selected]\n",
    "        # If not, include only the selected currencies based on the checkboxes\n",
    "\n",
    "    # Filter the data based on the selected date range and currencies\n",
    "    filtered_data = devise[(devise.index >= start_date) & (devise.index <= end_date)]\n",
    "    filtered_data = filtered_data[selected_currencies]\n",
    "\n",
    "    # Plot the time series for each selected currency\n",
    "    for currency in selected_currencies:\n",
    "        sb.lineplot(data=filtered_data, x=filtered_data.index, y=currency, label=currency, marker='o')\n",
    "\n",
    "    # Configure the plot with a title, x and y labels, and a legend\n",
    "    plt.title(f'Valeurs des devises (équivalent en euros) ({start_date.strftime(\"%m/%Y\")} - {end_date.strftime(\"%m/%Y\")})')\n",
    "    plt.xlabel('TIME_PERIOD')\n",
    "    plt.ylabel('OBS_VALUE')\n",
    "    plt.legend(title='Currency')  # Currency legend\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Widgets for selecting the start and end dates\n",
    "start_date_widget = widgets.DatePicker(description='Début', value=pd.to_datetime('2013-01-01'))\n",
    "end_date_widget = widgets.DatePicker(description='Fin', value=pd.to_datetime('2023-12-31'))\n",
    "\n",
    "# Dynamically generate checkboxes for each currency\n",
    "currency_checkboxes = {\n",
    "    currency: widgets.Checkbox(description=currency, value=False)  # Default value is False (unchecked)\n",
    "    for currency in devise.columns\n",
    "}\n",
    "\n",
    "# Checkbox to \"Select All\" currencies\n",
    "select_all_widget = widgets.Checkbox(description='Tout sélectionner', value=False)\n",
    "\n",
    "# Function to dynamically update checkboxes based on \"Select All\"\n",
    "def update_checkboxes(change):\n",
    "    for checkbox in currency_checkboxes.values():\n",
    "        checkbox.value = change['new']  # Update the state of checkboxes based on the \"Select All\" checkbox\n",
    "\n",
    "select_all_widget.observe(update_checkboxes, names='value')  # Observe changes to the \"Select All\" checkbox\n",
    "\n",
    "# Create a container for all the checkboxes\n",
    "checkbox_container = VBox([select_all_widget] + list(currency_checkboxes.values()))\n",
    "\n",
    "# Interactive interface to control the plot function with widgets\n",
    "interact(\n",
    "    plot_devise_data,  # The function to interact with\n",
    "    select_all=select_all_widget,  # \"Select All\" widget\n",
    "    start_date=start_date_widget,  # Start date widget\n",
    "    end_date=end_date_widget,  # End date widget\n",
    "    **currency_checkboxes  # Pass each currency checkbox widget as a parameter\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrélations\n",
    "\n",
    "Argument: data, type, historique (pd.Series), countries\\\n",
    "Calcule la correlation entre (colonne et historique, pour colonne dans data.get_group(type).loc[;, countries])\\\n",
    "Fais une moyenne des correlation.\\\n",
    "Retourne la correlation moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th colspan=\"10\" halign=\"left\">PIB</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">IPCH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Autriche</th>\n",
       "      <th>Belgique</th>\n",
       "      <th>Bulgarie</th>\n",
       "      <th>Chypre</th>\n",
       "      <th>Allemagne</th>\n",
       "      <th>Danemark</th>\n",
       "      <th>Estonie</th>\n",
       "      <th>Grèce</th>\n",
       "      <th>Espagne</th>\n",
       "      <th>Finlande</th>\n",
       "      <th>...</th>\n",
       "      <th>Lituanie</th>\n",
       "      <th>Luxembourg</th>\n",
       "      <th>Lettonie</th>\n",
       "      <th>Malte</th>\n",
       "      <th>Pays-Bas</th>\n",
       "      <th>Pologne</th>\n",
       "      <th>Portugal</th>\n",
       "      <th>Roumanie</th>\n",
       "      <th>Suède</th>\n",
       "      <th>Slovénie</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">PIB</th>\n",
       "      <th>Autriche</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990586</td>\n",
       "      <td>0.906579</td>\n",
       "      <td>0.962533</td>\n",
       "      <td>0.993234</td>\n",
       "      <td>0.963908</td>\n",
       "      <td>0.938519</td>\n",
       "      <td>0.637396</td>\n",
       "      <td>0.958764</td>\n",
       "      <td>0.962428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649532</td>\n",
       "      <td>0.649376</td>\n",
       "      <td>0.587124</td>\n",
       "      <td>0.483208</td>\n",
       "      <td>0.635156</td>\n",
       "      <td>0.769894</td>\n",
       "      <td>0.523888</td>\n",
       "      <td>0.743113</td>\n",
       "      <td>0.632781</td>\n",
       "      <td>0.637924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Belgique</th>\n",
       "      <td>0.990586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930290</td>\n",
       "      <td>0.973268</td>\n",
       "      <td>0.979347</td>\n",
       "      <td>0.981178</td>\n",
       "      <td>0.945167</td>\n",
       "      <td>0.656340</td>\n",
       "      <td>0.939378</td>\n",
       "      <td>0.948630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.685807</td>\n",
       "      <td>0.684618</td>\n",
       "      <td>0.621437</td>\n",
       "      <td>0.514694</td>\n",
       "      <td>0.678838</td>\n",
       "      <td>0.814145</td>\n",
       "      <td>0.557605</td>\n",
       "      <td>0.771293</td>\n",
       "      <td>0.661347</td>\n",
       "      <td>0.668158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bulgarie</th>\n",
       "      <td>0.906579</td>\n",
       "      <td>0.930290</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983123</td>\n",
       "      <td>0.880897</td>\n",
       "      <td>0.925532</td>\n",
       "      <td>0.988240</td>\n",
       "      <td>0.388354</td>\n",
       "      <td>0.819593</td>\n",
       "      <td>0.797372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776166</td>\n",
       "      <td>0.732925</td>\n",
       "      <td>0.724778</td>\n",
       "      <td>0.618549</td>\n",
       "      <td>0.741145</td>\n",
       "      <td>0.897544</td>\n",
       "      <td>0.653624</td>\n",
       "      <td>0.795084</td>\n",
       "      <td>0.772618</td>\n",
       "      <td>0.706029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chypre</th>\n",
       "      <td>0.962533</td>\n",
       "      <td>0.973268</td>\n",
       "      <td>0.983123</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942629</td>\n",
       "      <td>0.962962</td>\n",
       "      <td>0.990660</td>\n",
       "      <td>0.492993</td>\n",
       "      <td>0.892280</td>\n",
       "      <td>0.882768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727528</td>\n",
       "      <td>0.712580</td>\n",
       "      <td>0.666345</td>\n",
       "      <td>0.560293</td>\n",
       "      <td>0.706104</td>\n",
       "      <td>0.851151</td>\n",
       "      <td>0.597249</td>\n",
       "      <td>0.786932</td>\n",
       "      <td>0.718094</td>\n",
       "      <td>0.682381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Allemagne</th>\n",
       "      <td>0.993234</td>\n",
       "      <td>0.979347</td>\n",
       "      <td>0.880897</td>\n",
       "      <td>0.942629</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963257</td>\n",
       "      <td>0.921830</td>\n",
       "      <td>0.616938</td>\n",
       "      <td>0.936946</td>\n",
       "      <td>0.978944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.576721</td>\n",
       "      <td>0.573872</td>\n",
       "      <td>0.514082</td>\n",
       "      <td>0.403540</td>\n",
       "      <td>0.564351</td>\n",
       "      <td>0.720011</td>\n",
       "      <td>0.446874</td>\n",
       "      <td>0.679405</td>\n",
       "      <td>0.561292</td>\n",
       "      <td>0.564102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Type                 PIB                                                    \\\n",
       "Country         Autriche  Belgique  Bulgarie    Chypre Allemagne  Danemark   \n",
       "Type Country                                                                 \n",
       "PIB  Autriche   1.000000  0.990586  0.906579  0.962533  0.993234  0.963908   \n",
       "     Belgique   0.990586  1.000000  0.930290  0.973268  0.979347  0.981178   \n",
       "     Bulgarie   0.906579  0.930290  1.000000  0.983123  0.880897  0.925532   \n",
       "     Chypre     0.962533  0.973268  0.983123  1.000000  0.942629  0.962962   \n",
       "     Allemagne  0.993234  0.979347  0.880897  0.942629  1.000000  0.963257   \n",
       "\n",
       "Type                                                    ...      IPCH  \\\n",
       "Country          Estonie     Grèce   Espagne  Finlande  ...  Lituanie   \n",
       "Type Country                                            ...             \n",
       "PIB  Autriche   0.938519  0.637396  0.958764  0.962428  ...  0.649532   \n",
       "     Belgique   0.945167  0.656340  0.939378  0.948630  ...  0.685807   \n",
       "     Bulgarie   0.988240  0.388354  0.819593  0.797372  ...  0.776166   \n",
       "     Chypre     0.990660  0.492993  0.892280  0.882768  ...  0.727528   \n",
       "     Allemagne  0.921830  0.616938  0.936946  0.978944  ...  0.576721   \n",
       "\n",
       "Type                                                                         \\\n",
       "Country        Luxembourg  Lettonie     Malte  Pays-Bas   Pologne  Portugal   \n",
       "Type Country                                                                  \n",
       "PIB  Autriche    0.649376  0.587124  0.483208  0.635156  0.769894  0.523888   \n",
       "     Belgique    0.684618  0.621437  0.514694  0.678838  0.814145  0.557605   \n",
       "     Bulgarie    0.732925  0.724778  0.618549  0.741145  0.897544  0.653624   \n",
       "     Chypre      0.712580  0.666345  0.560293  0.706104  0.851151  0.597249   \n",
       "     Allemagne   0.573872  0.514082  0.403540  0.564351  0.720011  0.446874   \n",
       "\n",
       "Type                                          \n",
       "Country         Roumanie     Suède  Slovénie  \n",
       "Type Country                                  \n",
       "PIB  Autriche   0.743113  0.632781  0.637924  \n",
       "     Belgique   0.771293  0.661347  0.668158  \n",
       "     Bulgarie   0.795084  0.772618  0.706029  \n",
       "     Chypre     0.786932  0.718094  0.682381  \n",
       "     Allemagne  0.679405  0.561292  0.564102  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correlation_matrix(data, variables):\n",
    "  \"\"\"Return the correlation matrix for specified variables.\"\"\"\n",
    "  data_flat = data.apply(lambda x: x.droplevel(0, axis=1))  # Flatten MultiIndex\n",
    "  return data_flat[variables].corr()  # Compute and return correlation matrix\n",
    "\n",
    "# Test\n",
    "variables = ['PIB', 'Chomage','IPCH']\n",
    "correlation_matrix(data, variables).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4958012269152313\n"
     ]
    }
   ],
   "source": [
    "def compute_avg_correlation(data, hist_action, country, data_type, company):\n",
    "    \"\"\"\n",
    "    Compute avg correlation between a company's data in hist_action and a country's data in the provided dataset.\n",
    "    \"\"\"\n",
    "    # Flatten the MultiIndex structure in the dataset to simplify access to columns.\n",
    "    ungrouped_data = data.apply(lambda df: df.droplevel(0, axis=1))\n",
    "\n",
    "    # Check if the specified company and country exist in their respective datasets.\n",
    "    if company not in hist_action.columns or country not in ungrouped_data[data_type].columns:\n",
    "        return None  # Return None if either is not present.\n",
    "    \n",
    "    # Extract a specific column for the company.\n",
    "    # If the company's data is a DataFrame (multi-column), select the 'Close' column by default.\n",
    "    if isinstance(hist_action[company], pd.DataFrame):\n",
    "        col_company = hist_action[company]['Close']  # Use the 'Close' column for the company's data.\n",
    "    else:\n",
    "        col_company = hist_action[company]  # Use the entire series if no multi-column structure exists.\n",
    "    \n",
    "    # Extract the country's data as a Series.\n",
    "    col_country = ungrouped_data[data_type][country]\n",
    "    \n",
    "    # Ensure both columns have valid (non-NaN) data by dropping null values.\n",
    "    col_company = col_company.dropna()\n",
    "    col_country = col_country.dropna()\n",
    "\n",
    "    # Align the two Series by finding the intersection of their indices (dates).\n",
    "    common_index = col_company.index.intersection(col_country.index)\n",
    "    col_company = col_company.loc[common_index]\n",
    "    col_country = col_country.loc[common_index]\n",
    "    \n",
    "    # Compute the correlation between the two aligned Series if there are valid data points.\n",
    "    if len(col_company) > 0 and len(col_country) > 0:\n",
    "        return col_company.corr(col_country)  # Return the correlation coefficient.\n",
    "    else:\n",
    "        return None  # Return None if there are no valid data points.\n",
    "\n",
    "\n",
    "  \n",
    "result = compute_avg_correlation(data, hist_action, 'France', 'PIB', 'Air Liquide')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.66980193921972)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_material_correlation(material, hist_action, commodity, company):\n",
    "  \"\"\"compute avg correlation between commodity in material and company in hist_action\"\"\"\n",
    "  col_material = material[commodity]  # select commodity column\n",
    "  col_company = hist_action.xs(company, axis=1, level='Company')  # select company column\n",
    "  correlations = [col_material.corr(col_company[c]) for c in col_company.columns]  # compute correlations\n",
    "  return sum(correlations) / len(correlations)  # return mean correlation\n",
    "\n",
    "avg_material_correlation(material, hist_action, 'Or', 'Air Liquide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-0.43735104125250024)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_avg_currency_correlation(devise, hist_action, currency, company):\n",
    "  \"\"\"compute avg correlation between company in hist_action and currency in devise\"\"\"\n",
    "  col_company = hist_action.xs(company, axis=1, level='Company')  # select company column\n",
    "  col_currency = devise[currency]  # select currency column\n",
    "  correlations = [col_company[c].corr(col_currency) for c in col_company.columns]  # compute correlations\n",
    "  return sum(correlations) / len(correlations)  # return mean correlation\n",
    "\n",
    "compute_avg_currency_correlation(devise, hist_action, 'US dollar', 'Air Liquide')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
